---
title             : "Extending the Bicriterion Approach for Anticlustering: Optimal and Heuristic Approaches"
shorttitle        : "Extending the Bicriterion Approach for Anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich-Heine-Universität Düsseldorf, Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"
  - name          : "Martin Breuer"
    affiliation   : "1"
    corresponding : no
  - name          : "Max Diekhoff"
    affiliation   : "1"
    corresponding : no
  - name          : "Gunnar W. Klau"
    affiliation   : "1"
    corresponding : no

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"
    

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf. Martin Breuer, Gunnar W. Klau, Max Diekhoff, 
  Department of Computer Science, Heinrich Heine University Düsseldorf.
  
abstract: |

    Numerous applications in psychological research require that a data set is partitioned via the maximization---instead of the more common minimization---of a clustering criterion. Such anticlustering seeks for high similarity between groups (i.e., maximum diversity) or high pairwise dissimilarity within groups (i.e., maximum dispersion). Brusco et al. (2020) proposed a powerful bicriterion algorithm (BILS) that simultaneously seeks for maximum diversity and dispersion, introducing the bicriterion approach for anticlustering. Here, we advance the bicriterion approach in several ways. As our main contribution, we combine a novel exact algorithm for maximum dispersion with the BILS heuristic to obtain high diversity on top of an optimal dispersion. Despite its theoretical computational hardness, our optimal method scales to rather large data sets. It generates multiple partitions that we use as initializations of the BILS's local search, thus guaranteeing that the BILS also returns a partition with optimal dispersion. In a simulation study, we compared several adaptations of the BILS. To obtain high diversity, initializing the BILS with multiple optimal partitions is preferable to using only one. Moreover, in data sets where maintaining optimal dispersion severely restricts the search for feasible partitions, the iterated local search phase of the BILS is crucial to obtain high diversity. While the original BILS oftentimes finds optimal dispersion values, our simulation illustrates conditions when it does not. The optimal dispersion algorithm and the BILS including our extensions are available via the free and open source R package `anticlust`. Practical examples are included to illustrate their application.

bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, maximum dispersion, optimal algorithm, bicriterion optimization"
wordcount         : "12345"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

output            : 
  papaja::apa6_pdf

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \floatstyle{plaintop}
  - \restylefloat{figure}
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - |
    \makeatletter
    \renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
      \hskip -\arraycolsep
      \let\@ifnextchar\new@ifnextchar
      \array{#1}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

---

```{r setup, include = FALSE}
library(papaja) # use the development version: `remotes::install_github("crsh/papaja@devel")`; currently 0.1.1.9001
library(anticlust)
library(knitr)
# library(dplyr)
# library(ggplot2)
# library(tidyr)
# library(DescTools)
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library(prmisc) # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```


Anticlustering problems arise in many research fields including psychology [@brusco2019; @schaper2023], test assessment [@gierl2017], education [@krauss2013; @baker2002], artificial intelligence [@steghofer2013], machine learning [@mauri2023], network transmission [@mohebi2022], and operations research [e.g. @gallego2013; @gliesch2021]. Anticlustering seeks for similarity between groups and heterogeinity within groups by maximizing a clustering objective criterion. Thus, anticlustering is the logical and mathematical opposite of classical clustering, which conducts partitioning via minimization of the same criteria [@steinley2006; @brusco2019; @spath1986]

More formally, anticlustering is used to partition $N$ objects into $K$ groups $C = (C_1, \ldots, C_K)$, where $C_k$ is the set of $n_k$ objects in the $k^{th}$ group, and $K$ and $n_k$ are predetermined by the application. In many anticlustering applications, equal-sized groups are required, i.e., $n_k = \frac{N}{K}$ [@schulz2022; @gallego2013]. The partitioning is characterized by assignment to exhaustive and mutually exclusive subgroups:

\begin{align}
\bigcup\limits_{k = 1}^{K} C_k = X \label{formalization:1} \\
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k\label{formalization:2} \\
\vert C_k \vert = n_k, \; \forall k \in \{1, \ldots, K\} \label{formalization:3}
\end{align}

We assume that a data matrix $\mathbf{X}_{N \times M} = \{x_{ij}\}_{N \times M}$ is available where the $M$ columns contain numeric measurements on the $N$ objects. The partitioning is obtained by maximizing a clustering criterion $f_i(C)$. For example, *k*-means anticlustering aims to maximize between-group similarity by maximizing the error sum of squares, i.e., the sum the squared distances between each object and its cluster center [@spath1986; @papenberg2024]:

$$
f_1(C) = 
  \sum\limits_{j=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The $k^{th}$ cluster center $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ contains the mean values of the $M$ variables computed across all observations belonging to the  $k^{th}$ group:

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Maximizing $f_1$ ensures that the cluster centers, i.e., the mean values of the $M$ measurements, are as close to each other as possible. The *k*-plus criterion is an extension of the $k$-means criterion for anticlustering that can be implemented by adding fictitious to the data matrix $\mathbf{X}_{N \times M}$ [@papenberg2024]. *K*-plus anticlustering is used to equate not only means between clusters, but higher order moments such as the variance or skewness as well. Because the approach can be reduced to an augmentation of the data set, *k*-plus anticlustering leaves the original *k*-means criterion $f_1$ unchanged. 

Other anticlustering criteria usually use as input a matrix $\mathbf{D}_{N \times N} = \{d_{ij}\}_{N \times N}$ of pairwise dissimilarity measurements. The *diversity* criterion is the most important criterion of this kind. It is computed as the within-group sum of pairwise dissimilarities [@brusco2019; @papenberg2020; @feo1990]:
 
$$
f_2(C) = \sum_{(i < j) \in C_k} d_{ij}
$$

Maximizing the diversity simultaneously minimizes the sum of distances between objects not in the same group. Therefore, the diversity represents within-group heterogeneity as well as between-group similarity [@feo1990]. Being a sum instead of an average, however, it no longer adequately captures between-group similarity if group sizes are not equal. The diversity is oftentimes obtained by first converting the data matrix $\mathbf{X}_{N \times M}$ into a matrix of pairwise Euclidean distances. However, most anticlustering algorithms that use a dissimilarity matrix as input are agnostic as to how it was was generated [@brusco2019; cf. @schulz2022]. 

Unlike $f_1$ and $f_2$, the *dispersion* criterion does not represent between-group similarity, but is a pure measure of within-group heterogeneity. It is defined as the minimum dissimilarity across any two objects that are part of the same group [@brusco2019; @fernandez2013]:

$$
f_3(C) = \textrm{min}_{(i < j) \in C_k} d_{ij}
$$

The dispersion is a measure of the worst-case pairwise dissimilarity. Maximizing the dispersion ensures that any two objects in the same group are as dissimilar from each other as possible. 

@brusco2019 outlined the importance of simultaneously considering multiple criteria when tackling anticlustering problems. In particular, they argued that anticlustering algorithms should incorporate an objective of between-group similarity---such as the diversity---and pairwise within-group dissimilarity---i.e., the dispersion---at the same time.

## Algorithms for anticlustering

Heuristics dominate (local maximum search @spath1986; @weitz1998, multiple repetitions robust for k-means @steinley2007, ILS @brusco2017, VARIABLE NEIGHBORHOOD SEARCH @urovsevic2014; tabu search @gallego2013; multiple improvement phases based on initial local maximum search @yang2022). Exact algorithms for diversity [@papenberg2020; @schulz2022] and for the dispersion [@fernandez2013; @gliesch2021; @brucker1978]. Exact algorithms for diversity perform much worse than for the reversed clustering problems, where many hundred elements can often be processed. Maximum diversity can be achieved with much better success. Our novel contribution is to combine optimal maximum dispersion with high diversity. We present a novel algorithm for maximum diversity that is similar to the approaches by @fernandez2013 and @gliesch2021 and is based on the logic that was already introduced by @brucker1978. It scales to hundreds or even thousands of data points and therefore can be used to solve most real-world applications we have in mind. We then proceed to discuss how on top of an optimal dispersion, we can implement high diversity. A logical next step is to obtain optimal maximum diversity as well, which is however only limited to rather small data sets, as we show. Due to its bicriterion nature, the BILS by @brusco2019 is naturally suited to maintain optimum dispersion while optimizing diversity. We discuss several adaptations of the BILS, which are compared in a simulation study. 

In within-subjects experiments, participants are presented with several stimulus sets that are affiliated with the different experimental conditions. Due to carry-over effects, showing the same stimuli in different conditions is oftentimes prohibited. This is evident in studies on memory, where studying a list of words in one conditions necessarily makes it impossible to repeat their study without being influenced by the earlier study phase; memory cannot be erased after an experimental block has been completed [@lahl2006]. While the experimental logic dictates that different lists must be used, the lists should be as parallel as possible with regard to variables that affect the response variable. @lintz2021 gave two reasons as to why it is important that study participants should be confronted with balanced lists: (a) if lists are not sufficiently parallel, the variance of within-subjects effect sizes due to the experimental manipulation will be inflated, reducing statistical power---if there is no proper statistical control of the stimulus materials, which is often the case [@judd2012]; (b) highly unbalanced sets may introduce secondary effects beyond the influence of the intrinsic stimulus features. For example, highly biased lists may induce a strategic behavior shift between experimental blocks, possibly due to suspicion, boredom or frustration. These effects invalidate any observed differences between conditions and importantly, this bias cannot be accounted for via statistical techniques.

Traditionally, assigning stimuli to experimental conditions was done manually [e.g., @schaper2019metamory] or randomly [e.g., @kroneisen2018], both of which methods however do not ensure that the stimulus sets will be similar [@papenberg2020; @cutler1981; @vancasteren2007; @armstrong2012]. Generally, automated approaches are preferred that quantify similarity on the basis of the stimulus features and use an algorithm to assign stimuli in such a way that this similarity is optimized [@brusco2019; @papenberg2020]. One approach that has recently gained traction

**Using anticlustering to equate stimulus sets**

**BUT DISPERSION IS ALSO IMPORTANT!**

For example, in tasks of recognition memory, participants commonly have to decide if a target stimuli (e.g., a word) has been presented in an earlier phase of the experiment. In this case, words that are orthographically similar to a target word should serve as distractors. 

The BILS algorithm has been available in the anticlust package since 2021, but we assume that many researchers do not yet know about its potential, also given that the introduction by @brusco2019 was rather technical in nature. For now, users seem to rely on the unicriterion methods focusing on between-set similarity only. Therefore, we provide readers with an application-focused introduction and extend BILS with parameters that may be useful when designing experiments. 

--- 

**OLD**

Anticlustering refers to the process of partitioning elements into disjoint groups with the goal of obtaining high between-group similarity and high within-group heterogeneity [@brusco2019; @spath1986; @valev1998; @valev1983]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and separation between clusters [@rokach2005]. Anticlustering has many applications in research psychology [@steinley2006; @brusco2019; @papenberg2020]. Examples include splitting tests into parts of equal difficulty [@gierl2017], assigning students to work groups [@baker2002], and assigning stimuli to different, but parallel experimental conditions [@lahl2006]. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, anticlustering problems can be formalized as mathematical optimization problems [e.g., @baker2002; @brusco2019; @spath1986; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods are characterized by (a) an objective function that quantifies between-group similarity and/or within-group heterogeneity, and (b) an algorithm that determines how elements are assigned to groups to maximize the objective function. Several anticlustering objective functions use pairwise dissimilarity ratings such as the Euclidean distance as input. The most prominent criterion of this kind is the *diversity*, which is the total sum of pairwise dissimilarities between elements within the same group [@brusco2019; @gallego2013]. By considering pairwise dissimilarities between elements in the same group, the diversity criterion reflects the total degree of within-group heterogeneity. High within-group diversity simultaneously ensures high between-group similarity.[^equalsizedgroups] Another important anticlustering criterion based on the information in a dissimilarity matrix is the *dispersion*, which is the minimum dissimilarity between any two elements within the same group [@fernandez2013]. Maximizing the dispersion increases within-group heterogeneity by ensuring that any two elements in the same group are as dissimilar from each other as possible. @brusco2019 convincingly argued that anticlustering applications striving for within-group heterogeneity should incorporate both dispersion and diversity, and they presented a bicriterion algorithm to approximate the Pareto efficient set of solutions according to both criteria. 

[^equalsizedgroups]: As will be shown later, the diversity objective only strictly equalizes within-group heterogeneity and between-group similarity when groups are equal-sized.

Oftentimes, partitioning applications in psychology are focused on between-group similarity rather than within-group heterogeneity, even though both goals usually coincide. High between-group similarity is for example desirable when designing experimental conditions using different stimulus sets that should be as similar as possible with regard to response-relevant attributes [@lahl2006]. Such stimulus selection tasks are usually conducted on the basis of attribute data and not on pairwise (dis)similarity ratings. That is, individual stimuli are numerically coded on relevant dimensions [@dry2009]. For stimuli in psycholinguistic experiments, attributes may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attributes are binary and may represent the presence or absence of a feature [@tversky1977]. When attribute values are available, anticlustering approaches that do not require a dissimilarity matrix[^dissimilarityfromfeatures] can be used to partition the data. K-means is probably the best-known clustering objective that can directly be computed on attribute values. In k-means clustering, the sum of the squared Euclidean distances between data points and their cluster centers is minimized, usually using the k-means heuristic [@jain2010; @steinley2006; @brusco2006branch]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. Reversing the k-means objective function---using maximization instead of minimization---has been recognized as a useful approach when aiming for high between-group similarity [@spath1986; @valev1998; @valev1983]. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. However, other distribution characteristics---such as the variance---are not targeted. @papenberg2020 showed that k-means anticlustering tends to over-optimize between-group similarity with regard to the mean at the cost of similarity in variance. However, when aiming for overall between-group similarity, neglecting all distribution characteristics other than the mean is misguided; similar means can be obtained even when the underlying distributions are clearly different [e.g., @anscombe1973]. 

[^dissimilarityfromfeatures]: It is still possible to convert the attribute data into a dissimilarity matrix, e.g. by computing all pairwise Euclidean distances, and then optimize a distance-based anticlustering criterion to obtain a partitioning. For Likert scale data or binary attributes, the Manhattan distance may be preferred over the Euclidean distance; for mixed-type attributes, it is even possible to compute a combined distance metric [@rokach2005].

To optimize overall between-group similarity---and hence to overcome the limitations of k-means anticlustering---this paper introduces k-plus anticlustering. K-plus is an extension of the k-means objective that quantifies between-group similarity as discrepancy with regard to several distribution characteristics instead of only the mean. Specifically, k-plus offers the possibility to minimize differences with regard to the variance and higher order moments such as skewness and kurtosis. After formally introducing the k-plus objective, two simulation studies show that k-plus anticlustering is well-suited to create groups having minimal differences according to multiple distribution characteristics, significantly outperforming traditional criteria such as the diversity and the k-means objective. Practical examples illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust` [@papenberg2020; @R-anticlust], an extension to the popular statistical programming language R [@R-base].

# Evaluation

Two simulation studies were conducted to evaluate the new k-plus objective for anticlustering. Simulation 1 compared a selected number of weighting schemes for the k-plus criterion $\mathit{SSE_{kplusGeneralized}}$. The results of this initial test determined the benchmark implementation of k-plus anticlustering that was used as contestant in the follow-up simulation. Simulation 2 was a large scale comparison of the k-plus objective and the most important traditional anticlustering objectives striving for between-group similarity: the diversity and the k-means objective. 

The simulation studies were implemented using the statistical programming language R [Version 4.2, @R-base]. The R package `anticlust` [Version 0.6.1, @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R package `faux` [Version 1.1.0, @R-faux] was used to generate the multivariate normal data sets that were processed during Simulation 2. The R packages `dplyr` [Version 1.0.7, @R-dplyr], `ggplot2` [Version 3.3.5, @R-ggplot2], `papaja` [Version 0.1.1.9001, @R-papaja], `tidyr` [Version 1.1.3, @R-tidyr], `fossil` [Version 0.4.0, @R-fossil], and `DescTools` [Version 0.99.45, @R-desctools] were used for data analysis and result presentation. The simulation study is fully reproducible via code and data that has been made accessible in an accompanying OSF repository [@osf2023].

## Optimization algorithm

In both simulation studies, the same local maximum search was applied to optimize the competing anticlustering objectives [Method LCW, @weitz1998]. Building on an initial random allocation, the algorithm proceeds by swapping elements between groups in such a way that each swap improves the objective criterion by the largest possible margin. That is, it starts with the first element and simulates all exchanges with elements that are currently assigned to a different group. After each exchange has been evaluated, the one exchange is realized that improves the objective function the most. No exchange is realized if no improvement is possible. The exchange process is repeated for each element. After that, the procedure restarts at the beginning and is repeated until an iteration through the entire data set no longer yields an improvement. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. In the simulation studies, five repetitions were used. Note that the local maximum search can be performed on the basis of any arbitrary initial grouping. In particular, the initial grouping determines the sizes of the anticlusters, because the exchange procedure following the set-up never changes the sizes of the groups. While the data sets in the simulation studies were always partitioned into equal-sized groups, it would also be possible to ensure that the group sizes differ by 1 at most if an even split is not possible; even different-sized groups can be requested if required by the application.

## Conditions

For Simulation 2, I generated 10,000 data sets following a normal distribution. The data sets were subsequently processed by the competing methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features varied between 2 and 5; (c) the standard deviation of all features was set to 1, 2 or 3 (the mean was always zero); (d) the correlation between all features was $r = 0$, $r = .1$, $r = .2$, $r = .3$, $r = .4$ or $r = .5$. All anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$ (i.e., each method was applied 30,000 times). Groups sizes were always equal. 

## Evaluation criteria


## Results

## Example Application II: Small data set ($N = 96$)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore, a second application illustrates that k-plus anticlustering yields satisfactory results in practice even when group sizes are smaller. I make use of a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Marie L. Schaper [@schaper2019metacognitive; @schaper2019metamory]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

# Discussion

\newpage

# References

\begingroup
<div id="refs" custom-style="Bibliography"></div>
\endgroup

