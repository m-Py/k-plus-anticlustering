---
title             : "A bicriterion extension of k-means anticlustering to optimize between-group similarity"
shorttitle        : "Bicriterion K-means anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf.  


abstract: |
    Anticlustering is an automated method to partition data sets into groups 
    in such a way that high between-group similarity and high within-group 
    heterogeneity is obtained. Anticlustering has important applications in 
    psychological research, for example to partition stimulus sets into 
    equivalent sets, or to divide a group of students into heterogeneous 
    classes. K-means anticlustering is maximizes the error sum of squares, 
    i.e., the sum of squared Euclidean distances from each element to the 
    center of the group to which it belongs. Thereby, k-means anticlustering 
    directly maximizes the similarity of the mean values of numeric 
    attributes across groups, but ignores the spread of the data. 
    This paper presents a straight-forward extension of 
    the k-means objective function that allows to simultaneously maximize 
    similarity with regard to the mean as well as the variance across 
    groups. It is shown that when both criteria receive equal weights, the 
    bicriterion optimization problem reduces to an augmentation of the input 
    data, leaving the original k-means criterion unchanged. Tests on 
    simulated and real data show that similarity with regard to means and 
    variance criteria can usually be achieved to an astonishing degree with 
    almost no cost to either criterion. It is therefore concluded that the 
    extended model should be preferred to classical k-means anticlustering 
    for optimizing between-group similarity. An exception is given for 
    binary data that does not profit from considering equality in 
    variances. 
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, variance, k-means, k-variance, bicriterion optimization"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE) 

source("misc-functions.R")

```

Anticlustering is a method to partition a set of elements into different 
groups with the focus of obtaining high between-group similarity and 
within-group heterogeneity. Anticlustering reverses the logic of its 
better known twin, cluster analysis that seeks homogeneity within 
clusters and heterogeneity between clusters [@rokach2005; 
@steinley2006]. The method of anticlustering is useful in many 
tasks surrounding the work of researchers in psychology, for example, 
when splitting a test into parts of equal difficulty [@gierl2017], when 
assigning students to work groups [@baker2002], or when assigning 
stimuli to experimental conditions [@lahl2006]. Solving such problems 
"by hand" is a tedious and time-consuming task and the quality of 
manually established groups is usually improvable. Fortunately, these 
problems can be formalized mathematically via several anticlustering 
objective functions [e.g., @brusco2019; @spath1986; @baker2002; 
@fernandez2013], and accessible open source software solutions exist 
[@papenberg2020].

Several anticlustering objectives are available to assess between-group 
similarity or within-group heterogeneity. In many cases, both objectives 
are directly related, that is, maximizing heterogeneity within groups 
simultaneously maximizes between-group similarity. Several 
anticlustering objectives are based pairwise dissimilarity ratings; the 
most prominent criterion of this kind is the *diversity*, which is the 
total sum of dissimilarities between any elements within the same group. 
Note that maximizing the diversity is equivalent to minimizing 
the sum of dissimilarities between elements in different clusters, 
directly illustrating the correspondence between within-group 
heterogeneity and between-group similarity. Another important criterion 
based on pairwise dissimilarities is the dispersion, which is the 
minimum dissimilarity between any two items in the same group 
[@fernandez2013; @brusco2019]. Maximizing the dispersion increases the 
within-group heterogeneity by ensuring that any two elements as 
dissimilar as possible from each other. Note that maximizing the 
dispersion no longer simultaneously maximize between-group similarity. 
In this case, these two potential aims of anticlustering diverge.

Oftentimes, the available data does not represent pairwise similarity, 
but rather numeric, categorical or binary attributes [@dry2009]. That 
is, the data is a two-way two-mode [@arabie1992] table where rows 
represent the entities of study, and columns represent variables 
describing the elements. For example, numeric ratings may describe the 
attributes of word stimuli for psycholinguistic experiments. Such data 
may include Norms for imagery, concreteness and orthographic variables 
[@friendly1982]. In other cases, attributes are binary and may represent 
the presence or absence of a feature [@tversky1977]. Such data can be 
converted to pairwise dissimilarities for example by computing an 
appropriate dissimilarity measure, such as the pairwise Euclidean or 
squared Euclidean distance across a set of numeric attributes 
[@brusco2019]. Then, anticlustering criteria based on pairwise 
dissimilarity can be applied to partition the elements. 

A different approach to anticlustering directly works with feature based 
data and does not require a transformation into pairwise 
dissimilarities. Instead, the sum of the squared Euclidean distances 
between elements and the cluster they are assigned to is minimized 
[@brusco2006branch], most prominently using the k-means heuristic 
[@jain2010; @steinley2006]. Minimizing the k-means criterion 
simultaneously maximizes between-cluster separation and within-cluster 
homogeneity [@aloise2009np]. Späth [-@spath1986] and Valev [-@valev1998; 
-@valev1983] independently recognized that by optimizing the reversed 
k-means objective that is, maximizing the sum of distances between 
elements and cluster centroids -- a data set is partitioned into similar 
groups, thereby reversing the logic of classical clustering. They 
independently from each other coined the term anticlustering for this 
purpose. K-means anticlustering maximizes the sum of squared Euclidean 
distances from each element to the center of the cluster to which it 
belongs. As @spath1986 noted, k-means anticlustering thereby directly 
minimizes differences with regard to the means of the numeric variables 
between groups. Hence, groups only become similar with regard to the 
mean of the distribution of the numeric attributes, which may perform 
poorly as a measure of overall similarity; the spread of the data is not 
targeted and may differ between sets. @papenberg2020 showed that 
maximizing the diversity (based on the pairwise Euclidean distances) is 
better suited to minimize difference with regard to both the mean and 
the variance of the data; it was shown that even an entirely random 
split usually resulted in more similar standard deviations as k-means 
solely focuses on the mean. When requiring that groups are overall 
similar to each other, this focus may be detrimental as similar means 
can be obtained even when the underlying distributions are very 
different [@anscombe1973]. 

In this paper, I present a simple extension of the k-means objective 
function to maximize simililarity with regard to means and variances at 
the same time. The aim is to establish groups that are overall similar 
to each other. It is shown that this is achieved by formulating a 
bicriterion optimization objective for k-means anticlustering. 
Interestingly, under specific---and important---circumstances, this 
reformulation reduces to an augmentation of the original data matrix 
while the optimization objective remains unchanged. This insight has 
important consequences as it allows to adapt standard optimization 
schemes for single criterion anticlustering. I show that the extended 
k-means criterion outperforms other anticlustering criteria when the 
goal is the minimize difference with regard to the mean and variance of 
the different groups. In most cases, both objectives can be optimized
satisfactorily at the same time without any loss on either; when the 
data set is large enough (usually, when $N > XX$), the bicriterion 
optimization leads to > 99.99% objective performance as compared to the 
single optimization of either k-means or k-variance.

# Problem formalization

For the purpose of problem formalization, I adopt the notation Steinley 
(2006) provided in his thorough synthesis of a half-century literature 
on k-means. K-means clustering is used to partition two-way, two-mode 
data (that is, $N$ elements each having $P$ attributes) into $K$ groups 
$(C_1., \ldots, C_K)$ where $C_k$ is the set of $n_k$ objects in group 
$k$. The partitioning is subject to the requirements that each element 
is assigned to exactly one group, formalized by the two constraints

$$
\bigcup\limits_{k = 1}^{K} C_j = X
$$
$$
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k
$$

In anticlustering applications, we usually impose restrictions on 
the group sizes $n_k$, with the most common restriction being that all 
groups have equal size: 

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

In this paper, I will discuss only such applications where groups are of 
equal size.

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, 
k-means clustering aims to minimize the error sum of squares (SSE)---the 
within-group variance---which is the sum of the squared Euclidean 
distances between each data point and its cluster centroid. Conversely, 
k-means anticlustering aims to maximize the same objective. The cluster 
centroid is given by averaging the values on each variable over the 
elements within that cluster. The centroid of the *j*th variable in the 
*k*th cluster is given as: 

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

The k-means objective function is then given as

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

Note there is a direct connection between $SSE$ and the cluster 
centroids [@spath1986]. If the complete centroid vector for cluster 
$C_k$ is given by

$$
\overline{\mathbf{x}}^{(k)} = 
  (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})
$$

and the $SSE$ is minimal, the centroid values 
$\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as 
possible from the overall data centroid $\overline{\mathbf{x}} = 
(\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, 
\overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the 
overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $SSE$ is maximal---which is the goal of k-means 
anticlustering---the cluster centroids are as close as possible to the 
overall centroid $\overline{\mathbf{x}}$ and therefore to each other 
[@spath1986]. Thus, k-means anticlustering directly optimizes the 
similarity of the mean values of the $P$ attributes across the $K$ 
clusters. 

## K-Variances

**This is old, adjust to Steinley (2006):**

The squared distance between an element $x = (x_1, \ldots, x_M)$ and the 
average of all elements $\overline{\mathrm{\boldsymbol{x}}}$ is:

$$
\vert \vert x - \overline{\mathrm{\boldsymbol{x}}} \vert \vert^2 = 
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2
$$

Now define new data points representing the squared distances between 
data points and the grand mean:

$$ 
y_i = (x_i - \mathrm{\boldsymbol{x}})^2, \forall i \in N
$$

where $\mathrm{\boldsymbol{x}}$ is a vector of length $M$ containing 
the average for each feature.

Now:

$$
\sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
  \sum\limits_{m = 1}^{M}(y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2 =
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
    (y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2
$$

If we define $x_{m + 1} = y_1$, $x_{m + 2} = y_2$, ..., $x_{2M} = y_M$.
this is the same as:

$$
\sum\limits_{m = 1}^{2M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 
$$

Thus, we redefine the data points and include an additional $M$ features 
for each element. Then, we maximize $\mathit{Var}_{\mathrm{within}}$, 
which corresponds to a tradeoff between minimizing differences in mean 
features between clusters, and the average squared distance to the grand 
mean, i.e., the within-cluster variance. That is: We try to maximize the 
within-cluster variance, but to a certain degree we also strive to 
obtain similar within-cluster variance in each cluster.

## Criterion Tradeoff

```{r}

# Generate data and all partitions

set.seed(2207) # 2207

N <- 14
K <- 2
partitions <- generate_partitions(N, K)
features <- rnorm(N, 15, 3)

# Compute objectives for each partition
all_objectives_mean <- sapply(
  partitions,
  FUN = diff_mean,
  features = features
)

all_objectives_var <- sapply(
  partitions,
  FUN = diff_vars,
  features = features
)

df <- data.frame(
  diff_mean = all_objectives_mean,
  diff_var  = all_objectives_var
)

```

A motivating example illustrates that aiming to simultaneously optimize 
similarity of means and variances at the same time is not only 
desirable, but may also be feasible. I created 14 data points from a 
univariate normal distribution ($M = 15$, $SD = 3$) and generated all `r 
length(partitions)` possible ways to partition the 14 data points into 
$K = 2$ equal-sized groups. I computed the difference in the two means 
and two variances for each partition. This simple difference metric was 
chosen for purposes of interpretability; in fact, there is a a 1-to-1 
correspondence between these differences and $SSE$ and 
$SSE_{\mathrm{VAR}}$, respectively (Figure 1).[^onlyonegroup] Figure 2 
illustrates similarity in means plotted against similarity in variances 
across all partitions. Ideally, a partition is chosen that minimizes 
the difference with regard to means and variances (see the triangular 
symbol). However, optimal k-means anticlustering would create two groups 
that are not particularly similar with regard to their variance and 
chose the partition that is represented by the squared symbol. The 
diamond symbol represents that partition that where the two groups have 
the least difference in variances. The black symbols illustrate the 
other members of the optimal pareto set. 

[^onlyonegroup]: Note that this such a correspondence no longer holds 
when there are more than two groups or there is more than one 
numeric attribute (i.e., if $K > 2$ or $P > 1$).

```{r, fig.cap = "Illustrates the 1-to-1 relationship between SSE and difference in means (Panel A) and variance (Panel B). Each data point illustrates a partition, i.e., a way to divide a data set ($N = 14$) into two groups.", fig.height = 3.5}

# For each partition: Compute SSE objective, and SSE(VAR)
# SSE
all_objectives_kMeans <- sapply(
  partitions,
  FUN = var_objective,
  features = features
)

# SSE(VAR)
all_objectives_kVar <- sapply(
  partitions,
  FUN = var_objective,
  features = squared_from_mean(as.matrix(features))
)

# Plot relationship between objectives
par(mfrow = c(1, 2))
plot(
  all_objectives_mean, 
  all_objectives_kMeans, 
  xlab = "mean(f1) - mean(f2)",
  ylab = "SSE",
  pch = 4, 
  cex = .5,
  col = "darkgrey"
)
legend("topright", legend = "A", cex = 1.3, bty = "n")
plot(
  all_objectives_var, 
  all_objectives_kVar, 
  xlab = "var(f1) - var(f2)",
  ylab = "SSE_var",
  pch = 4, 
  cex = .5,
  col = "darkgrey"
)
legend("topright", legend = "B", cex = 1.3, bty = "n")
```

```{r, fig.width = 4.5, fig.height = 4.5, fig.cap = "All partitions. Points = pareto efficient set."}

# Plot SSE vs SSE(VAR)
plot(
  df, pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "mean(f1) - mean(f2)",
  ylab = "var(f1) - var(f2)"
)

# Illustrate Pareto set
pareto_pts <- pareto_set(df, sense = "min")
points(df[pareto_pts, ], pch = 19)

# Illustrate best partition wrt SSE (difference in means)
points(
  all_objectives_mean[which.min(all_objectives_mean)],
  all_objectives_var[which.min(all_objectives_mean)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

df$sum <- rowSums(df)

# Illustrate best partition wrt SSE(VAR) (i.e., difference in variance)
points(
  all_objectives_mean[which.min(df$sum)],
  all_objectives_var[which.min(df$sum)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt simililarity combined similarity / variance
points(
  all_objectives_mean[which.min(df$diff_var)],
  all_objectives_var[which.min(df$diff_var)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

## Bicriterion optimization model

```{r echo = FALSE}
# From Wikipedia on Multi-objective optimization:

# Without additional subjective preference information, all Pareto optimal 
# solutions are considered equally good. Researchers study multi-objective 
# optimization problems from different viewpoints and, thus, there exist 
# different solution philosophies and goals when setting and solving them. 
# The goal may be to find a representative set of Pareto optimal 
# solutions, and/or quantify the trade-offs in satisfying the different 
# objectives, and/or finding a single solution that satisfies the 
# subjective preferences of a human decision maker (DM). 

# The most preferred results can be found using different philosophies. 
# Multi-objective optimization methods can be divided into four 
# classes. In so-called no preference methods, no DM is expected to be 
# available, but a neutral compromise solution is identified without 
# preference information. The other classes are so-called a priori, a 
# posteriori and interactive methods and they all involve preference 
# information from the DM in different ways. 

# Solving a multi-objective optimization problem is sometimes understood 
# as approximating or computing all or a representative set of Pareto 
# optimal solutions.

# Scalarizing a multi-objective optimization problem is an a priori 
# method, which means formulating a single-objective optimization problem 
# such that optimal solutions to the single-objective optimization problem 
# are Pareto optimal solutions to the multi-objective optimization 
# problem. In addition, it is often required that every Pareto optimal 
# solution can be reached with some parameters of the scalarization.

```


- pragmatic solution [easy and practical approach, @marler2010; 
@naidu2014]: formalize the weighted-sum objective and use the 
weighted-sum approach. Then check out whether the simplest version is 
sufficient in this case (but see the problems of weighted sum approach 
in several contributions by Brusco et al.). Refer to @marler2010 to 
argue for weighted sum for the current problem

- it should be noted that both mathematical objectives capture different 
aspects of an underlying substantive objective, that is to maximize 
similarity of data distributions between different groups

- Weighted sum approach works well on this specific problem [although in 
general there are caveats to the weighted-sum method, see @brusco2019]. 
is fast, good results. in fact, the objectives seem to work well 
together

- weighted sum approach generates a single solution from the 
(approximated) pareto set

- In this paper: Formalization of the objective function, recognition
that this formulation does not change the basic structure of the k-means
objective, because it reduces to augmentation of the data input

- not interested in the Pareto efficient set! Because: Overall 
similarity should be maximized, which requires that both criteria are 
satisfied to a strong degree (not interested in just maximizing 
similarity wrt variance; this may be interesting in other applications). 
Instead: A tradeoff is possible, and in many cases, simultaneous 
optimization will meet both criteria almost as well as optimizing just
either criterion. To justify this approach: show the pareto front, which 
is not a diagonal / circle [see @brusco2012] but a RIGHT TRIANGLE! (this 
is probably because this is *anti*clustering!)

- Evaluation will consist of a comparison of different objectives based 
on the same (simple) exchange algorithm

### Weight selection

- How to select weights? (Oftentimes: for equal weights, the results 
are good)

- Preference to weight [e.g., @marler2010]. However, the relationship 
between weights and outcome is not always straightforward and the 
results may disappoint. On the one hand, vastly different weights need 
not lead to different solutions; and even equal weights can be 
sufficient if the solution space offers partitions that maximize both 
objectives at the same time. "the solution may not preserve one’s
initial preferences no matter how the weights are set"; "even if one 
determines acceptable values for the weights a priori, the final 
solution may not accurately reflect initial preferences" [@marler2010, 
p. 860]. 

- Standardization of the extended data matrix to ensure that the two
criterion functions "do not differ markedly in scale" [@brusco2019]. 
See Figure 1: the objectives do differ in scale, because k-variance 
is based on a quadratic distance and therefore has larger values.

- Using variable weighting (see Steinley, 2006, p. 13), we can 
adjust the relative importance of minimizing differences in means
and variances. Standard in `anticlust`: Equal weight, this is 
achieved by first creating the new variables, representing 
the squared differences from the mean, and then standardizing
the entire feature matrix. If $N$ is large enough (relative to $K$),
there is usually no need for weighting.

# Evaluation 

- Simulation 1: Show that k-means extended obtains 99.999% of the 
k-means and k-variance objective, on average. 
  + New objective seems to work best when N is high relative to K; optimizing 
two objectives at the same time is hard when N is low. Easier when only 
optimizing one objective (i.e., the mean, as done by normal k-means 
anticlustering). What I can say: When N / K > X, the objective is 
usually 99.99% of both objectives.
  + Two examples: $K = 2$, $N = 20$ vs. $N = 100$, using classical
k-means: only mean; using k-variance: only variance; combination:
both, but works better for larger $N$

- Simulation 2: show that k-variance anticlustering outperforms 
other objectives with regard to similarity in means / variances 
(in particular: anticluster editing)

- Often, maximizing k-means-variance yields more similar variances 
than k-variance alone. Why is that? I guess: structure in the solution 
space, local maxima are real. When also considering means, you more 
often escape bad local maxima? The finding is: k-means-extended better 
minimizes the maximum difference between the variance across groups 
than k-variance (for K > 2), which in the eye of the researcher, may 
be a more intuitive criterion.

# Examples

- use some examples (e.g., OASIS data set, 900 items allocated to 10 
sets, same mean and variance across the 10 sets)

# Discussion

- Focus on presentation of new objective function, the bicriterion 
optimization process might be improved [e.g., by using a direct 
algorithm as @brusco2019 did]

- Future research: Application of more sophisticated bicriterion 
optimization algorithms are interesting

- new criterion: optimizes between-group similarity, no correspondence 
to within-group heterogeneity [see @brusco2019 for a exceptional method 
maximizing within-group heterogeneity]

- Similarly: the reversed objective no longer has a useful clustering 
interpretation: Maximizing differences in variances does not lead to 
homogeneous and/or well separated clusters

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
