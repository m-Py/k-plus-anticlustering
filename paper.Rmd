---
title             : "k-plus: a bicriterion extension of k-means anticlustering"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

#  If the study extends or replicates previous research, cite the relevant work with an author–date citation.

abstract: |

    Anticlustering refers to a collection of methods that partition data sets into disjunct groups with two primary goals: (a) high between-group similarity and (b) high within-group heterogeneity. Some anticlustering objectives simultaneously pursue both goals while other methods focus on a particular aspect. The present paper introduces a novel bicriterion objective aiming at optimizing between-group similarity. *K-plus anticlustering* is an extension of k-means anticlustering, which minimizes the difference between mean values of numeric attributes across clusters. The k-plus objective extends k-means by including a term that aims to optimize similarity with regard to the variance of data across groups, which is ignored by k-means. Surprisingly, it is shown that the combined objective (i.e., the bicriterion) can be reduced to an augmentation of the input data, leaving the original k-means criterion unchanged. Tests on simulated and real data show that k-plus anticlustering usually achieves high similarity both with regard to mean attribute values as well as the variance, with no noticable sacrifice to either criterion. The k-plus extension is therefore preferred over classical k-means anticlustering for optimizing between-group similarity.
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, bicriterion optimization"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE) 

source("misc-functions.R")

```

Anticlustering is a collection of methods to partition elements into disjunct groups, with the goal of obtaining high between-group similarity or within-group heterogeneity [@brusco2019; @spath1986; @valev1998]. Oftentimes, both goals coincide: several anticlustering objectives imply that similarity between groups is maximal whenever similarity within groups is minimal [@brusco2019; @papenberg2020; @feo1990]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and heterogeneity between clusters [@rokach2005; @steinley2006]. Whereas a variety of different approaches exist for cluster analysis such as hierarchical methods, model-based clustering, or network approaches [see @brusco_emergent_2012, for an overview], anticlustering is usually formalized as a partitioning process that divides a set of elements into disjunct groups in such a way that an objective function---representing between-group similarity and/or within-group heterogeneity---is optimized [@papenberg2020; @spath1986; @brusco2019].

Anticlustering is useful in many tasks surrounding research work in psychology. For example, anticlustering can be used to split a test into parts of equal difficulty [@gierl2017], to assign students to work groups [@baker2002] or when assigning stimuli to experimental conditions [@lahl2006]. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually suboptimal. Fortunately, these problems can be formalized as mathematical optimization problems [e.g., @brusco2019; @spath1986; @baker2002; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. For a general overview of anticlustering approaches and applications in Psychology, see @brusco2019 and @papenberg2020.

Anticlustering objectives differ in the assumed input data and their mathematical definition of cluster similarity. Some anticlustering objectives employ pairwise dissimilarity ratings to compute a total measure of cluster (dis)similarity. The most prominent criterion of this kind is the *diversity*, which is the total sum of dissimilarities between any elements within the same group. Maximizing the diversity simultaneously optimizes within-group heterogeneity as well as between-group similarity. Another important criterion based on pairwise dissimilarities is the *dispersion*, i.e., the minimum dissimilarity between any two elements within the same group [@fernandez2013; @brusco2019]. Maximizing the dispersion increases the within-group heterogeneity by ensuring that any two elements as dissimilar from each other as possible.

Oftentimes, the data available to researchers does not represent pairwise similarity, but numeric, categorical or binary attributes [@dry2009]. That is, the data is a two-way two-mode [@arabie1992] table where rows represent the entities of study, and columns represent variables describing the elements. For example, numeric ratings may describe the attributes of word stimuli for psycholinguistic experiments. Such data may include norms for imagery, concreteness and orthographic variables [@friendly1982]. In other cases, attributes are binary and may represent the presence or absence of a feature [@tversky1977]. In the context of anticlustering, One possible approach for handling two-way two-mode data is to compute an appropriate dissimilarity measure such as the pairwise Euclidean or squared Euclidean distance across a set of numeric attributes [@brusco2019]. Then, anticlustering criteria based on pairwise dissimilarities such as the dispersion or diversity can be optimized to partition the elements into groups.

A different approach to anticlustering directly works with the two-way two-mode feature data using the k-means criterion. K-means is probably the best-known clustering method and its reversal has been recognized as a useful anticlustering tool by Späth [-@spath1986] and Valev [-@valev1998; -@valev1983]. In k-means clustering, the sum of the squared Euclidean distances between elements and the center of the cluster they are assigned to is minimized [@brusco2006branch], usually using the k-means heuristic [@jain2010; @steinley2006]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. For the anticlustering application, the k-means criterion is maximized instead, thereby achieving similarity between groups. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. Hence, groups are similar with regard to the mean of the distribution of the numeric attributes wheras other parameters---such as the variance---are ignored. When requiring that groups are overall similar to each other, this focus may be misguided as similar means can be obtained even when the underlying distributions are different [@anscombe1973]. @papenberg2020 showed that maximizing the diversity (based on the pairwise Euclidean distances) is better suited to minimize difference with regard to both the mean and the variance of the data; it was shown that even an entirely random split usually resulted in more similar variances because k-means solely focuses on the mean similarity.

To optimize overall group similarity---and hence to overcome the limitations of k-means anticlustering---I present k-plus anticlustering in this paper. The k-plus criterion is an extension of the k-means objective that includes a term aiming at optimizing similarity with regard to the variance of data across groups. Interestingly, this bicriterion reformulation can be understood as an augmentation of the original data matrix, leaving the original k-means objective unchanged. I show that the k-plus criterion outperforms other anticlustering criteria when the goal is the minimize difference with regard to the mean and variance of the different groups. In most cases, both objectives can be optimized satisfactorily at the same time without any loss on either. In several examples I illustrate how readers can use k-plus anticlustering easily using the free and open source software anticlust, which is an extension to the statistical programming language R [@papenberg2020].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means clustering (and anticlustering) is used to partition two-way, two-mode data into groups. That is, $N$ elements each having $P$ attributes are assigned to $K$ groups $(C_1., \ldots, C_K)$, $C_k$ being the set of $n_k$ objects in group $k$. The partitioning is subject to the requirement that each element is assigned to exactly one group, formalized by the following two constraints:

$$
\bigcup\limits_{k = 1}^{K} C_j = X
$$

$$
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k
$$

In anticlustering applications, we impose an additional constraint on the group sizes $n_k$, where the most common restriction requires that all groups have equal size:

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares (SSE), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging the $k$th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note there is a direct connection between the $SSE$ and the location of the cluster centroids [@spath1986]. If the $SSE$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $SSE$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values across clusters.

## Quantifying differences in variance

In the following, I present a rather straight-forward adaption of the $SSE$ to quantify how the variance of numeric data differs between groups. To motivate the new criterion, we first consider a set of one-dimensional observations $(x_1, \ldots, x_N)$. The extension to the general case of $P$ attributes will be straight-forward.

The sample variance of a set of observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
y_i = (x_i - \overline{\mathbf{x}})^2
$$

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average, k-means anticlustering can be employed to equalize the variance across groups. That is, we can formulate an adaption of the $SSE$ to quantify the degree to which the variance differs between groups. First, however, we generalize the idea to the case of $P$ attributes. For each attribute, we have to compute the squared deviation between all values and the mean of the attribute. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$, where

$$
y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2.
$$

To reflect how strongly the variances of all attributes vary across the $K$ clusters, we now define the criterion $\mathit{SSE_{Var}}$:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Analogously to the standard $SSE$, the centroid values $\overline{y}_j ^{(k)}$ are given as:

$$
\overline{y}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} y_{ij}
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points.

## A bicriterion model: K-plus anticlustering

It is unlikely that an exclusive optimization of $SSE_{Var}$ (i.e., equalizing variances) is of interest without also considering the $SSE$ (i.e., equalizing means). A combination of $SSE_{Var}$ and $SSE$, however, can be employed to simultaneously optimize both objectives, which is a reasonable idea when striving for overall cluster similarity. The most simple approach for such a bicriterion optimization is the weighted sum approach [@naidu2014; @marler2010]. That is, both criteria are computed independently from each other and then combined into a single criterion through a weighted sum:

$$
\mathit{SSE_{kplus}} = w_1 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{x}_j ^{(k)})^2 + 
  w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{y}_j ^{(k)})^2
$$

The weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of 
equalizing means and variances, respectively. Often, they are chosen such that 
$w_1 + w_2 = 1$ [@brusco2019].

Some simplifications can be arranged when ignoring the weights $w_1$ and $w_2$, e.g., by setting them both to 1. When defining $z_i = (x_{i1}, \ldots, x_{iP}, y_{i1}, \ldots, x_{iP})$, i.e., the concatenation of the $x_{ij}$ and $y_{ij}$ values, the bicriterion $\mathit{SSE_{kplus}}$ is then computed as

$$
\mathit{SSE_{kplus}} =
  \sum\limits_{j=1}^{2 P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (z_{ij} - \overline{z}_j ^{(k)})^2
$$

This formula again has the same form as $SSE$ (and $SSE_{Var}$), but is computed on different data. In this case, both the original data $x_{ij}$ as well as the squared distances $y_{ij}$ are used as attributes that enter the objective function. That is, computing the k-plus criterion can be done by first computing the values $y_{ij}$, appending these to the original data matrix, and then computing the $SSE$ on the augmented data matrix. Hence, k-plus anticlustering reduces to an augmentation of the data matrix, leaving the original k-means criterion unchanged.

Note that a problem may arise if the weights $w_1$ and $w_2$ are ignored when optimizing the k-plus objective. Since the computation of the data matrix $\mathbf{Y}_{N \times P}$ involves squaring, the squared distances $y_{ij}$ have much larger values, on average, than the raw values $x_{ij}$ (also see Figure 1). Therefore, the $SSE_{Var}$ criterion naturally receives greater weight when computing $\mathit{SSE_{kplus}}$ than the standard $SSE$ criterion. This imbalance is most likely unwarranted because optimizing similarity with regard to variances is not more important than optimizing similarity with regard to means. A solution is to standardize all values $x_{ij}$ and $y_{ij}$ to the same scale before computing $SSE_{kplus}$, e.g. via $z$-standardization.

## Criterion tradeoff

```{r}

# First rendering is slow because the data needs to be generated
path <- "partition_objectives.csv"

if (!file.exists(path)) {
  set.seed(2207) # 2207
  # Generate data and all partitions
  N <- 14
  K <- 2
  partitions <- generate_partitions(N, K)
  features <- rnorm(N, 15, 3)

  # Compute objectives for each partition
  # For each partition: Compute SSE objective, and SSE(VAR)
  # 1. SSE
  all_objectives_kMeans <- sapply(
    partitions,
    FUN = var_objective,
    features = features
  )
  
  squared_distances_from_mean <- squared_from_mean(as.matrix(features))
  
  # 2. SSE(VAR)
  all_objectives_kVar <- sapply(
    partitions,
    FUN = var_objective,
    features = squared_distances_from_mean
  )
  
  # 3. SSE(KPLUS)
  # Use standardization to compute k-plus criterion!
  all_objectives_kPlus <- sapply(
    partitions,
    FUN = var_objective,
    features = scale(cbind(features, squared_distances_from_mean))
  )
  
  df <- data.frame(
    kmeans = all_objectives_kMeans,
    kvar = all_objectives_kVar,
    kplus = all_objectives_kPlus
  )

  write.table(
    df, path, row.names = FALSE, sep = ","
  )
} else {
  df <- read.csv(path)
}

```

When optimizing a bicriterion objective such as $SSE_{kplus}$, the ideal outcome would be to find a partition where both the means and the variances are very similar between groups, i.e., a which partition where both the standard $SSE$ and $SSE_{Var}$ are close to the global optimum value. Generally, research on multi-objective optimization has shown that cases are rare where a single solution satisfies all criteria to an optimal degree [@ferligoj1992direct]. Usually, a tradeoff is necessary. For anticlustering, a motivating example illustrates that aiming to simultaneously optimize similarity of means and variances is not only desirable, but may actually be feasible.

```{r, fig.cap = "The relationship between the objectives $SSE$ and $SSE_{Var}$ across all partitions (for $N = 14$ and $K = 2$). The partition represented by the triangular minimizes the difference in variances between groups; the partition represented by the square minimizes the difference in means between groups; the partition represented by the diamond maximizes $SSE_{kplus}$ and represents a good tradeoff, satisfying both criteria to an almost optimal degree.", fig.height = 5, fig.width = 5}

# Plot SSE vs SSE(VAR)
plot(
  df[, c("kmeans", "kvar")], 
  pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "SSE",
  ylab = "SSE_Var"
)

# Illustrate best partition wrt difference in means (i.e., SSE)
points(
  df$kmeans[which.max(df$kmeans)],
  df$kvar[which.max(df$kmeans)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt difference in variance (i.e., SSE(VAR))
points(
  df$kmeans[which.max(df$kvar)],
  df$kvar[which.max(df$kvar)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt k-plus criterion
points(
  df$kmeans[which.max(df$kplus)],
  df$kvar[which.max(df$kplus)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

For this motivating example, I created 14 data points following a univariate normal distribution ($M = 15$, $SD = 3$) and then generated all `r n_partitions(14, 2)` possible ways to partition the 14 data points into $K = 2$ equal-sized groups. Figure 1 shows the scatterplot of the criteria $SSE$ and $SSE_{Var}$ across partitions. Note that for both criteria, a larger value is better, i.e., indicates higher similarity with regard to means and variances, respectively. Therefore, to obtain overall between-group similarity, a partition should be chosen that is depicted in the upper right corner of the scatterplot. However, optimal k-means anticlustering would create two groups that are not particularly similar with regard to their variance. The partition that is represented by the squared symbol has the overall best highest $SSE$, but is outperformed by many other partitions with regard to $SSE_{Var}$. Similarly, the partition that maximizes $SSE_{Var}$ (the triangular symbol in Figure 1) is not well-suited to optimize SSE at the same time. The diamond symbol represents the partition that maximizes the unweighted $SSE_{kplus}$, i.e., the sum of $SSE$ and $SSE_{Var}$. This partition is located in the right upper corner and yields `r df$kmeans[which.max(df$kplus)] / max(df$kmeans) * 100`% of the global maximum $SSE$, and `r df$kvar[which.max(df$kplus)] / max(df$kvar) * 100`% of the global maximum value of $SSE_{Var}$.

This initial example shows that maximizing the combined k-plus criterion may well be suited to optimize both similarity with regard to means as well as variances between groups. Later, I will show that this result also holds for real data sets, using more groups and a larger number of numeric features.

```{r}


```

# Evaluation 

- Simulation 1: Show that k-means extended obtains 99.999% of the 
k-means and k-variance objective, on average. 
  + New objective seems to work best when N is high relative to K; optimizing 
two objectives at the same time is hard when N is low. Easier when only 
optimizing one objective (i.e., the mean, as done by normal k-means 
anticlustering). What I can say: When N / K > X, the objective is 
usually 99.99% of both objectives.
  + Two examples: $K = 2$, $N = 20$ vs. $N = 100$, using classical
k-means: only mean; using k-variance: only variance; combination:
both, but works better for larger $N$

- Simulation 2: show that k-variance anticlustering outperforms 
other objectives with regard to similarity in means / variances 
(in particular: anticluster editing)

- Often, maximizing k-means-variance yields more similar variances 
than k-variance alone. Why is that? I guess: structure in the solution 
space, local maxima are real. When also considering means, you more 
often escape bad local maxima? The finding is: k-means-extended better 
minimizes the maximum difference between the variance across groups 
than k-variance (for K > 2), which in the eye of the researcher, may 
be a more intuitive criterion.

# Applying k-plus anticlusting using the R package anticlust

In this section I demonstrate how researchers can easily employ k-plus anticlustering using the R package anticlust [@papenberg2020]. The results presented here used anticlust version 0.5.6. The package can be installed in the R environment using the `install.packages()` command:

```R
install.packages("anticlust")
```

In this application, I use norming data for the OASIS image data set that is freely available online [@kurdi2017introducing]. @kurdi2017introducing assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense collected an additional rating dimension by measuring how the same 900 images were rated with regard to their beauty. In my application, I use all three features, currently available from Github.[^githubbeauty]

[^githubbeauty]: https://github.com/aenneb/OASIS-beauty

I used k-plus anticlustering to divide the data set of 900 images into 9 groups of 100 images each; this process takes about 3-5 seconds on a contemporary personal computer, depending on how many iterations are needed to find the local maximum. The task was accomplished using the following R code:

```R
library(anticlust) # load the package anticlust

anticlustering(
  features,
  standardize = TRUE,
  K = 9,
  objective = "kplus",
  method = "local-maximum"
)
```

Here, the variable `features` has to be a data table containing the image data: 900 rows representing images; 3 columns representing the features beauty, arousal, and valence. The accompanying OSF repository contains the fully reproducible code that includes downloading and reading the OASIS data, selecting the relevant columns, and finally storing them in the `features` variable (**TODO**). The arguments `objective = "kplus"` and `method = "local-maximum"` ensure that the k-plus criterion is optimized using the local maximum search method described in the previous section (**TODO**). The argument `K` describes the number of groups. The argument `standardize = TRUE` ensures that all features are standardized before the optimization starts; in particular, this option enforces that equalizing means and variances receives the same weight during the optimization process.

Table 1 shows the results of the anticlustering application by listing the descriptive statistics (means and standard deviations) for each of the 9 groups and for each of the 3 features. Additionaly, Table 1 shows the descriptive statistics for an application of k-means anticlustering and anticluster editing (which maximizes the diversity criterion). It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations. Anticluster editing was also well-suited to match both means and standard deviations, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---anticluster editing does not directly maximize similarity with regard to the means and standard deviations.

```{r}

path <- "results_oasis_same_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- 9
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
      features,
      standardize = TRUE,
      K = K,
      objective = criterion,
      method = "local-maximum"
    )
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(
      1:K,
      nrow(features) / K,
      mean_sd_tabs[[criterion]]
    )
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Anticluster editing}" = 19:27)
)
```

Table 2 illustrates another advantage of k-plus anticlustering over anticluster editing. When creating groups of unequal size, anticluster editing tends to increase the spread of the data in the largest group in comparison to the other groups. K-plus anticlustering strives for equal means and variances in all groups, regardless of group size.

Unequal group sizes can be requested in anticlust by passing the group sizes as a vector to the `K` argument:

```R
anticlustering(
  features,
  standardize = TRUE,
  K = c(100, 100, 100, 600),
  objective = "kplus",
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
      features,
      standardize = TRUE,
      K = K,
      objective = criterion,
      method = "local-maximum"
    )
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Anticluster editing}" = 9:12)
)
```


# Discussion

- Focus on presentation of new objective function, the bicriterion 
optimization process might be improved [e.g., by using a direct 
algorithm as @brusco2019 did]

- Future research: Application of more sophisticated bicriterion 
optimization algorithms are interesting

- new criterion: optimizes between-group similarity, no correspondence 
to within-group heterogeneity [see @brusco2019 for a exceptional method 
maximizing within-group heterogeneity]

- Similarly: the reversed objective no longer has a useful clustering 
interpretation: Maximizing differences in variances does not lead to 
homogeneous and/or well separated clusters

- extending on covariances?

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
