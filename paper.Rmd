---
title             : "Extending the Bicriterion Approach for Anticlustering: Optimal and Heuristic Approaches"
shorttitle        : "Extending the Bicriterion Approach for Anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich-Heine-Universität Düsseldorf, Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"
  - name          : "Martin Breuer"
    affiliation   : "1"
    corresponding : no
  - name          : "Max Diekhoff"
    affiliation   : "1"
    corresponding : no
  - name          : "Gunnar W. Klau"
    affiliation   : "1"
    corresponding : no

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"
    

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf. Martin Breuer, Gunnar W. Klau, Max Diekhoff, 
  Department of Computer Science, Heinrich Heine University Düsseldorf.
  
abstract: |

    Numerous applications in psychological research require that a data set is partitioned via the maximization---instead of the more common minimization---of a clustering criterion. Such anticlustering seeks for high similarity between groups (i.e., maximum diversity) or high pairwise dissimilarity within groups (i.e., maximum dispersion). Brusco et al. (2020) proposed a powerful bicriterion algorithm (BILS) that simultaneously seeks for maximum diversity and dispersion, introducing the bicriterion approach for anticlustering. Here, we advance the bicriterion approach in several ways. As our main contribution, we combine a novel exact algorithm for maximum dispersion with the BILS heuristic to obtain high diversity on top of an optimal dispersion. Despite its theoretical computational hardness, our optimal method scales to rather large data sets. It generates multiple partitions that we use as initializations of the BILS's local search, thus guaranteeing that the BILS also returns a partition with optimal dispersion. In a simulation study, we compared several adaptations of the BILS. To obtain high diversity, initializing the BILS with multiple optimal partitions is preferable to using only one. Moreover, in data sets where maintaining optimal dispersion severely restricts the search for feasible partitions, the iterated local search phase of the BILS is crucial to obtain high diversity. While the original BILS oftentimes finds optimal dispersion values, our simulation illustrates conditions when it does not. The optimal dispersion algorithm and the BILS including our extensions are available via the free and open source R package `anticlust`. Practical examples are included to illustrate their application.

bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, maximum dispersion, optimal algorithm, bicriterion optimization"
wordcount         : "12345"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

output            : 
  papaja::apa6_pdf

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \floatstyle{plaintop}
  - \restylefloat{figure}
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - |
    \makeatletter
    \renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
      \hskip -\arraycolsep
      \let\@ifnextchar\new@ifnextchar
      \array{#1}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

---

```{r setup, include = FALSE}
library(papaja) # use the development version: `remotes::install_github("crsh/papaja@devel")`; currently 0.1.1.9001
library(anticlust)
library(knitr)
# library(dplyr)
# library(ggplot2)
# library(tidyr)
# library(DescTools)
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library(prmisc) # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```


Partitioning data sets via anticlustering has uses in many research fields including psychology [@brusco2019; @schaper2023], test assembly [@gierl2017], education [@krauss2013; @baker2002], artificial intelligence [@steghofer2013], machine learning [@mauri2023], network systems [@mohebi2022], and operations research [e.g. @gallego2013; @gliesch2021]. In psychological research, an important application is the assignment of stimuli to different experimental sets that are presented in within-subjects designs: The different sets should be as similar as possible with regard to variables that affect the responses, and presenting the same set repeatedly is usually prohibited due to carry-over effects. Anticlustering is a powerful tool for automatically solving such tasks, which are time-consuming and usually inadequately handled manually [@lahl2006; @lintz2021; @cutler1981]. The first introduction to anticlustering in a Psychological journal was provided by @brusco2019. @papenberg2020 presented the free and open source software package `anticlust` that implements many algorithms for anticlustering, thus providing researchers with a tool for solving a problem that frequently occurs in psychological research, but previously lacked an appropriate software solution.

Formally, anticlustering is used to partition $N$ objects into $K$ exhaustive and mutually exclusive subsets $C = (C_1, \ldots, C_K)$. The partitioning restrictions are formalized as follows: 

\begin{align}
\vert C_k \vert = n_k, \; (k = 1, \ldots, K) \label{formalization:1} \\
\sum_{k = 1}^{K} n_k = N \label{formalization:2} \\
C_j \cap C_k = \emptyset (j, k = 1, \ldots, K), \; j \ne k\label{formalization:3}
\end{align}

$C_k$ is the set of $n_k$ objects in the $k^{th}$ group, and $K$ and $n_k$ are predetermined by the application. We will refer to the subsets $C_k$ ($k = 1, \ldots, K$), as anticlusters, clusters, or groups interchangeably, and to the collection $C$ as the anticlustering partitioning. 

Anticlustering seeks for similarity between groups and heterogeneity within groups by maximizing a clustering objective criterion. Thus, anticlustering is the logical and mathematical opposite of classical clustering, which partitions data sets via minimization of the same criteria [@steinley2006; @brusco2019; @spath1986]. We assume that a data matrix $\mathbf{Z}$ is used as input for computing the anticlustering criterion on the basis of a partitioning $C$. $\mathbf{Z}$ is either a feature matrix $\mathbf{X}_{N \times M} = \{x_{ij}\}_{N \times M}$ where the $M$ columns contain numeric measurements on the $N$ objects, or a symmetric matrix $\mathbf{D}_{N \times N} = \{d_{ij}\}_{N \times N}$ of pairwise dissimilarity measurements. The partitioning is obtained by maximizing a clustering criterion $f_i(C, \mathbf{Z})$. For example, *k*-means anticlustering aims to maximize between-group similarity by maximizing the error sum of squares, i.e., the sum the squared distances between each object and its cluster center [@spath1986; @steinley2006]:

$$
f_1(C, \mathbf{X}_{N \times M}) =
  \sum\limits_{j=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

The $k^{th}$ cluster center $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ contains the mean values of the $M$ variables computed across all observations belonging to the $k^{th}$ group. Maximizing $f_1$ ensures that the cluster centers are as close to each other as possible [@spath1986]. 

The *k*-plus criterion is an extension of the $k$-means criterion for anticlustering that can be implemented by adding fictitious variables to the data matrix $\mathbf{X}_{N \times M}$ [@papenberg2024]. *K*-plus anticlustering is used to equate not only means between clusters, but higher order moments such as the variance or skewness as well. For example, to equate means and variances, we define a new data matrix $\mathbf{Y}_{N \times M} = \{y_{ij}\}_{N \times M}$ with $y_{ij} = (x_{ij} - \overline{x}_j)^2$, where $\overline{x}_j$ is the overall mean of the $j^{th}$ attribute. Then, the *k*-plus criterion is computed as

$$
f_2(C, \mathbf{(X | Y)_{N \times 2M}}) =
  \sum\limits_{j=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Another important anticlustering criterion is the *diversity*, which uses a dissimilarity matrix $\mathbf{D}_{N \times N}$ as input. It is computed as the within-group sum of pairwise dissimilarities across all clusters [@brusco2019; @papenberg2020; @feo1990]:
 
$$
f_3(C, \mathbf{D}_{N \times N}) = \sum\limits_{k = 1}^{K} \sum_{(i < j) \in C_k} d_{ij}
$$

Maximizing the diversity $f_3$ simultaneously minimizes the sum of distances between objects not in the same group. Therefore, the diversity represents within-group heterogeneity as well as between-group similarity [@feo1990]. Being a sum instead of an average, however, it no longer adequately captures between-group similarity if group sizes are not equal [@papenberg2024]. 

The diversity is oftentimes computed by first converting a feature matrix $\mathbf{X}_{N \times M}$ into a matrix of pairwise Euclidean distances [@brusco2019; @papenberg2020; @papenberg2024; @gallego2013]. If $\mathbf{D}_{N \times N}$ is obtained by using the squared pairwise Euclidean distances instead, we note an important equivalence between the k-means criterion $f_1$ and the diversity $f_3$. Traditionally, the k-means criterion is denoted via a sum of squared distances between data points and cluster centroids. However, it can also be expressed using the pairwise squared Euclidean distances between data points (Brusco, 2006; Späth, 1980). If we let $\mathbf{D'}_{N \times N}$ = \{$d'_{ij}$\} represent the squared Euclidean distance, i.e., $d'_{ij} = \sum\limits_{m = 1}^{M}(x_{im} - x_{jm})^2$, the k-means criterion can be expressed as $f_4$:

$$
f_4(C, \mathbf{D'}_{N \times N}) = 
  \sum\limits_{k = 1}^{K}\left( \frac{1}{n_k} \sum_{(i<j)\in C_k} d'_{ij} \right) = 
  f_1(C, \mathbf{X}_{N \times M}).
$$

That is, "the sum of squared distances of a collection of points from the centroid associated with those points is equal to the sum of the pairwise squared distances between those points divided by the size (number of objects) of the collection" (Brusco, 2006, p. 350). An important special case of anticlustering applications is that all groups are equal-sized, i.e., $n_k = \frac{N}{K}$ ($k = 1, \dots K$). In this context, $f_4$ can be simplified to $f_4^*$ via

$$
f_4^*(C, \mathbf{D'}_{N \times N}) = 
  \frac{K}{N} \sum\limits_{k = 1}^{K} \sum_{(i<j)\in C_k} d_{ij}
$$

In the context of maximization, the factor $\frac{K}{N}$ can be ignored because it does not depend on the partitioning $C$.  

$$
f_4^*(C, \mathbf{D'}_{N \times N})
  \propto \sum\limits_{k = 1}^{K} \sum_{(i<j)\in C_k} d_{ij} = f_3(C, \mathbf{D'}_{N \times N})
$$

Therefore, the criterion $f_4^*$ (i.e., the k-means criterion for equal-sized groups) reduces to the standard diversity $f_3$---if the diversity is computed via the squared Euclidean distance. We will use this insight later when we discuss applying algorithms for maximum diversity to the $k$-means and $k$-plus objectives. For arbitrary measures of dissimilarity (and not just the squared Euclidean distance), we denote $f_4$ as the *average diversity* criterion.

Unlike $f_1$, $f_2$, $f_3$, and $f_4$, the *dispersion* criterion does not represent between-group similarity, but is a pure measure of within-group heterogeneity. It is defined as the minimum dissimilarity across any two objects that are part of the same group [@fernandez2013]:

$$
f_5(C) = \textrm{min}_{(i < j) \in C_k} d_{ij}
$$

The dispersion is a measure of the worst-case pairwise dissimilarity [@brusco2019]. Maximizing the dispersion ensures that any two objects in the same group are as dissimilar from each other as possible, which is desireable when striving for high within-group heterogeneity.

@brusco2019 outlined the importance of simultaneously considering multiple criteria when addressing anticlustering problems. In particular, they argued that anticlustering algorithms should incorporate an objective of between-group similarity such as the diversity, and pairwise within-group dissimilarity, i.e., the dispersion. To this end, @brusco2019 presented a bicriterion algorithm (*BILS*) that simultaneously maximizes the diversity $f_3$ and the dispersion $f_5$ by approximating the Pareto efficient set of partitions according to both criteria. By investigating the Pareto set for many problem instances, they found that a considerable improvement in one of these criteria is often possible with little or even no sacrifice in the other. We endorse Brusco et al.'s bicriterion approach for anticlustering and extend it in several ways. First, we allow that not only the diversity $f_3$ can be optimized alongside the dispersion, but we also allow that the k-means, k-plus or the average diversity can be employed as measures of between-group similarity during the bicriterion optimization (i.e., the criteria $f_1$, $f_2$ and $f_4$). Second, we allow that the dispersion can be optimized on the basis of a different dissimilarity matrix than the diversity (or a different objective of between-group similarity). This is useful if the objectives should be based on different information, or, as we discuss below, to induce custom pairwise cannot-link constraints. As our most important contribution, we present a novel exact algorithm for maximum dispersion and investigate different algorithmic techniques for providing high diversity on top of it. All of the methods newly developed here as well as the original BILS algorithm are available via the free and open source `R` package `anticlust` [@papenberg2020; @R-anticlust].

Our paper is organized as follows. In the next section, we discuss algorithms for maximizing anticlustering criteria. In particular, we review the bicriterion algorithm by @brusco2019 in detail. After that, we provide the technical background for our own extensions of the bicriterion approach. We continue by evaluating these contributions: (a) we probe the practical feasibility of our optimal algorithm for maximum dispersion for varying $N$ and $K$; (b) we compare different algorithmic approaches in their ability to provide high diversity on top of an optimal dispersion; (c) we apply our new methods to real norming data and present functioning `R` code to reproduce the examples [@R-base].

## The BILS

## Algorithms for anticlustering

Heuristics dominate (local maximum search @spath1986; @weitz1998, multiple repetitions robust for k-means @steinley2007, ILS @brusco2017, VARIABLE NEIGHBORHOOD SEARCH @urovsevic2014; tabu search @gallego2013; multiple improvement phases based on initial local maximum search @yang2022). Exact algorithms for diversity [@papenberg2020; @schulz2022] and for the dispersion [@fernandez2013; @gliesch2021; @brucker1978]. Exact algorithms for diversity perform much worse than for the reversed clustering problems, where many hundred elements can often be processed. Maximum diversity can be achieved with much better success. Our novel contribution is to combine optimal maximum dispersion with high diversity. We present a novel algorithm for maximum diversity that is similar to the approaches by @fernandez2013 and @gliesch2021 and is based on the logic that was already introduced by @brucker1978. It scales to hundreds or even thousands of data points and therefore can be used to solve most real-world applications we have in mind. We then proceed to discuss how on top of an optimal dispersion, we can implement high diversity. A logical next step is to obtain optimal maximum diversity as well, which is however only limited to rather small data sets, as we show. Due to its bicriterion nature, the BILS by @brusco2019 is naturally suited to maintain optimum dispersion while optimizing diversity. We discuss several adaptations of the BILS, which are compared in a simulation study. 

# Discussion

\newpage

# References

\begingroup
<div id="refs" custom-style="Bibliography"></div>
\endgroup

