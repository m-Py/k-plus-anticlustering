---
title             : "k-plus: a bicriterion extension of k-means anticlustering"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

abstract: |

    Anticlustering refers to a collection of methods that partition data sets into disjunct groups while aiming for two objectives: (a) high between-group similarity and (b) high within-group heterogeneity. For some anticlustering formulations, these two objectives coincide, i.e., high between-group similarity is achieved when within-group heterogeneity is maximized. Other anticlustering criteria focus on one of the two aspects. The present paper focuces on maximizing between-group similarity by introducing a novel bicriterion objective for anticlustering. *K-plus anticlustering* is an extension of k-means anticlustering, which minimizes the difference between the mean of numeric attributes across clusters. The k-plus objective extends k-means by adding a term that aims to optimize similarity with regard to the variance of the attributes data across clusters. It is shown that the combined bicriterion objective can be reduced to an augmentation of the input data, leaving the original k-means criterion unchanged. A computer simulation and practical examples on real data show that k-plus anticlustering usually achieves high similarity both with regard to mean attribute values as well as the variance, with no noticable sacrifice to either criterion. The k-plus extension is therefore preferred over classical k-means anticlustering for optimizing between-group similarity. K-plus anticlustering is easily accessible to researchers and practitioners: the method has been implemented as part of the open source R package `anticlust` and is freely available via CRAN (https://cran.r-project.org/package=anticlust) and Github (https://github.com/m-Py/anticlust).
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, bicriterion optimization"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : 
  papaja::apa6_pdf: 
    number_sections: true
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
library("dplyr")
library("ggplot2")
library("tidyr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

source("misc-functions.R")

```

Anticlustering is a collection of methods to partition elements into disjunct groups, with the goal of obtaining high between-group similarity or within-group heterogeneity [@brusco2019; @spath1986; @valev1998]. Oftentimes, both goals coincide: several anticlustering objectives imply that similarity between groups is maximal whenever similarity within groups is minimal [@brusco2019; @papenberg2020; @feo1990]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and heterogeneity between clusters [@rokach2005; @steinley2006]. Whereas a variety of different approaches exist for cluster analysis such as hierarchical methods, model-based clustering, or network approaches [see @brusco_emergent_2012, for an overview], anticlustering is usually formalized as a partitioning process that divides a set of elements into disjunct groups in such a way that an objective function---representing between-group similarity and/or within-group heterogeneity---is optimized [@papenberg2020; @spath1986; @brusco2019].

Anticlustering is useful in many tasks surrounding research work in psychology. For example, anticlustering can be used to split a test into parts of equal difficulty [@gierl2017], to assign students to work groups [@baker2002] or when assigning stimuli to different---but parallel---experimental conditions [@lahl2006]. For a more comprehensive overview of anticlustering applications in Psychology, see @brusco2019 and @papenberg2020. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, these problems can be formalized as mathematical optimization problems [e.g., @brusco2019; @spath1986; @baker2002; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods can be distinguished by the input data (pairwise dissimilarity vs. attributes values) and the objective function that is used to optimize group between-group similarity or within-group heterogeneity. Some anticlustering objectives quantify group similarity on the basis of pairwise dissimilarity rating such as the Euclidean distance. The most prominent criterion on the basis of pairwise dissimilarity ratings is the *diversity*, i.e., the total sum of dissimilarities between any elements within the same group. Maximizing the diversity simultaneously optimizes within-group heterogeneity and between-group similarity. Because the diversity is the reversal of the clustering editing objective [@zahn1964; @shamir2004vf], @papenberg2020 used the term *anticluster editing* to refer to anticlustering approaches that optimize the diversity. Another important criterion based on pairwise dissimilarities is the *dispersion*, i.e., the minimum dissimilarity between any two elements within the same group [@fernandez2013; @brusco2019]. Maximizing the dispersion increases the within-group heterogeneity by ensuring that any two elements in the same group as dissimilar from each other as possible.

Oftentimes, researchers work with attribute values [or *features*; @dry2009] instead of dissimilarity ratings. This use case is for example common when selecting stimuli for an experiment on the basis of norming data [e.g., @kurdi2017introducing].  For word stimuli in psycholinguistic experiments, norming data may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attribute values are binary and may represent the presence or absence of a feature [@tversky1977]. In the context of anticlustering, one approach for handling attribute data is to compute an appropriate dissimilarity measure such as the pairwise Euclidean or squared Euclidean distance across the set of attributes [@brusco2019]. Subsequently, an anticlustering criterion such as the diversity can be optimized to partition the elements into groups. A different approach to anticlustering directly works with the attribute data by maximizing the k-means criterion. K-means is probably the best-known clustering method. Reversing the k-means objective function---using maximization instead of minimization---has been recognized as a useful anticlustering tool by Späth [-@spath1986] and Valev [-@valev1998; -@valev1983]. In k-means clustering, the sum of the squared Euclidean distances between elements and the center of the cluster they are assigned to is minimized [@brusco2006branch], usually using the k-means heuristic [@jain2010; @steinley2006]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. For the anticlustering application, the k-means criterion is maximized instead, thereby achieving similarity between groups. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. Hence, groups are similar with regard to the mean of the distribution of the numeric attributes wheras other parameters---such as the variance---are ignored. When requiring that groups are overall similar to each other, this focus may be misguided as similar means can be obtained even when the underlying distributions are different [e.g., @anscombe1973]. @papenberg2020, showed that maximizing the diversity (based on the Euclidean distance) is better suited to minimize difference with regard to both the mean and the variance of the data; it was shown that even an entirely random split usually resulted in more similar variances because k-means solely focuses on the mean similarity.

To optimize overall group similarity---and hence to overcome the limitations of k-means anticlustering---I present k-plus anticlustering in this paper. The k-plus criterion is an extension of the k-means objective that includes a term that quantifies similarity with regard to the variance of data across groups. Interestingly, this bicriterion reformulation can be understood as an augmentation of the original data matrix, leaving the original k-means objective unchanged. I show that the k-plus criterion outperforms other anticlustering criteria when the goal is to minimize differences with regard to the mean and the variance of groups. In most cases, both objectives can be optimized satisfactorily at the same time without any loss on either. Moreover, in two pratical examples I illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust`, an extension to the popular statistical programming language R [@papenberg2020; @R-base; @R-anticlust].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means clustering (and anticlustering) is used to partition two-way, two-mode data into groups. That is, $N$ elements each having $P$ attributes are assigned to $K$ groups $(C_1., \ldots, C_K)$, $C_k$ being the set of $n_k$ objects in group $k$. The partitioning is subject to the requirement that each element is assigned to exactly one group, formalized by the following two constraints [@papenberg2020]:

$$
\bigcup\limits_{k = 1}^{K} C_j = X
$$

$$
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k
$$

In anticlustering applications, we impose an additional constraint on the group sizes $n_k$, where the most common restriction requires that all groups have equal size:

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares ($\mathit{SSE}$), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging the $k$th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note there is a direct connection between the $\mathit{SSE}$ and the location of the cluster centroids [@spath1986]. If the $\mathit{SSE}$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $\mathit{SSE}$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values across clusters.

## Quantifying differences in variance

In the following, I present a variation of the $\mathit{SSE}$ that quantifies how the variance of numeric data differs between groups. To motivate the new criterion, we first consider a set of one-dimensional observations $(x_1, \ldots, x_N)$. The extension to the general case of $P$ attributes will be straight-forward.

The sample variance of a set of observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
y_i = (x_i - \overline{\mathbf{x}})^2
$$

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average, k-means anticlustering can be employed to equalize the variance across groups. That is, we can formulate an adaption of the $\mathit{SSE}$ to quantify the degree to which the variance differs between groups. First, however, we generalize the idea to the case of $P$ attributes. For each attribute, we have to compute the squared deviation between all values and the mean of the attribute. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$, where

$$
y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2.
$$

To reflect how strongly the variances of all attributes vary across the $K$ clusters, we now define the criterion $\mathit{SSE_{Var}}$:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Analogously to the standard $\mathit{SSE}$, the centroid values $\overline{y}_j ^{(k)}$ are given as:

$$
\overline{y}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} y_{ij}
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points.

## A bicriterion model: K-plus anticlustering {#kplus}

It is unlikely that an exclusive optimization of $\mathit{SSE_{Var}}$ (i.e., equalizing variances) is of interest without also considering the $\mathit{SSE}$ (i.e., equalizing means). A combination of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$, however, can be employed to simultaneously optimize both objectives, which is a reasonable idea when striving for overall cluster similarity. The most simple approach for such a bicriterion optimization is the weighted sum approach [@naidu2014; @marler2010]. That is, both criteria are computed independently from each other and then combined into a single criterion through a weighted sum:

$$
\mathit{SSE_{kplus}} = w_1 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{x}_j ^{(k)})^2 + 
  w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{y}_j ^{(k)})^2
$$

The weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of equalizing means and variances, respectively. Some simplifications can be arranged when ignoring the weights $w_1$ and $w_2$, e.g., by setting them both to 1. When defining $z_i = (x_{i1}, \ldots, x_{iP}, y_{i1}, \ldots, x_{iP})$, i.e., the concatenation of the $x_{ij}$ and $y_{ij}$ values, the bicriterion $\mathit{SSE_{kplus}}$ is then computed as

$$
\mathit{SSE_{kplus}} =
  \sum\limits_{j=1}^{2 P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (z_{ij} - \overline{z}_j ^{(k)})^2
$$

This formula again has the same form as $\mathit{SSE}$ (and $\mathit{SSE_{Var}}$), but is computed on different data. In this case, both the original data $x_{ij}$ as well as the squared distances $y_{ij}$ are used as attributes that enter the objective function. That is, computing the k-plus criterion can be done by first computing the values $y_{ij}$, appending these to the original data matrix, and then computing the $\mathit{SSE}$ on the augmented data matrix. Hence, k-plus anticlustering reduces to an augmentation of the data matrix, leaving the original k-means criterion unchanged.

Note that a problem may arise if the weights $w_1$ and $w_2$ are ignored when optimizing the k-plus objective. Since the computation of the data matrix $\mathbf{Y}_{N \times P}$ involves squaring, the squared distances $y_{ij}$ have much larger values, on average, than the raw values $x_{ij}$ (also see Figure 1). Therefore, the $\mathit{SSE_{Var}}$ criterion naturally receives greater weight when computing $\mathit{SSE_{kplus}}$ than the standard $\mathit{SSE}$ criterion. This imbalance is most likely unwarranted because optimizing similarity with regard to variances is not more important than optimizing similarity with regard to means. A solution is to standardize all values $x_{ij}$ and $y_{ij}$ to the same scale before computing $SSE_{kplus}$, e.g. via $z$-standardization.

## Criterion tradeoff

```{r}

# First rendering is slow because the data needs to be generated
path <- "partition_objectives.csv"

if (!file.exists(path)) {
  set.seed(2207) # 2207
  # Generate data and all partitions
  N <- 14
  K <- 2
  partitions <- generate_partitions(N, K)
  features <- rnorm(N, 15, 3)

  # Compute objectives for each partition
  # For each partition: Compute SSE objective, and SSE(VAR)
  # 1. SSE
  all_objectives_kMeans <- sapply(
    partitions,
    FUN = var_objective,
    features = features
  )
  
  squared_distances_from_mean <- squared_from_mean(as.matrix(features))
  
  # 2. SSE(VAR)
  all_objectives_kVar <- sapply(
    partitions,
    FUN = var_objective,
    features = squared_distances_from_mean
  )
  
  # 3. SSE(KPLUS)
  # Use standardization to compute k-plus criterion!
  all_objectives_kPlus <- sapply(
    partitions,
    FUN = var_objective,
    features = scale(cbind(features, squared_distances_from_mean))
  )
  
  df <- data.frame(
    kmeans = all_objectives_kMeans,
    kvar = all_objectives_kVar,
    kplus = all_objectives_kPlus
  )

  write.table(
    df, path, row.names = FALSE, sep = ","
  )
} else {
  df <- read.csv(path)
}

```

When optimizing a bicriterion objective such as $SSE_{kplus}$, the ideal outcome would be to find a partition where both the means and the variances are very similar between groups, i.e., a which partition where both the standard $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ are close to the global optimum value. Generally, research on multi-objective optimization has shown that cases are rare where a single solution satisfies all criteria to an optimal degree [@ferligoj1992direct]. Usually, a tradeoff is necessary. However, a motivating example illustrates that aiming to simultaneously optimize similarity of means and variances is not only desirable, but may actually be feasible.

```{r, fig.cap = "The relationship between the objectives $SSE$ and $SSE_{Var}$ across all partitions (for $N = 14$ and $K = 2$). The partition represented by the triangular minimizes the difference in variances between groups; the partition represented by the square minimizes the difference in means between groups; the partition represented by the diamond maximizes $SSE_{kplus}$ and represents a good tradeoff, satisfying both criteria to an almost optimal degree.", fig.height = 5, fig.width = 5}

# Plot SSE vs SSE(VAR)
plot(
  df[, c("kmeans", "kvar")], 
  pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "SSE",
  ylab = "SSE_Var"
)

# Illustrate best partition wrt difference in means (i.e., SSE)
points(
  df$kmeans[which.max(df$kmeans)],
  df$kvar[which.max(df$kmeans)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt difference in variance (i.e., SSE(VAR))
points(
  df$kmeans[which.max(df$kvar)],
  df$kvar[which.max(df$kvar)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt k-plus criterion
points(
  df$kmeans[which.max(df$kplus)],
  df$kvar[which.max(df$kplus)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

For this example I created 14 data points following a univariate normal distribution ($M = 15$, $SD = 3$) and then generated all `r n_partitions(14, 2)` possible ways to partition the 14 data points into $K = 2$ equal-sized groups. Figure 1 shows the scatterplot of the criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ across partitions. Note that for both criteria, a larger value is better, i.e., indicates higher similarity with regard to means and variances, respectively. Therefore, to obtain overall between-group similarity, a partition should be chosen that is depicted in the upper right corner of the scatterplot. However, optimal k-means anticlustering would create two groups that are not particularly similar with regard to their variance. The partition that is represented by the squared symbol has the overall best highest $\mathit{SSE}$, but is outperformed by many other partitions with regard to $\mathit{SSE_{Var}}$. Similarly, the partition that maximizes $\mathit{SSE_{Var}}$ (the triangular symbol in Figure 1) is not well-suited to optimize SSE at the same time. The diamond symbol represents the partition that maximizes (the unweighted) $SSE_{kplus}$, i.e., the sum of $\mathit{SSE}$ and $\mathit{SSE_{Var}}$. This partition is located in the right upper corner and yields `r df$kmeans[which.max(df$kplus)] / max(df$kmeans) * 100`% of the global maximum $\mathit{SSE}$, and `r df$kvar[which.max(df$kplus)] / max(df$kvar) * 100`% of the global maximum value of $\mathit{SSE_{Var}}$.

This initial example shows that maximizing the combined k-plus criterion may well be suited to optimize both similarity with regard to means as well as variances between groups. In the next section I present a simulation study that more systematically investigates the performance of the k-plus criterion.

# Simulation study

To systematically evaluate the new k-plus criterion, I compared three anticlustering approaches in a simulation study: (a) k-means anticlustering, optimizing the $\mathit{SSE}$; (b) k-plus anticlustering, optimizing the $SSE_{kplus}$; (c) anticluster editing, optimizing the diversity criterion. All methods employed an a priori standardization of the data sets. In the case of k-plus anticlustering, this means that the augmented input data (i.e., the original data and the squared distance data) were subjected to a standardization, which is recommended.

The simulation used the statistical programming environment R [@R-base]. The R package `anticlust` [Version 0.5.6; @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R packages `dplyr` [Version 1.0.7; @R-dplyr], `ggplot2` [Version 3.3.5; @R-ggplot2], `papaja` [Version 0.1.0.9997; @R-papaja] and `tidyr` [Version 1.1.3; @R-tidyr] were used for data processing and result presentation. The simulation study is fully reproducible via code and data that is accessible from the accompanying OSF repository (**link TODO**). 

## Conditions

First, I generated 10,000 data sets which were subsequently subjected to the three anticlustering methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features $M$ varied between 1 and 10; (c) the distribution of features was randomly determined to be either (i) uniform in [0, 1], (ii) a standard normal distribution with *M* = 0 and *SD* = 1, or (iii) a wider normal distribution with *M* = 0 and *SD* = 2. For a given data set, all $M$ features were independently sampled from the given distribution and no correlations were assumed between features. All three anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$; groups sizes were always equal. 

## Optimization algorithm

To optimize the three anticlustering criteria, a local-maximum search was applied that systematically swaps elements between groups to gradually improve the objective function [Method LCW in @weitz1998]. Building on an initial random assignment of elements to groups, elements are swapped between groups in such a way that each swap improves the objective criterion by the largest amount that is possible. That is, for each element, all possible swaps are simulated and the resulting objective values are stored. After each exchange has been tested, the one exchange is realized that improves the objective the most. No swap is realized if no swap improves the criterion. The exchange procedure is repeated for each element. Once the last element has been processed, the procedure restarts at the beginning and once again iterates through all elements. The search stops as soon as an iteration through the entire set no longer yields any improvement, i.e., as soon as a local maximum has been found. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. The simulation employed five repetitions of the local maximum search.

## Evaluation criteria

Two primary performance criteria were assessed. First, for k-plus anticlustering, I checked how well the identified solution approximated the ordinary k-means criterion. Specifically, I computed the $\mathit{SSE}$ for the solution that was returned by k-means anticlustering---that directly optimizes $\mathit{SSE}$---and for the solution that was returned by k-plus anticlustering that only indirectly optimizes $\mathit{SSE}$ through $SSE_{kplus}$. If optimizing $SSE_{kplus}$ simultaneously optimizes the standard $\mathit{SSE}$ to a satisfactory degree, we may conclude that simultaneously striving for similar means and variances is feasible by optimizing the k-plus criterion. 

Second, to compare the three anticlustering criteria with regard to their ability to create similar groups, I computed the means and standard deviations and assessed the degree to which discrepancies occurred between groups. To this end, I then computed the difference between the minimum and maximum value of the means and standard deviations between groups, for each of the $M$ features. The mean of the differences across all features quantified the total dissimilarity in means ($\Delta M$) and standard deviations ($\Delta \mathit{SD}$). Therefore, lower values of $\Delta M$ and $\Delta \mathit{SD}$ indicated that groups were more similar with regard means or standard deviations, respectively.

## Results

```{r, fig.cap = "Displays how well maximizing the k-plus criterion approximates the k-means criterion that is found via direct maximization of the k-means criterion.", fig.height = 5, fig.width = 5}

## Analyze data for K = 2 and K = 3 and K = 4
simulation_results <- list()
for (K in 2:4) {
  filename <- paste0("./Simulation_Study/results-K", K, "-objectives-raw.csv")
  df <- read.csv(filename, sep = ";", stringsAsFactors = FALSE)
  df$K <- K
  simulation_results[[paste0("K-", K)]] <- df
}

df <- do.call(rbind, simulation_results)
rownames(df) <- NULL

# Make long format
ldf <- pivot_longer(
  df,
  cols = paste0(c("kvar", "kmeans", "means", "sd"), "_obj"),
  names_to = "Objective",
  names_pattern = "(.*)_obj"
)

# Check how well k-plus approximates k-means
rel <- ldf %>% 
  group_by(method, Objective, N, K) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(Objective == "kmeans", method %in% c("kplus", "k-means-exchange")) %>% 
  pivot_wider(names_from = method, values_from = Mean) %>% 
  group_by(N, K) %>% 
  summarise(rel = kplus / `k-means-exchange`) 

rel %>% 
  ggplot(aes(x = N, y = rel, colour = factor(K), linetype = factor(K))) + 
  geom_line(size = 1) +
  geom_point(aes(colour = factor(K), shape = factor(K)), size = 3) +
  scale_x_continuous(breaks = seq(36, 300, 24)) + 
  theme_apa() +
  theme(
    legend.position = "top",
    legend.title = element_text()
  ) +
  scale_colour_grey(name = "K") +
  guides(
    linetype = guide_legend(title = "K"),
    shape = guide_legend(title = "K")
  ) + 
  labs(y = "k-plus approximation of k-means criterion")

# approximation at N > 100
approx_n100 <- mean(filter(rel, N > 100)$rel) 
approx_n24_k4 <- mean(filter(rel, N == 24, K == 4)$rel) 

# cut decimals after n decimals
cut_decimals <- function(x, n) {
  x <- as.character(x)
  x1 <- strsplit(x, "\\.")[[1]][1]
  x2 <- strsplit(x, "\\.")[[1]][2]
  x2 <- strsplit(x2, "")[[1]]
  as.numeric(paste0(x1, ".", paste(x2[1:n], collapse = "")))
}

```

First, I investigated how well maximizing the k-plus criterion approximates the ordinary k-means criterion at the same time. Figure 2 illustrates the relative values of $\mathit{SSE}$ that were found when optimizing $\mathit{SSE_{kplus}}$---as compared to the direct optimization of $\mathit{SSE}$. Generally, optimizing $\mathit{SSE_{kplus}}$ yielded an adequate approximation of $\mathit{SSE}$. In particular, for large group sizes, there is virtually no sacrifice in $\mathit{SSE}$ when  $\mathit{SSE_{kplus}}$ is optimized: For $N > 100$, $\mathit{SSE}$ was approximated to more than `r cut_decimals(approx_n100, 4) * 100`% when conducting k-plus anticlustering as compared to k-means anticlustering. In the simulation, the worst approximation was found when the group sizes were smallest: for $N = 24$ and $K = 4$ was approximated to $\mathit{SSE}$ `r approx_n24_k4 * 100`%. Thus, the quality of the approximation depended on the group size: smaller groups led to worse approximations, while in general there was very little in $\mathit{SSE}$, expecially for larger groups. 

In a next step, I compared k-means anticlustering, k-plus anticlustering and anticluster editing in their ability to equalize means and standard deviations between groups. 

```{r, fig.cap = "Awesome caption", fig.height = 6.5, fig.width = 5}

ldf %>% 
  group_by(method, Objective, N, K) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(Objective %in% c("means", "sd"), method != "random") %>% 
  ggplot(aes(x = N, y = Mean, colour = method)) + 
  geom_line(aes(colour = method), size = .7) + 
  geom_point(aes(colour = method, shape = method)) +
  facet_grid(cols = vars(Objective), rows = vars(K), scales = "free") + 
  theme_apa() +
  theme(
    legend.position = "top",
    legend.title = element_text()
  ) + 
  guides(
    linetype = guide_legend(title = "Method"),
    shape = guide_legend(title = "Method")
  ) +
  theme(
    legend.position = "top",
    legend.title = element_text()
  ) +
  scale_colour_grey(name = "Method") 


```

- k-plus approximates k-means very well, especially for large $N$ (for larger $K$, $N$ schould increase as well)
- k-plus is the best method to optimize similarity with regard to means and standard deviations at the same time

# K-plus anticlustering using the R package anticlust 

In this section I demonstrate how researchers can easily employ k-plus anticlustering using the R package `anticlust` [@papenberg2020]. The results presented here used `anticlust` version 0.5.6. The package can be installed in the R environment using the `install.packages()` command:

```R
install.packages("anticlust")
```

## Example Application I

In this application, I use norming data for the OASIS image data set that is freely available online [@kurdi2017introducing]. @kurdi2017introducing assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense collected an additional rating dimension by measuring how the same 900 images were rated with regard to their beauty. In my application, I use all three features, currently available from Github.[^githubbeauty]

[^githubbeauty]: https://github.com/aenneb/OASIS-beauty

I used k-plus anticlustering to divide the data set of 900 images into 9 groups of 100 images each; this process takes about 3-5 seconds on a contemporary personal computer, depending on how many iterations are needed to find the local maximum. The task was accomplished using the following R code:

```R
library(anticlust) # load the package anticlust

anticlustering(
  features,
  standardize = TRUE,
  K = 9,
  objective = "kplus",
  method = "local-maximum"
)
```

Here, the variable `features` has to be a data table containing the image data: 900 rows representing images; 3 columns representing the features beauty, arousal, and valence. The accompanying OSF repository contains the fully reproducible code that includes downloading and reading the OASIS data, selecting the relevant columns, and finally storing them in the `features` variable (**TODO**). The arguments `objective = "kplus"` and `method = "local-maximum"` ensure that the k-plus criterion is optimized using the local maximum search method described in the previous section (**TODO**). The argument `K` describes the number of groups. The argument `standardize = TRUE` ensures that all features are standardized before the optimization starts; in particular, this option enforces that equalizing means and variances receives the same weight during the optimization process (see [Section 1.2](#kplus)).

Table 1 shows the results of the anticlustering application by listing the descriptive statistics (means and standard deviations) for each of the 9 groups and for each of the 3 features. Additionaly, Table 1 shows the descriptive statistics for an application of k-means anticlustering and anticluster editing (which maximizes the diversity criterion). It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations. Anticluster editing was also well-suited to match both means and standard deviations, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---anticluster editing does not directly maximize similarity with regard to the means and standard deviations, but instead optimizes the within-group heterogeneity.

```{r}

path <- "results_oasis_same_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- 9
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
      features,
      standardize = TRUE,
      K = K,
      objective = criterion,
      method = "local-maximum"
    )
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(
      1:K,
      nrow(features) / K,
      mean_sd_tabs[[criterion]]
    )
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Anticluster editing}" = 19:27)
)
```

Table 2 illustrates an advantage of k-plus anticlustering over anticluster editing: When creating groups of unequal size, anticluster editing tends to increase the spread of the data in the largest group in comparison to the other groups. K-plus anticlustering strives for equal means and variances in all groups, regardless of group size. Unequal group sizes can be requested in anticlust by passing the different group sizes to the `K` argument:

```R
anticlustering(
  features,
  K = c(100, 100, 100, 600),
  standardize = TRUE,
  objective = "kplus",
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
      features,
      standardize = TRUE,
      K = K,
      objective = criterion,
      method = "local-maximum"
    )
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Anticluster editing}" = 9:12)
)
```

## Example Application II: Small data set (N = 96)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore,    this second example application illustrates that k-plus anticlustering may also yield satisfactory results when group are smaller than in the OASIS example. I employ a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Dr. Schaper [@schaper2019metacognitive; @schaper2019metamory; also see @papenberg2020 for a description of the norming data]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

Table 3 illustrates the results that are obtained when using k-plus anticlustering to divide the stimulus set into 6 groups of size `r nrow(schaper2019) / 6` each. Even though the means and standard deviations are no longer perfectly matched between groups, arguably, they are still similar enough for any practical purpose---especially when considering inevitable measurement error in norming studies. The following code was used to achieve the results reported in Table 3:

```{r, echo = TRUE}
features <- schaper2019[, 3:6]
anticlusters <- anticlustering(
  features,
  K = 6,
  objective = "kplus",
  method = "local-maximum",
  repetitions = 5,
  standardize = TRUE
)
```

Note that standardization is particularly useful for anticlustering when the score range differs strongly between features, as is the case for the Schaper data set.

```{r}

tab <- mean_sd_tab(features, anticlusters)
tab <- cbind(1:6, 96 / 6, tab)
colnames(tab) <- c("Group", "N", "Typicality", "Atypicality", "Syllables", "Frequency")

apa_table(
  tab,
  caption = "Descriptive statistics by group for the Schaper et al. (2019) data set, after applying k-plus anticlustering.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r")
)

```

# Discussion

- k-plus anticlustering is useful and should in the default case be preferred to k-means anticlustering. For optimizing similarity in means and variances it also outperforms anticluster editing, which, to be fair, does not directly strive to do so. Unequal group sizes give another advantage for k-plus anticlustering in comparison to anticluster editing. For small samples sizes (or large $K$), k-plus anticlustering may return slightly less-than-optimal results in the eye of the user, but in this case it is no problem to return to k-means and only focus on the means. There is not "anticlustering hacking"; users may simply choose the results they like best. For now, I strongly recommend using k-plus anticlustering for creating groups with high between group similarity.

## Limitations and outlook

- This paper focused on the presentation of new objective function, the bicriterion optimization process might be improved [e.g., by using a direct algorithm as @brusco2019 did]. From the algorithmic point of view, this paper did not strive to apply the latest cutting edge methodology. For most practical purposes, however, I assume that optimizing the k-plus criterion will yield very satisfactory results for researchers. I have already worked with researchers planning their experiments who are generally amazed by the groupd that are returned by k-plus anticlustering.

## Future research

- Future research: Application of more sophisticated bicriterion optimization algorithms are interesting

- extending k-means anticlustering on maximizing similarity with regard to covariances? Mahalanobis distance?

# General thoughts on anticlustering

- new criterion: optimizes between-group similarity, no correspondence to within-group heterogeneity [see @brusco2019 for a exceptional method maximizing within-group heterogeneity]

- Similarly: the reversed objective no longer has a useful clustering interpretation: Maximizing differences in variances does not lead to homogeneous and/or well separated clusters


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
