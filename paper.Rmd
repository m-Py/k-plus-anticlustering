---
title             : "k-plus Anticlustering: An Improved k-means Criterion for Maximizing Between-Group Similarity"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

abstract: |

    Anticlustering refers to the process of partitioning elements into disjoint groups with the goal of obtaining high between-group similarity and high within-group heterogeneity. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---and is usually approached by maximizing instead of minimizing a clustering objective function. Introducing k-plus, this paper presents an extension of the k-means clustering objective to maximize between-group similarity in anticlustering applications. The k-plus criterion represents between-group similarity as discrepancy in distribution moments (means, variance, higher order moments, and covariances), while the classical k-means criterion only reflects the degree to which groups differ in their means. While constituting a new criterion for anticlustering, it is surprisingly shown that k-plus anticlustering can be implemented by optimizing the classical k-means criterion after the input data has been augmented with additional variables. A computer simulation and practical examples show that k-plus anticlustering achieves high between-group similarity with regard to multiple objectives. In particular, optimizing between-group similarity with regard to variances usually does not compromise similarity with regard to means; the k-plus extension is therefore generally preferred over classical k-means anticlustering. Examples are given on how k-plus anticlustering can be applied to real norming data using the open source R package `anticlust`, which is freely available via CRAN (https://cran.r-project.org/package=anticlust).
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, covariance, skewness, kurtosis, stimulus selection"
wordcount         : "5114"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

output            : 
  papaja::apa6_pdf
appendix: "appendix.Rmd"

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \floatstyle{plaintop}
  - \restylefloat{figure}
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - |
    \makeatletter
    \renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
      \hskip -\arraycolsep
      \let\@ifnextchar\new@ifnextchar
      \array{#1}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
---

```{r setup, include = FALSE}
library(papaja) # use the development version: `remotes::install_github("crsh/papaja@devel")`; currently 0.1.1.9001
library(anticlust)
library(knitr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(DescTools)
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library(prmisc) # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```

Anticlustering refers to the process of partitioning elements into disjoint groups with the goal of obtaining high between-group similarity and high within-group heterogeneity [@brusco2019; @spath1986; @valev1998; @valev1983]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and separation between clusters [@rokach2005]. Anticlustering has many applications in research psychology [@steinley2006; @brusco2019; @papenberg2020]. Examples include splitting tests into parts of equal difficulty [@gierl2017], assigning students to work groups [@baker2002], and assigning stimuli to different, but parallel experimental conditions [@lahl2006]. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, anticlustering problems can be formalized as mathematical optimization problems [e.g., @baker2002; @brusco2019; @spath1986; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods are characterized by (a) an objective function that quantifies between-group similarity and/or within-group heterogeneity, and (b) an algorithm that determines how elements are assigned to groups to maximize the objective function. Several anticlustering objective functions use pairwise dissimilarity ratings such as the Euclidean distance as input. The most prominent criterion of this kind is the *diversity*, which is the total sum of pairwise dissimilarities between elements within the same group [@brusco2019; @gallego2013]. By considering pairwise dissimilarities between elements in the same group, the diversity criterion reflects the total degree of within-group heterogeneity. High within-group diversity simultaneously ensures high between-group similarity.[^equalsizedgroups] Another important anticlustering criterion based on the information in a dissimilarity matrix is the *dispersion*, which is the minimum dissimilarity between any two elements within the same group [@fernandez2013]. Maximizing the dispersion increases within-group heterogeneity by ensuring that any two elements in the same group are as dissimilar from each other as possible. @brusco2019 convincingly argued that anticlustering applications striving for within-group heterogeneity should incorporate both dispersion and diversity, and they presented a bicriterion algorithm to approximate the Pareto efficient set of solutions according to both criteria. 

[^equalsizedgroups]: As will be shown later, the diversity objective only strictly equalizes within-group heterogeneity and between-group similarity when groups are equal-sized.

Oftentimes, partitioning applications in psychology are focused on between-group similarity rather than within-group heterogeneity, even though both goals oftentimes coincide. High between-group similarity is for example desirable when designing experimental conditions using different stimulus sets that should be as similar as possible with regard to response-relevant attributes [@lahl2006]. Such stimulus selection tasks are usually conducted on the basis of attribute data and not on pairwise (dis)similarity ratings. That is, individual stimuli are numerically coded on relevant dimensions [@dry2009]. For stimuli in psycholinguistic experiments, attributes may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attributes are binary and may represent the presence or absence of a feature [@tversky1977]. When attribute values are available, anticlustering approaches that do not require a dissimilarity matrix[^dissimilarityfromfeatures] can be used to partition the data. K-means is probably the best-known clustering objective that can directly be computed on attribute values. In k-means clustering, the sum of the squared Euclidean distances between data points and their cluster centers is minimized, usually using the k-means heuristic [@jain2010; @steinley2006; @brusco2006branch]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. Reversing the k-means objective function---using maximization instead of minimization---has been recognized as a useful approach when aiming for high between-group similarity [@spath1986; @valev1998; @valev1983]. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. However, other distribution characteristics---such as the variance---are not targeted. @papenberg2020 showed that k-means anticlustering tends to over-optimize between-group similarity with regard to the mean at the cost of similarity in variance. However, when aiming for overall between-group similarity, neglecting all distribution characteristics other than the mean is misguided; similar means can be obtained even when the underlying distributions are clearly different [e.g., @anscombe1973]. 

[^dissimilarityfromfeatures]: It is still possible to convert the attribute data into a dissimilarity matrix, e.g. by computing all pairwise Euclidean distances, and then optimize a distance-based anticlustering criterion such as the diversity to obtain a partitioning.

To optimize overall between-group similarity---and hence to overcome the limitations of k-means anticlustering---this paper introduces k-plus anticlustering. K-plus is an extension of the k-means objective that quantifies between-group similarity as discrepancy with regard to several distribution characteristics instead of only the mean. Specifically, k-plus offers the possibility to minimize differences with regard to the variance, covariances, and higher order moments such as skewness and kurtosis. After formally introducing the k-plus objective, a simulation study shows that k-plus anticlustering is well-suited to create groups having minimal differences according to multiple distribution characteristics. Practical examples illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust` [@papenberg2020; @R-anticlust], an extension to the popular statistical programming language R [@R-base].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means anticlustering is used to partition $N$ elements each having $P$ attributes into $K$ groups $(C_1, \ldots, C_K)$. The partitioning is subject to the requirement that each element is assigned to exactly one group. In anticlustering applications, an additional constraint is usually imposed on the group sizes, with the most common restriction being that all groups have equal size [@papenberg2020].

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares ($\mathit{SSE}$), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging to the $k$'th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note that there is a direct connection between the $\mathit{SSE}$ and the location of the cluster centroids [@spath1986]. If the $\mathit{SSE}$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $\mathit{SSE}$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values between clusters.

## Quantifying differences in variance

In the following, I present a variation of the $\mathit{SSE}$ that quantifies how the variance of numeric data differs between groups. To motivate the new criterion, we first consider a set of one-dimensional observations. The sample variance of a set of $N$ observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
y_i = (x_i - \overline{\mathbf{x}})^2
$$

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average---and k-means anticlustering minimizes differences with regard to averages---k-means anticlustering can be employed to equalize the variance across groups. This constitutes the central insight motivating the new k-plus criterion. 

To obtain an adaptation of the $\mathit{SSE}$ that quantifies the degree to which the variance differs between groups, a new variable is computed for each original variable; the new variable represents the squared distance of each of the original values to the overall mean of the variable. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$ where $y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2$. On the basis of the data matrix $\mathbf{Y}_{N \times P}$ we define the criterion $\mathit{SSE_{Var}}$, reflecting how strongly the variances of all attributes vary across the $K$ clusters:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points. Maximizing $\mathit{SSE_{Var}}$ will lead to similar variances across the $K$ groups in the $P$ attributes. 

## A bicriterion model: K-plus anticlustering {#kplus}

It is unlikely that an exclusive maximization of $\mathit{SSE_{Var}}$ (i.e., equalizing variances) is of interest without also considering the standard $\mathit{SSE}$ (i.e., equalizing means). A combination of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$, however, can be employed to simultaneously optimize both criteria, which is a reasonable idea when striving for overall between-group similarity. The most simple approach for such a bicriterion optimization is the weighted sum approach [@naidu2014; @marler2010]. That is, both criteria are computed independently from each other and are then combined into a single criterion through a weighted sum:

$$
w_1 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + 
w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

The weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of equalizing means and variances, respectively. Some simplifications can be arranged when ignoring the weights $w_1$ and $w_2$, e.g., by setting them both to 1. This simplification leads to the *unweighted sum approach* for k-plus anticlustering: When defining a new data matrix $\mathbf{Z}_{N \times 2P} = \{z_{ij}\}_{N \times 2P}$ as the augmentation of $\mathbf{X}_{N \times P}$ with $\mathbf{Y}_{N \times P}$, i.e.

$$
  \mathbf{Z}_{N \times 2P} =
  \left[ {\begin{array}{cccc|cccc}
    x_{11} & x_{12} & \cdots & x_{1P} & y_{11} & y_{12} & \cdots & y_{1P}\\
    x_{21} & x_{22} & \cdots & x_{2P} & y_{21} & y_{22} & \cdots & y_{2P}\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
    x_{N1} & x_{N2} & \cdots & x_{NP} & y_{N1} & y_{N2} & \cdots & y_{NP}\\
  \end{array} } \right]
$$

and $z_{i.} = (x_{i1}, \ldots, x_{iP}, y_{i1}, \ldots, y_{iP})$, the bicriterion $\mathit{SSE_{kplus}}$ can be computed as

$$
\mathit{SSE_{kplus}} =
  \sum\limits_{j=1}^{2 P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (z_{ij} - \overline{z}_j ^{(k)})^2.
$$

This formula again has the same form as $\mathit{SSE}$ but is computed on different data. In this case, both the original values $x_{ij}$ as well as the squared distances $y_{ij}$ are used as attributes that enter the objective function. That is, computing the k-plus criterion can be done by first computing the values $y_{ij}$, appending them as new columns to the original data matrix, and then computing $\mathit{SSE}$ on the augmented data. Hence, using the unweighted sum approach, k-plus anticlustering leaves the original k-means criterion unchanged. 

Note that a problem may arise if the weights $w_1$ and $w_2$ are ignored when optimizing the k-plus objective. Because the computation of the new attribute values $y_{ij}$ involves squaring, their range of values is expected to be very different from the raw values $x_{ij}$ (e.g., see Figure 1). This imbalance can strongly and unpredictably influence the relative weight of $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ when computing $\mathit{SSE_{kplus}}$. To ensure an equal weight of both criteria, all attributes ($x_{ij}$ and $y_{ij}$) can be standardized before computing $\mathit{SSE_{kplus}}$, e.g. via $z$-standardization.

```{r motivatingExample}

# some functions for motivating example

# Variance objective function with changed order of the arguments 
# (can be called by sapply() etc.)
var_objective <- function(clusters, features) {
  anticlust::variance_objective(features, clusters)
}

# Compute features for k-plus criterion
squared_from_mean <- function(data) {
  apply(data, 2, function(x) (x - mean(x))^2)
}

# First rendering is slow because the data needs to be generated
path <- "partition_objectives.csv"

if (!file.exists(path)) {
  set.seed(2207) # 2207
  # Generate data and all partitions
  N <- 14
  K <- 2
  partitions <- generate_partitions(N, K)
  features <- rnorm(N, 15, 3)

  # Compute objectives for each partition
  # For each partition: Compute SSE objective, and SSE(VAR)
  # 1. SSE
  all_objectives_kMeans <- sapply(
    partitions,
    FUN = var_objective,
    features = features
  )
  
  squared_distances_from_mean <- squared_from_mean(as.matrix(features))
  
  # 2. SSE(VAR)
  all_objectives_kVar <- sapply(
    partitions,
    FUN = var_objective,
    features = squared_distances_from_mean
  )
  
  # 3. SSE(KPLUS)
  # Use standardization to compute k-plus criterion!
  all_objectives_kPlus <- sapply(
    partitions,
    FUN = var_objective,
    features = scale(cbind(features, squared_distances_from_mean))
  )
  
  df <- data.frame(
    kmeans = all_objectives_kMeans,
    kvar = all_objectives_kVar,
    kplus = all_objectives_kPlus
  )

  write.table(
    df, path, row.names = FALSE, sep = ","
  )
} else {
  df <- read.csv(path)
}

```

Research on multiobjective optimization indicates that solutions oftentimes fail to satisfactorily address several criteria at the same time [@ferligoj1992direct]. It is therefore unclear whether optimizing a bicriterion objective such as $\mathit{SSE_{kplus}}$ can yield a partitioning where both constituting criteria (mean and variance) are similar between groups. A motivating example illustrates that in the case of k-plus anticlustering, aiming to simultaneously optimize similarity of means and variances may be feasible. I created 14 data points following a univariate normal distribution ($M = 15$, $SD = 3$) and then generated all `r n_partitions(14, 2)` possible ways to partition the 14 data points into $K = 2$ equal-sized groups. Figure 1 shows the scatterplot of the criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ across partitions. The partition that is represented by the squared symbol has the overall best highest $\mathit{SSE}$, but is outperformed by many other partitions with regard to $\mathit{SSE_{Var}}$. Similarly, the partition that maximizes $\mathit{SSE_{Var}}$ (represented by the triangular symbol) is not well-suited to optimize $\mathit{SSE}$ at the same time. Thus, when only considering one of the objectives, the mathematically optimal partitioning fails to establish groups that are similar both with regard to their means and variances. The diamond symbol represents the partition that maximizes $\mathit{SSE_{kplus}}$, which yields `r df$kmeans[which.max(df$kplus)] / max(df$kmeans) * 100`% of the global maximum $\mathit{SSE}$, and `r df$kvar[which.max(df$kplus)] / max(df$kvar) * 100`% of the global maximum value of $\mathit{SSE_{Var}}$. Thus, using the unweighted sum approach, the optimal partition according to $\mathit{SSE_{kplus}}$ fulfills the two constituting criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ to an astonishing degree. 


```{r, fig.cap = "The relationship between the objectives $\\mathit{SSE}$ and $\\mathit{SSE_{Var}}$ across all ways to partition $N = 14$ normally distributed random values into $K = 2$ groups. The partition represented by the square maximizes $\\mathit{SSE}$ and thus minimizes the difference in means between groups; the partition represented by the triangular maximizes $\\mathit{SSE_{Var}}$ and thus minimizes the difference in variances between groups. The partition represented by the diamond maximizes $\\mathit{SSE_{kplus}}$ and satisfies both constituting criteria to an almost optimal degree.", fig.height = 5, fig.width = 5}

# Plot SSE vs SSE(VAR)
plot(
  df[, c("kmeans", "kvar")], 
  pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "",
  ylab = ""
)
mtext(side = 2, text = expression(italic(SSE[Var])), line = 3)
mtext(side = 1, text = expression(italic(SSE)), line = 3)

# Illustrate best partition wrt difference in means (i.e., SSE)
points(
  df$kmeans[which.max(df$kmeans)],
  df$kvar[which.max(df$kmeans)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt difference in variance (i.e., SSE(VAR))
points(
  df$kmeans[which.max(df$kvar)],
  df$kvar[which.max(df$kvar)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt k-plus criterion
points(
  df$kmeans[which.max(df$kplus)],
  df$kvar[which.max(df$kplus)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

## Skewness, kurtosis, and higher order moments

The k-plus approach to equalizing the variance can be generalized to higher order moments. The *j*'th sample moment of a variable $x = (x_1, \ldots, x_N)$ can be computed as 

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^j.
$$

where $j = 2$ gives the variance, $j = 3$ the skewness, and $j = 4$ the kurtosis [@joanes1998]. Thus, by changing $j$---the power of the deviation between mean and data points---we can use the k-plus logic to equalize any desired sample moment between groups. For each moment $j$ and for each variable $x$, this is accomplished by adding a new variable $y^{(j)} = (y^{(j)}_1, \ldots, y^{(j)}_N)$ with $y^{(j)}_i = (x_i - \overline{\mathbf{x}})^j$ to the data set and subsequently applying standard k-means anticlustering. I expect this to be particularly interesting for the skewness and kurtosis when aiming to minimize differences with regard to distribution asymmetry (skewness) and the propensity to include outliers (kurtosis).

## Covariances

Because the covariance is also defined as an expected value, the k-plus logic can be extended to minimize differences with regard to covariance structure. The covariance between two variables $x = (x_1, \ldots, x_N)$ and $y = (y_1, \ldots, y_N)$ is defined as

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}}).
$$

Thus, to maximize between-group similarity with regard to covariance structure, it is possible to augment the data set with an additional variable for each pair of variables $x$ and $y$, computed as $(x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}})$.

# Simulation study

To thoroughly evaluate the k-plus approach to anticlustering, three specific k-plus criteria were implemented in a simulation study: (i) "standard" k-plus anticlustering, minimizing differences with regard to the mean and variance; (ii) k-plus *skew/kurtosis*, minimizing differences with regard to the mean, variance, skew and kurtosis; (iii) k-plus *correlation*, minimizing differences with regard to the mean, variance and covariance structure. To implement all k-plus objectives, the unweighted sum approach was employed. Thus, to optimize a k-plus criterion, the original data set was augmented to incorporate the additional criteria and all variables---the original and the augmented variables---were $z$-standardized. Subsequently, the original k-means criterion $\mathit{SSE}$ was maximized on the basis of the augmented data set.

The three k-plus methods were compared to (i) k-means anticlustering, minimizing differences with regard to group means; (ii) diversity anticlustering, maximizing the sum of pairwise Euclidean distances within groups; (iii) a simple random allocation of elements to groups.

The simulation was implemented using the statistical programming language R [Version 4.2, @R-base]. The R package `anticlust` [Version 0.6.1, @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R package `faux` [Version 1.1.0, @R-faux] was used to generate the multivariate normal data sets that were processed during the simulation. The R packages `dplyr` [Version 1.0.7, @R-dplyr], `ggplot2` [Version 3.3.5, @R-ggplot2], `papaja` [Version 0.1.1.9001, @R-papaja], `tidyr` [Version 1.1.3, @R-tidyr] and `DescTools` [Version 0.99.45, @R-desctools] were used for data analysis and result presentation. The simulation study is fully reproducible via code and data that has been made accessible via an accompanying OSF repository [@osf2023].

## Optimization algorithm

The same local maximum search was applied to optimize each of the five competing anticlustering objectives [Method LCW, @weitz1998]. Building on an initial random allocation, the algorithm proceeds by swapping elements between groups in such a way that each swap improves the objective criterion by the largest possible margin. That is, it starts with the first element and simulates all exchanges with elements that are currently assigned to a different group. After each exchange has been evaluated, the one exchange is realized that improves the objective function the most. No exchange is realized if no improvement is possible. The exchange process is repeated for each element. After that, the procedure restarts at the beginning and is repeated until an iteration through the entire data set no longer yields an improvement. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. In the simulation study five repetitions were used.

## Conditions

For the simulation, I generated 10,000 data sets following a normal distribution. The data sets were subsequently processed by the competing methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features varied between 2 and 5; (c) the standard deviation of all features was set to 1, 2 or 3 (the mean was always zero); (d) the correlation between all features was $r = 0$, $r = .1$, $r = .2$, $r = .3$, $r = .4$ or $r = .5$. All anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$ (i.e., each method was applied 30,000 times). Groups sizes were always equal. 

## Evaluation criteria

After the five anticlustering methods and the random allocation procedure had been applied to the 10,000 data sets for $K = 2$, $K = 3$, $K = 4$, I investigated how well the competing methods were able to create similar groups. To this end, I analyzed the discrepancy in means, standard deviations, skewness, kurtosis, and correlation between groups---for each data set for each $K$. To quantify the discrepancy in group means, the following computation was used: for each of the (2, 3, 4 or 5) features, all (2, 3 or 4) group means were computed. For each feature, the difference between the minimum and maximum group mean was used as a measure of discrepancy; the global discrepancy in means ($\Delta M$) was then determined as the average discrepancy across features. The same procedure was applied to compute the discrepancy in standard deviations ($\Delta \mathit{SD}$), skewness ($\Delta \mathit{Skew}$) and kurtosis ($\Delta \mathit{Kurtosis}$). Quantifying discrepancy in correlations ($\Delta \mathit{Cor}$) followed the same rule, but discrepancies in correlations had to be averaged across pairs of features instead of single features.

## Results

Table 1 displays the global simulation results aggregated across all conditions. Standard k-means performed best at minimizing discrepancy with regard to means ($\Delta \mathit{M}$), but did not outperform a random assignment with regard to all other distribution characteristics. Replicating @papenberg2020, k-means' performance with regard to similarity in standard deviations was even below that of a random assignment. K-plus anticlustering addressed $\Delta \mathit{M}$ almost as well as k-means, and at the same time minimized discrepancy with regard to the standard deviations ($\Delta \mathit{SD}$); all three k-plus approaches strongly outperformed all other methods with regard to minimizing $\Delta \mathit{SD}$. 


\begin{singlespace}

```{r Table1}

# Load required packages
library(dplyr)
library(ggplot2)
library(tidyr)

## Analyze data for K = 2 and K = 3 and K = 4
simulation_results <- list()
for (K in 2:4) {
  filename <- paste0("./Simulation_Study/results-K", K, "-objectives-raw.csv")
  df <- read.csv(filename, sep = ";", stringsAsFactors = FALSE)
  df$K <- K
  simulation_results[[paste0("K-", K)]] <- df
}

df <- do.call(rbind, simulation_results)
rownames(df) <- NULL

# Make long format
ldf <- pivot_longer(
  df,
  cols = paste0(c("means", "sd", "skew", "kur", "cor"), "_obj"),
  names_to = "Objective",
  names_pattern = "(.*)_obj"
)

## Global results, aggregated across all simulation variables
tab <- ldf %>% 
  group_by(method, Objective) %>% 
  summarise(Mean = round(mean(value), 3)) %>% 
  pivot_wider(names_from = Objective, values_from = Mean) %>% 
  select(c(means, sd, skew, kur, cor))

colnames(tab) <- c("",
  "$\\Delta M$", 
  "$\\Delta \\mathit{SD}$",
  "$\\Delta \\mathit{Skew}$",
  "$\\Delta \\mathit{Kurtosis}$",
  "$\\Delta \\mathit{Cor}$"
)

apa_table(
  tab,
  caption = "Results of the simulation study",
  note = "The global results of the simulation study aggregated across all conditions. Cells contain information about the average global between-group discrepancy with regard to means, standard deviations, skewness, kurtosis, and correlations. Lower values indicate less discrepancy, i.e., higher between-group similarity. Each cell value is the result of averaging across 30,000 data points (10,000 data sets $\\times$ $K = 2$, $K = 3$ or $K = 4$).",
  escape = FALSE,
  align = c("r")
)

```

\end{singlespace}

```{r Figure1, fig.width = 8, fig.height = 10, fig.cap = "Results of the simulation study: Depicts the performance of the anticlustering methods with regard to minimizing discrepancy in means, variances, skewness, kurtosis and correlations (split by $N$, averaged across the remaining variables that varied in the simulation)."}

# Plot the results, by N
axis_ticks <- seq(24, 300, by = 12)
axis_labels <- axis_ticks
axis_labels[c(FALSE, TRUE)] <- ""

ldf %>% 
  group_by(method, Objective, N) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(method != "random") %>% 
  mutate(
    Objective = ordered(
      Objective, 
      levels = c("means", "sd", "skew", "kur", "cor"),
      labels = paste0(
        "Delta[italic(",
        c("M", "SD", "Skew", "Kurtosis", "Cor"),
        ")]"
      )
    ) 
  ) %>%
  ggplot(aes(x = N, y = Mean, colour = method)) + 
  geom_line(aes(linetype = method), size = .85) + 
  facet_grid(rows = vars(Objective), scales = "free", labeller = label_parsed) + 
  ylab("Mean discrepancy")+
  xlab(expression(italic(N))) +
  theme_bw(base_size = 16) + 
  scale_x_continuous(
    breaks = axis_ticks, 
    labels = axis_labels
  )

```


Figure 2 shows that with increasing $N$, k-means and the k-plus methods converged on the same level with regard to minimizing $\Delta \mathit{M}$. This is an important observation because it shows that k-plus anticlustering is capable of addressing multiple objectives at the same time---especially when $N$ increases. However, while k-plus anticlustering was capable of addressing multiple objectives simultaneously, increasing the number of optimization criteria made it more difficult to fulfill each single criterion: k-plus skew/kurtosis and k-plus correlation performed worse at minimizing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$ than standard k-plus. However, every k-plus method did what it was supposed to do: k-plus skew/kurtosis was best at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$, k-plus correlation was best at minimizing discrepancy with regard to $\Delta \mathit{Cor}$. At the same time, these two k-plus objectives maintained a comparably good level of addressing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$, outperforming diversity anticlustering. 

It is maybe surprising that k-plus skew/kurtosis does not seem to have performed much better than diversity anticlustering with regard to skewness and---in particular---kurtosis, even though the method has been specifically tailored to these criteria. Figure 2 gives a more fine grained assessment of this observation: Surprisingly, for small $N$, k-plus skew/kurtosis was even worse than diversity anticlustering at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. However, with increasing $N$, it clearly outperformed diversity anticlustering. These results illustrate the cost of optimizing several criteria at the same time: For low $N$, k-plus skew/kurtosis was apparently preoccupied with fulfilling the objectives $\Delta \mathit{M}$ and $\Delta \mathit{SD}$. Increasing $N$ then facilitated to also address $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. 

Generally, it should be noted that diversity anticlustering is a good all-arounder method that tends to address all distribution characteristics. For the specific objectives that are addressed by the k-plus methods, however, diversity anticlustering was outperformed. As as side note, apart from k-plus correlation, diversity anticlustering was the only method that equalized correlation structure among groups. To the best of my knowledge, this observation has not been made previously, even though the maximum diversity problem has been widely studied. 

The Appendix includes additional figures splitting the simulation results according to the other variables that varied between simulation runs (number of features; correlation between features; standard deviation of features; number of groups). Here, the only meaningful moderating effect on anticlustering performance was that k-plus correlation was more strongly impaired than other methods when the number of features increased. Because the number of covariances increases quadratically when the number of features increases, it gets comparatively more difficult to fulfill the correlation objective with a higher number of features.

# Applying k-plus anticlustering using the R package anticlust 

This section demonstrates how readers can easily employ k-plus anticlustering using the R package `anticlust` [@papenberg2020] using the function `kplus_anticlustering()`, which is available from version 0.6.3 onward. The package can be installed in the R environment using the `install.packages()` command:

```R
install.packages("anticlust")
```

## Example Application I

In the first example, I use norming data for the OASIS image data set that is freely available online [@kurdi2017introducing]. Kurdi et al. assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense collected additional norming data by measuring how the same 900 images were rated with regard to their beauty. In my application, I used all three features; the corresponding data is available from Github.[^githubbeauty]

[^githubbeauty]: https://github.com/aenneb/OASIS-beauty

I used the function `kplus_anticlustering()` to divide the data set of 900 images into 9 groups of 100 images each. The first argument of the function call is a variable named `features`, which contains the image data as a table: 900 rows representing images and 3 columns representing the features beauty, arousal, and valence. An OSF repository accompanying this manuscript contains the full code to reproduce the example [@osf2023].

```R
library(anticlust) # load the package anticlust

kplus_anticlustering(
  features, # data table with 900 rows and 3 features
  K = 9,
  variance = TRUE,
  skew = TRUE,
  kurtosis = FALSE,
  covariances = FALSE,
  moments = NULL,
  method = "local-maximum",
  standardize = TRUE
)
```

The function call takes about 10 seconds on a contemporary personal computer. The function `kplus_anticlustering()` has four boolean arguments to specify whether variance, skewness, kurtosis and covariances should be included as part of the k-plus criterion. Only the argument `variance` is set to `TRUE` by default---corresponding to the understanding that k-plus is an extension to k-means that at least equalizes variances in addition to means. The arguments corresponding to skewness, kurtosis and convariance have to be spefically "turned on", i.e., by setting them to `TRUE` in the function call of `kplus_anticlustering()`. If other higher order moments should be included as part of the optimization, the optional argument `moments` can be used to specify the desired moments as an integer vector. The argument `method = "local-maximum"` ensures that the k-plus criterion is optimized using the local maximum search method described in the previous section. The argument `K` describes the number of groups. By default, the function ensures that all features are standardized before the optimization starts; in particular, this option enforces that all k-plus criteria receive the same weight during the optimization process. This behaviour can be adjusted using the boolean argument `standardize`.

Table 2 shows the results of the anticlustering application by listing the descriptive statistics (means, standard deviations and skewness) for each of the 9 groups and for each of the 3 features. Table 2 also contains the results after applying k-means anticlustering and diversity anticlustering. It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations, and the skewness was also very evenly matched. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations and skewness. Diversity anticlustering was well-suited to match means, standard deviations and skew, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---diversity anticlustering does not directly maximize similarity with regard to these criteria, but instead maximizes within-group heterogeneity.

```{r}

example1_file <- "results_oasis_same_size.csv"
if (!file.exists(example1_file)) {

    oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
    )

    features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

    K <- 9

    # k-plus variance / skew
    groups_kplus <- kplus_anticlustering(
        features,
        K = K,
        variance = TRUE,
        skew = TRUE,
        method = "local-maximum",
        standardize = TRUE
    )

    # k-means
    groups_kmeans <- anticlustering(
        features,
        K = K,
        objective = "variance",
        method = "local-maximum",
        standardize = TRUE
    )

    # diversity
    groups_diversity <- anticlustering(
        features,
        K = K,
        objective = "diversity",
        method = "local-maximum",
        standardize = TRUE
    )


    # Functions to compute means / SD / skew per group 
    descriptives_by_group <- function(features, anticlusters, FUN, name) {
        df <- data.frame(
            t(data.frame(lapply(by(features, anticlusters, FUN), c)))
        )
        df$group <- 1:nrow(df)
        df$Descriptive = name
        df
    }


    all_descriptives_by_group <- function(features, anticlusters) {
    means <- descriptives_by_group(features, anticlusters, colMeans, "mean")
    sds <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, sd),
        "sds"
    )
    skew <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, DescTools::Skew),
        "skew"
    )
    rbind(means, sds, skew)
    }


    kplus <- all_descriptives_by_group(features, groups_kplus)
    kplus <- data.frame(kplus, Objective = "kplus")

    kmeans <- all_descriptives_by_group(features, groups_kmeans)
    kmeans <- data.frame(kmeans, Objective = "kmeans")
    diversity <- all_descriptives_by_group(features, groups_diversity)
    diversity <- data.frame(diversity, Objective = "diversity")

    all_objs <- rbind(kplus, kmeans, diversity) %>% 
    pivot_wider(names_from = Descriptive, values_from = c(beauty_mean, Valence_mean, Arousal_mean))
    
    write.table(
        all_objs, example1_file, row.names = FALSE, sep = ";"
    )

    
} else {
  all_objs <- read.csv(example1_file, sep = ";")
}

table_all_objs <- subset(all_objs, select = -Objective)
table_all_objs <- as.matrix(table_all_objs)
colnames(table_all_objs) <- c("Group", rep(c("$M$", "$\\mathit{SD}$", "Skew"), 3))
table_all_objs <- apply(table_all_objs, 2, force_or_cut)
#mode(table_all_objs) <- "character"

apa_table(
  table_all_objs,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The k-plus criterion that was optimized included a term to minimize differences with regard to means, variances, and skewness.",
  escape = FALSE,
  align = c("c", rep("r", 9)),
  col_spanners = list(Beauty = c(2, 4), Valence = c(5, 7), Arousal = c(8, 10)), 
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Diversity anticlustering}" = 19:27)
)
```

Table 3 illustrates another advantage of k-plus anticlustering over diversity anticlustering: When creating groups of unequal size, diversity anticlustering tends to increase the spread of the data in the largest group in comparison to the other groups. K-plus anticlustering strives for between-group similarity regardless of group size. Unequal group sizes can be requested by passing the different group sizes to the argument `K`. The following code was used to equalize means and variances between four groups using the k-plus approach:

```R
kplus_anticlustering(
  features,
  K = c(100, 100, 100, 600),
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    if (criterion == "kplus") {
        oasis[[paste(criterion, "group", sep = "_")]] <-  kplus_anticlustering(
            features,
            K = c(100, 100, 100, 600),
            method = "local-maximum"
        )
    } else {
        oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
        features,
        standardize = TRUE,
        K = K,
        objective = criterion,
        method = "local-maximum"
        )
    }
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Diversity anticlustering}" = 9:12)
)
```

## Example Application II: Small data set ($N = 96$)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore, a second application illustrates that k-plus anticlustering yields satisfactory results in practice even when group sizes are smaller. I make use of a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Marie L. Schaper [@schaper2019metacognitive; @schaper2019metamory]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

Table 4 illustrates the results that are obtained when using standard[^standardkplus] k-plus anticlustering to divide the stimulus set into 6 groups of size `r nrow(schaper2019) / 6` each. Even though the means and standard deviations are no longer perfectly matched between groups, arguably they are similar enough for any practical purposes.

[^standardkplus]: I refer to *standard* k-plus anticlustering as the minimization of differences in means and variances, but not in higher order moments or covariances. 

```{r, echo = FALSE}
features <- schaper2019[, 3:6]
anticlusters <- anticlustering(
  features,
  K = 6,
  objective = "kplus",
  method = "local-maximum",
  repetitions = 5,
  standardize = TRUE
)
```

```{r}

tab <- mean_sd_tab(features, anticlusters)
tab <- cbind(1:6, 96 / 6, tab)
colnames(tab) <- c("Group", "N", "Typicality", "Atypicality", "Syllables", "Frequency")

apa_table(
  tab,
  caption = "Descriptive statistics by group for the Schaper et al. (2019a; 2019b) data set, after applying k-plus anticlustering.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r")
)

```

# Discussion

Anticlustering is a partitioning method that aims for high between-group similarity and high within-group heterogeneity. In this paper, I presented the k-plus criterion for anticlustering, which focuses on maximizing between-group similarity. K-plus represents between-group similarity as discrepancies in distribution moments between groups: means, variances, skewness, kurtosis, higher order moments, and covariances. Depending on the application, a different subset of these objectives can be included in the anticlustering process. The k-plus criterion is an extension of the classical k-means criterion, which only reflects how similar groups are with regard to their means. Interestingly, however, the k-plus extension does not change the original k-means criterion because additional criteria can be incorporated by augmenting the input data.

A simulation study and examples on real norming data showed that the k-plus criterion is well-suited to maximize between-group similarity with regard to multiple criteria. Given that similarity with regard to variances was achieved with almost no sacrifice to similarity in means, k-plus anticlustering should be considered as the default replacement of classical k-means when striving for between-group similarity.[^binarydata] The results did however show that fulfilling multiple criteria at the same time can be challenging in small data sets. In practice, users can try out different objectives and see which specification fulfills their needs. A reasonable approach is to start "greedy", e.g. by specifying that multiple distribution moments should equalized. If the results of such a greedy setup are subpar, users can reduce their requirements by opting for fewer criteria (e.g., only the mean and variance). Using the open source R package `anticlust`, the (repeated) application of k-plus anticlustering is easily accessible and free. 

[^binarydata]: A reasonable exception is given when the input data consists of binary variables because for binary data, the mean and the variance are directly dependent. 

## Limitations and outlook

The present paper focused on the new k-plus criterion for anticlustering, but only superficially considered the multicriterion nature of this criterion. A simple unweighted sum approach was used to optimize the k-plus objective, while more sophisticated algorithmic procedures exist to tackle multicriterion optimization problems [e.g., @brusco2009cross]. The sole focus on the objective function followed two reasonings. First, the applications showed that the local maximum search, maximizing the unweighted k-plus sum, yielded satisfying results that probably suffice in most applications. It is possible that k-plus anticlustering has a fortunate solution space where many suitable partitions exist that fulfill multiple criteria to a satisfactory degree (as suggested by Figure 1). Still, more sophisticated approaches may yield theoretically improved results, as well as the opportunity to deliberately select among partitionings that may be preferred according to different k-plus objectives. @brusco2019 gave an example of how direct algorithms can be employed to optimize multiple criteria in anticlustering applications aiming for high within-group heterogeneity. Future research should investigate if k-plus anticlustering may profit from similar algorithmic treatment. A second reason for focusing on the k-plus objective per se---independently from the algorithm that is used to optimize it---was to obtain a fair comparison to existing anticlustering objectives. In the simulation study, differences in performance could not have been interpreted if the objective function as well as the algorithm used to optimize it had differed between anticlustering methods. It was of particular interest to compare the k-plus objective with the popular diversity objective. @papenberg2020 compared k-means anticlustering and diversity anticlustering[^clusterediting] in a simulation study and concluded that the diversity criterion should be preferred because k-means does not equalize the spread of the distribution between groups. In contrast, maximizing the diversity leads to an appropriate balance between location and spread. Using the k-plus extension, however, the k-means criterion has regained attractivity: When opting for similarity in means and variances---and for increasing $N$, skewness and kurtosis as well---, k-plus anticlustering clearly outperformed diversity anticlustering. 

[^clusterediting]: In their study, they used the term anticluster editing to refer to the maximization of the diversity criterion. This is because the diversity criterion is minimized as part of the cluster editing method [@shamir2004vf].

## Conclusion

This paper introduced k-plus anticlustering for maximizing between-group similarity, improving the classical k-means approach. K-plus anticlustering can be adopted whenever a group of elements has to be partitioned into equivalent parts. The method is accessible and easily usable via the R package `anticlust`.

\newpage

# References

\begingroup
<div id="refs" custom-style="Bibliography"></div>
\endgroup


\newpage

# (APPENDIX) Appendix {-}

```{r child = "appendix.Rmd"}
```

