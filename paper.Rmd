---
title             : "An improved k-means criterion for optimizing between-group similarity"
shorttitle        : "K-means anticlustering extended"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf.  


abstract: |
  This is the abstract.
  
keywords          : "K-Means, Anticlustering, Variance"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
options(digits = 2)
```

- It is often important to divide a set of elements into similar 
subgroups
- This task can be automized and open source solutions exist 
- Anticlustering = opposite of clustering, does that
- Objectives based on pairwise distances exist, but k-means also exists 
- k-means = optimizes similarity wrt to mean, but not other distribution
parameters, in particular not wrt to the variance
- here, I present an improved objective function for k-means 
anticlustering, which optimizes similarity wrt to means and variance / 
standard deviation at the same time

## A Motivating Example

```{r}

# set.seed(26178) # uncomment later

library(anticlust)

N <- 14
K <- 2
partitions <- generate_partitions(N, K)
features <- rnorm(N)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
diff_mean <- function(clusters, features) {
  abs(diff(tapply(features, clusters, mean)))
}

diff_vars <- function(clusters, features) {
  abs(diff(tapply(features, clusters, var)))
}

all_objectives_mean <- sapply(
  partitions,
  FUN = diff_mean,
  features = features
)

all_objectives_var <- sapply(
  partitions,
  FUN = diff_vars,
  features = features
)

df <- data.frame(
  diff_mean = all_objectives_mean,
  diff_var  = all_objectives_var
)

plot(df, pch = 4, col = "darkgrey", las = 1)
# Illustrate best partition wrt simililarity in means
points(
  all_objectives_mean[which.min(all_objectives_mean)],
  all_objectives_var[which.min(all_objectives_mean)],
  cex = 2, col = "#DF536B", pch = 16
)

df$sum <- rowSums(df)

points(
  all_objectives_mean[which.min(df$sum)],
  all_objectives_var[which.min(df$sum)],
  cex = 2, col = "#2297E6", pch = 16
)

points(
  all_objectives_mean[which.min(df$diff_var)],
  all_objectives_var[which.min(df$diff_var)],
  cex = 2, col = "#61D04F", pch = 16
)

```

The plot illustrates all possible ways to partition a data set of $N = 
14$ into two parts (there are `r length(partitions)` possible 
partitions). Red point: maximum similarity in means; green point 
= maximum similarity in variance. Blue point: Almost no difference to 
red point wrt similarity in means, but variance is much more similar. 
This blue partition is certainly preferable to the red one when maximal 
overall similarity is required; the red one just optimizes similarity in 
means (which is what maximizing the k-means objective does). 

Unfortunately, improving algorithmic procedures will not help to find 
better partitions -- the objective function has to change. I will show 
an objective function that captures both similarity in means and 
variance.

## K-Means criterion

In my treatment of k-means clustering, I adopt the notation Steinley 
(2006) provided in his thorough synthesis of the k-means literature. In 
clustering---or anticlustering---, an item pool $X$ = $\{x_1, \ldots, 
x_N\}$ has to be partitioned into $K$ groups $\{C_1, \ldots, C_K\}$. We 
will refer to the groups $C_j$, $j = 1, \ldots, K$, as anticlusters, 
clusters, or groups, and to the collection of anticlusters $C$ as the 
partitioning of the data set. We assume that each element $x_i$ ($i = 1, 
\ldots, N$), is a vector of length $P$, and each entry describes one of 
its numeric measurements. Generally hence, $X$ can be interpreted as an 
$N \times M$ matrix where each row represents an data point $x_i$ (e.g., 
representing a stimulus or a person) and each column is a numeric 
attribute (e.g., the number of syllables in a word, or the 
attractiveness rating of a face). 

K-Means anticlustering aims to maximize the error sum of squares (SSE), 
also known as the within-group variance, which is the sum of the 
squared Euclidean distances between each data point and its cluster 
centroid. The cluster centroid is given by averaging the values on each 
variable over the elements within that cluster. The centroid of the 
*j*th variable in the *k*th cluster, consisting of $n_k$ elements, is 
given as: 

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits {i \in C_k} x_{ij}
$$

The k-means objective function, the error sum of squares, is then given 
as:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

---
  
**This is old, adjust to Steinley (2006):**

The squared distance between an element $x = (x_1, \ldots, x_M)$ and the 
average of all elements $\overline{\mathrm{\boldsymbol{x}}}$ is:

$$
\vert \vert x - \overline{\mathrm{\boldsymbol{x}}} \vert \vert^2 = 
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2
$$

Now define new data points representing the squared distances between 
data points and the grand mean:

$$ 
y_i = (x_i - \mathrm{\boldsymbol{x}})^2, \forall i \in N
$$

where $\mathrm{\boldsymbol{x}}$ is a vector of length $M$ containing 
the average for each feature.

Now:

$$
\sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
  \sum\limits_{m = 1}^{M}(y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2 =
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
    (y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2
$$

If we define $x_{m + 1} = y_1$, $x_{m + 2} = y_2$, ..., $x_{2M} = y_M$.
this is the same as:

$$
\sum\limits_{m = 1}^{2M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 
$$

Thus, we redefine the data points and include an additional $M$ features 
for each element. Then, we maximize $\mathit{Var}_{\mathrm{within}}$, 
which corresponds to a tradeoff between minimizing differences in mean 
features between clusters, and the average squared distance to the grand 
mean, i.e., the within-cluster variance. That is: We try to maximize the 
within-cluster variance, but to a certain degree we also strive to 
obtain similar within-cluster variance in each cluster.

Important to note: We solve the same optimization problem, we just 
change the input a bit (we double the number of "columns"). 

**Example table!!**

# Stuff

New objective seems to work best when N is high relative to K; 
optimizing two objectives at the same time is hard when N is low. Easier 
when only optimizing one objective (i.e., the mean, as done by normal
k-means anticlustering).

# Test equivalence of objectives

Check that I can actually just append the squared difference data as 
additional columns to the original data frame:

```{r}

squared_from_mean <- function(data) {
  apply(data, 2, function(x) (x - mean(x))^2)
}


N <- 500
M <- 20
data <- matrix(rnorm(N * M), ncol = M)
var_data <- squared_from_mean(data)

K <- 10
clusters <- sample(rep_len(1:K, N))

obj1 <- variance_objective(data, clusters) + variance_objective(var_data, clusters)
obj2 <- variance_objective(cbind(data, var_data), clusters)

tolerance <- 1e-10

obj1 - obj2 < tolerance

obj1
obj2

```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
