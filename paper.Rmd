---
title             : "An improved k-means criterion for optimizing between-group similarity"
shorttitle        : "K-means anticlustering extended"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf.  


abstract: |
  This is the abstract.
  
bibliography      : ["lit.bib"]
  
keywords          : "K-Means, Anticlustering, Variance"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
options(digits = 2)
```

Oftentimes researchers have to divide a set of elements different, but 
equivalent groups. For example, this problem arises when 
splitting a test into parts of equal difficulty [@gierl2017], when 
assigning students to work groups [@baker2002], or when assigning 
stimuli to experimental conditions [@lahl2006]. Solving such problems 
manually is a tedious and time-consuming task and the similarity of 
manually established groups is usually improvable. Fortunately, these 
problems can be formalized mathematically [e.g., @brusco2019; 
@spath1986] and by now, accessible open source solutions for tackling 
this automatically exist [@papenberg2020].

Forming similar groups is also known as the *anticlustering* problem. 
Anticlustering reverses the logic of its better known twin, cluster 
analysis [@rokach2005]. In cluster analysis, groups are sought that are 
internally homogenous, whereas there is heterogeneity between groups 
[@steinley2006]. Anticlustering instead strives for high between-group 
similarity high within-group heterogeneity. Usually, cluster analysis 
aims to uncover "true" latent structure contained in a data set, whereas 
anticlustering imposes an "unnatural" division of the data.

One well-known criterion for anticlustering reverses the k-means 
clustering method by maximizing the within-group variation, or 
*variance* [@jain2010; @steinley2006; @spath1986; @valev1998; 
@valev1983]. Späth and Valev independently coined the term 
anticlustering specifically to denote the reversal of classical 
k-means. 


--- k-means

--- dissimilarity data vs. rectangular data

Although forming groups with high between-group similarity is often 
equivalent to establishing high within-group diversity, researchers 
usually only wish to obtain one of these goals. In experimental 
psychology, sets of stimuli should be as similar as possible; within-set 
heterogeneity is only an unimportant side-product. In this paper, I 
address such problems that require high between-group similarity. I 
present a novel extension to the k-means objective function, 
within-group sum-of-squares that improves its capacity to form similar 
groups [@steinley2006], but is no longer aims to maximize heterogeneity.

Ironically, even though maximizing the k-means objective maximizes 
similarity with regard to the means of the 

- here, I present an improved objective function for k-means 
anticlustering, which optimizes similarity wrt to means and variance / 
standard deviation at the same time



## A Motivating Example

```{r}

# set.seed(26178) # uncomment later

library(anticlust)

N <- 14
K <- 2
partitions <- generate_partitions(N, K)
features <- rnorm(N)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
diff_mean <- function(clusters, features) {
  abs(diff(tapply(features, clusters, mean)))
}

diff_vars <- function(clusters, features) {
  abs(diff(tapply(features, clusters, var)))
}

all_objectives_mean <- sapply(
  partitions,
  FUN = diff_mean,
  features = features
)

all_objectives_var <- sapply(
  partitions,
  FUN = diff_vars,
  features = features
)

df <- data.frame(
  diff_mean = all_objectives_mean,
  diff_var  = all_objectives_var
)

plot(df, pch = 4, col = rainbow(length(partitions)), las = 1)
# Illustrate best partition wrt simililarity in means
points(
  all_objectives_mean[which.min(all_objectives_mean)],
  all_objectives_var[which.min(all_objectives_mean)],
  cex = 2, col = "#DF536B", pch = 16
)

df$sum <- rowSums(df)

points(
  all_objectives_mean[which.min(df$sum)],
  all_objectives_var[which.min(df$sum)],
  cex = 2, col = "#2297E6", pch = 16
)

points(
  all_objectives_mean[which.min(df$diff_var)],
  all_objectives_var[which.min(df$diff_var)],
  cex = 2, col = "#61D04F", pch = 16
)

```

The plot illustrates all possible ways to partition a data set of $N = 
14$ into two parts (there are `r length(partitions)` possible 
partitions). Red point: maximum similarity in means; green point 
= maximum similarity in variance. Blue point: Almost no difference to 
red point wrt similarity in means, but variance is much more similar. 
This blue partition is certainly preferable to the red one when maximal 
overall similarity is required; the red one just optimizes similarity in 
means (which is what maximizing the k-means objective does). 

Unfortunately, improving algorithmic procedures will not help to find 
better partitions -- the objective function has to change. I will show 
an objective function that captures both similarity in means and 
variance.

## K-Means criterion

In this treatment of k-means, I adopt the notation Steinley (2006) 
provided in his synthesis of the k-means literature. In clustering---or 
anticlustering---, an item pool $X$ = $\{x_1, \ldots, x_N\}$ has to be 
partitioned into $K$ groups $\{C_1, \ldots, C_K\}$. We assume that each 
element $x_i$ ($i = 1, \ldots, N$), is a vector of length $P$, and each 
entry describes one of its numeric measurements. Generally, $X$ 
can is interpreted as an $N \times M$ matrix where each row represents 
an data point $x_i$ (e.g., representing a stimulus or a person) and each 
column is a numeric attribute (e.g., the number of syllables in a word, 
or the attractiveness rating of a face). 

K-Means anticlustering aims to maximize the error sum of squares (SSE), 
the within-group variance, which is the sum of the squared Euclidean 
distances between each data point and its cluster centroid. The cluster 
centroid is given by averaging the values on each variable over the 
elements within that cluster. The centroid of the *j*th variable in the 
*k*th cluster, consisting of $n_k$ elements, is given as: 

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits {i \in C_k} x_{ij}
$$

The k-means objective function, the error sum of squares, is then given 
as:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

In anticlustering applications, we usually assume that all cluster are 
of equal size. Hence 

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

---
  
**This is old, adjust to Steinley (2006):**

The squared distance between an element $x = (x_1, \ldots, x_M)$ and the 
average of all elements $\overline{\mathrm{\boldsymbol{x}}}$ is:

$$
\vert \vert x - \overline{\mathrm{\boldsymbol{x}}} \vert \vert^2 = 
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2
$$

Now define new data points representing the squared distances between 
data points and the grand mean:

$$ 
y_i = (x_i - \mathrm{\boldsymbol{x}})^2, \forall i \in N
$$

where $\mathrm{\boldsymbol{x}}$ is a vector of length $M$ containing 
the average for each feature.

Now:

$$
\sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
  \sum\limits_{m = 1}^{M}(y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2 =
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
    (y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2
$$

If we define $x_{m + 1} = y_1$, $x_{m + 2} = y_2$, ..., $x_{2M} = y_M$.
this is the same as:

$$
\sum\limits_{m = 1}^{2M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 
$$

Thus, we redefine the data points and include an additional $M$ features 
for each element. Then, we maximize $\mathit{Var}_{\mathrm{within}}$, 
which corresponds to a tradeoff between minimizing differences in mean 
features between clusters, and the average squared distance to the grand 
mean, i.e., the within-cluster variance. That is: We try to maximize the 
within-cluster variance, but to a certain degree we also strive to 
obtain similar within-cluster variance in each cluster.

Important to note: We solve the same optimization problem, we just 
change the input a bit (we double the number of "columns"). 

**Example table!!**

# Stuff

New objective seems to work best when N is high relative to K; 
optimizing two objectives at the same time is hard when N is low. Easier 
when only optimizing one objective (i.e., the mean, as done by normal
k-means anticlustering).

- Two examples: $K = 2$, $N = 20$ vs. $N = 100$, using classical
k-means: only mean; using k-variance: only variance; combination:
both, but works better for larger $N$

# Weighted k-means

Using variable weighting (see Steinley, 2006, p. 13), we can 
adjust the relative importance of minimizing differences in means
and variances. Standard in `anticlust`: Equal weight, this is 
achieved by first creating the new variables, representing 
the squared differences from the mean, and then standardizing
the entire feature matrix. If $N$ is large enough (relative to $K$),
there is usually no need for weighting.

# Simulation 

- show that k-variance anticlustering outperforms other objectives

# Examples

- use some examples (e.g., OASIS data set)

# Combined approach better than k-variance alone

- Often, maximizing k-means-variance yields more similar variances 
than k-variance alone. Why is that? I guess: structure in the solution 
space, local maxima are real. When also considering means, you more 
often escape bad local maxima?

# Test equivalence of objectives

Check that I can actually just append the squared difference data as 
additional columns to the original data frame:

```{r}

squared_from_mean <- function(data) {
  apply(data, 2, function(x) (x - mean(x))^2)
}


N <- 500
M <- 20
data <- matrix(rnorm(N * M), ncol = M)
var_data <- squared_from_mean(data)

K <- 10
clusters <- sample(rep_len(1:K, N))

obj1 <- variance_objective(data, clusters) + variance_objective(var_data, clusters)
obj2 <- variance_objective(cbind(data, var_data), clusters)

tolerance <- 1e-10

obj1 - obj2 < tolerance

obj1
obj2

```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
