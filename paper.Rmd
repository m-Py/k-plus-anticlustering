---
title             : "Extending the Bicriterion Approach for Anticlustering: Optimal and Heuristic Approaches"
shorttitle        : "Extending the Bicriterion Approach for Anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich-Heine-Universität Düsseldorf, Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"
  - name          : "Martin Breuer"
    affiliation   : "1"
    corresponding : no
  - name          : "Max Diekhoff"
    affiliation   : "1"
    corresponding : no
  - name          : "Gunnar W. Klau"
    affiliation   : "1"
    corresponding : no

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"
    

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf. Martin Breuer, Gunnar W. Klau, Max Diekhoff, 
  Department of Computer Science, Heinrich Heine University Düsseldorf.
  
abstract: |

    Numerous applications in psychological research require that a data set is partitioned via the maximization---instead of the more common minimization---of a clustering criterion. Such anticlustering seeks for high similarity between groups (i.e., maximum diversity) or high pairwise dissimilarity within groups (i.e., maximum dispersion). Brusco et al. (2020) proposed a powerful bicriterion algorithm (BILS) that simultaneously seeks for maximum diversity and dispersion, introducing the bicriterion approach for anticlustering. Here, we advance the bicriterion approach in several ways. As our main contribution, we combine a novel exact algorithm for maximum dispersion with the BILS heuristic to obtain high diversity on top of an optimal dispersion. Despite its theoretical computational hardness, our optimal method scales to rather large data sets. It generates multiple partitions that we use as initializations of the BILS's local search, thus guaranteeing that the BILS also returns a partition with optimal dispersion. In a simulation study, we compared several adaptations of the BILS. To obtain high diversity, initializing the BILS with multiple optimal partitions is preferable to using only one. Moreover, in data sets where maintaining optimal dispersion severely restricts the search for feasible partitions, the iterated local search phase of the BILS is crucial to obtain high diversity. While the original BILS oftentimes finds optimal dispersion values, our simulation illustrates conditions when it does not. The optimal dispersion algorithm and the BILS including our extensions are available via the free and open source R package `anticlust`. Practical examples are included to illustrate their application.

bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, maximum dispersion, optimal algorithm, bicriterion optimization"
wordcount         : "12345"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

output            : 
  papaja::apa6_pdf

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \floatstyle{plaintop}
  - \restylefloat{figure}
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - |
    \makeatletter
    \renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
      \hskip -\arraycolsep
      \let\@ifnextchar\new@ifnextchar
      \array{#1}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

---

```{r setup, include = FALSE}
library(papaja) # use the development version: `remotes::install_github("crsh/papaja@devel")`; currently 0.1.1.9001
library(anticlust)
library(knitr)
# library(dplyr)
# library(ggplot2)
# library(tidyr)
# library(DescTools)
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library(prmisc) # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```


Partitioning data sets via anticlustering has uses in many research fields including psychology [@brusco2019; @schaper2023; @papenberg2020], test assembly [@gierl2017], education [@krauss2013; @baker2002], artificial intelligence [@steghofer2013], machine learning [@mauri2023], network systems [@mohebi2022], and operations research [e.g. @gallego2013; @gliesch2021]. Anticlustering seeks for similarity between groups and heterogeneity within groups by maximizing a clustering objective criterion. Thus, anticlustering is the logical and mathematical opposite of classical clustering, which partitions data sets via minimization of the same criteria [@steinley2006; @brusco2019; @spath1986].

Formally, anticlustering is used to partition $N$ objects into $K$ exhaustive and mutually exclusive subsets $C = (C_1, \ldots, C_K)$. The partitioning restrictions are formalized as follows: 

\begin{align}
\vert C_k \vert = n_k, \; (k = 1, \ldots, K) \label{formalization:1} \\
\sum_{k = 1}^{K} n_k = N \label{formalization:2} \\
C_j \cap C_k = \emptyset (j, k = 1, \ldots, K), \; j \ne k\label{formalization:3} \\
\end{align}

$C_k$ is the set of $n_k$ objects in the $k^{th}$ group, and $K$ and $n_k$ are predetermined by the application. We will refer to the subsets $C_k$ ($k = 1, \ldots, K$), as anticlusters, clusters, or groups interchangeably, and to the collection $C$ as the anticlustering partitioning. 

We assume that a data matrix $\mathbf{Z}$ is used as input for computing the anticlustering criterion on the basis of a partitioning $C$. $\mathbf{Z}$ is either a feature matrix $\mathbf{X}_{N \times M} = \{x_{ij}\}_{N \times M}$ where the $M$ columns contain numeric measurements on the $N$ objects, or a symmetric matrix $\mathbf{D}_{N \times N} = \{d_{ij}\}_{N \times N}$ of pairwise dissimilarity measurements. The partitioning is obtained by maximizing a clustering criterion $f_i(C, \mathbf{Z})$. For example, *k*-means anticlustering aims to maximize between-group similarity by maximizing the error sum of squares, i.e., the sum the squared distances between each object and its cluster center [@spath1986; @papenberg2024]:

$$
f_1(C, \mathbf{X}_{N \times M}) =
  \sum\limits_{j=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

The $k^{th}$ cluster center $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ contains the mean values of the $M$ variables computed across all observations belonging to the $k^{th}$ group. Maximizing $f_1$ ensures that the cluster centers are as close to each other as possible [@spath1986]. 

The *k*-plus criterion is an extension of the $k$-means criterion for anticlustering that can be implemented by adding fictitious variables to the data matrix $\mathbf{X}_{N \times M}$ [@papenberg2024]. *K*-plus anticlustering is used to equate not only means between clusters, but higher order moments such as the variance or skewness as well. For example, to equate means and variances, we define a new data matrix $\mathbf{Y}_{N \times M} = \{y_{ij}\}_{N \times M}$ with $y_{ij} = (x_{ij} - \overline{x}_j)^2$, where $\overline{x}_j$ is the overall mean of the $j^{th}$ attribute. Then, the *k*-plus criterion is computed as

$$
f_2(C, \mathbf{(X | Y)_{N \times 2M}}) =
  \sum\limits_{j=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Another important anticlustering criterion is the *diversity*, which uses a dissimilarity matrix $\mathbf{D}_{N \times N}$ as input. It is computed as the within-group sum of pairwise dissimilarities across all clusters [@brusco2019; @papenberg2020; @feo1990]:
 
$$
f_3(C, \mathbf{D}_{N \times N}) = \sum\limits_{k = 1}^{K} \sum_{(i < j) \in C_k} d_{ij}
$$

Maximizing the diversity simultaneously minimizes the sum of distances between objects not in the same group. Therefore, the diversity represents within-group heterogeneity as well as between-group similarity [@feo1990]. Being a sum instead of an average, however, it no longer adequately captures between-group similarity if group sizes are not equal [@papenberg2024]. 

The diversity is oftentimes obtained by first converting the data matrix $\mathbf{X}_{N \times M}$ into a matrix of pairwise Euclidean distances [@brusco2019; @papenberg2020; @papenberg2024; @gallego2013]. If $\mathbf{D}_{N \times N}$ is obtained by using the squared pairwise Euclidean distances instead, we note an important equivalence between the k-means criterion $f_1$ and the diversity $f_3$. Traditionally, the k-means criterion is denoted via a sum of squared distances between data points and cluster centroids. However, it can also be expressed using the pairwise squared Euclidean distances between data points (Brusco, 2006; Späth, 1980). If we let $\mathbf{D'}_{N \times N}$ = \{$d'_{ij}$\} represent the squared Euclidean distance, i.e., $d'_{ij} = \sum\limits_{m = 1}^{M}(x_{im} - x_{jm})^2$, the k-means criterion can be expressed as as

$$
f_4(C, \mathbf{D'}_{N \times N}) = 
  \sum\limits_{k = 1}^{K}\left( \frac{1}{n_k} \sum_{(i<j)\in C_k} d'_{ij} \right) = 
  f_1(C, \mathbf{X}_{N \times M}).
$$

That is, "the sum of squared distances of a collection of points from the centroid associated with those points is equal to the sum of the pairwise squared distances between those points divided by the size (number of objects) of the collection" (Brusco, 2006, p. 350). An important special case of anticlustering applications is that all groups are equal-sized, i.e., $n_k = \frac{N}{K}$ ($k = 1, \dots K$). In this context, $f_4$ can be simplified to $f_4^*$ via

$$
f_4^*(C, \mathbf{D'}_{N \times N}) = 
  \frac{N}{K} \sum\limits_{k = 1}^{K} \sum_{(i<j)\in C_k} d_{ij}
$$

In the context of maximization, the normalizing factor $\frac{N}{K}$ can be ignored because it does not depend on the partitioning $C$. Hence, 

$$
f_4^*(C, \mathbf{D'}_{N \times N})
  \propto \sum\limits_{k = 1}^{K} \sum_{(i<j)\in C_k} d_{ij} = f_3(C, \mathbf{D'}_{N \times N})
$$

For anticlustering, the criterion $f_4^*$ (i.e., the k-means criterion for equal-sized groups) reduces to the sum of the (squared) distances between elements in the same cluster, i.e., the standard diversity $f_3$. Therefore, for equal-sized groups, the diversity anticlustering criterion $f_3$ is equivalent to the k-means criterion $f_1$---if the diversity is computed via the squared Euclidean distance. We will use this insight later when we discuss applying algorithms for maximum diversity to the $k$-means and $k$-plus objectives. For arbitrary measures of dissimilarity (and not just the squared Euclidean distance), we denote $f_4$ as the *average diversity criterion*.

Unlike $f_1$, $f_2$, $f_3$, and $f_4$, the *dispersion* criterion does not represent between-group similarity, but is a pure measure of within-group heterogeneity. It is defined as the minimum dissimilarity across any two objects that are part of the same group [@brusco2019; @fernandez2013]:

$$
f_5(C) = \textrm{min}_{(i < j) \in C_k} d_{ij}
$$

The dispersion is a measure of the worst-case pairwise dissimilarity. Maximizing the dispersion ensures that any two objects in the same group are as dissimilar from each other as possible. 

@brusco2019 outlined the importance of simultaneously considering multiple criteria when addressing anticlustering problems. In particular, they argued that anticlustering algorithms should incorporate an objective of between-group similarity---such as the diversity---and pairwise within-group dissimilarity---i.e., the dispersion---at the same time. To this end, @brusco2019 presented a bicriterion algorithm that simultaneously maximizes the diversity $f_3$ and the dispersion $f_5$. We endorse Brusco et al.'s bicriterion approach for anticlustering, and advance it in several ways in the current paper. First, we allow that not only the diversity $f_3$ can be optimized alongside the dispersion, but instead we allow that the k-means, k-plus or the average diversity can be used as measures of between-group similarity (i.e., the criteria $f_1$, $f_2$ and $f_4$). Second, we allow that the dispersion can be optimized on the basis of a different dissimilarity matrix than the diversity (or a different objective of between-group similarity). This is useful if the criteria should be based on different information, or to induce custom cannot-link constraints, as we will show. As our most important contribution, we combine a novel exact algorithm for maximum dispersion with the BILS heuristic to obtain high diversity on top of an optimal dispersion. Our paper is organized as follows. In the next section, we review the bicriterion method by @brusco2019 in detail. After that, we provide the technical background for our extensions. We then provide several evaluations of our contributions: (a) we probe the practical feasibility of our optimal algorithm for maximum dispersion, (b) we compare different adaptations of the BILS with regard to their ability to providing high diversity on top of an optimal dispersion, and (c) we provide practical examples on using our new methods. 

## The BILS

## Algorithms for anticlustering

Heuristics dominate (local maximum search @spath1986; @weitz1998, multiple repetitions robust for k-means @steinley2007, ILS @brusco2017, VARIABLE NEIGHBORHOOD SEARCH @urovsevic2014; tabu search @gallego2013; multiple improvement phases based on initial local maximum search @yang2022). Exact algorithms for diversity [@papenberg2020; @schulz2022] and for the dispersion [@fernandez2013; @gliesch2021; @brucker1978]. Exact algorithms for diversity perform much worse than for the reversed clustering problems, where many hundred elements can often be processed. Maximum diversity can be achieved with much better success. Our novel contribution is to combine optimal maximum dispersion with high diversity. We present a novel algorithm for maximum diversity that is similar to the approaches by @fernandez2013 and @gliesch2021 and is based on the logic that was already introduced by @brucker1978. It scales to hundreds or even thousands of data points and therefore can be used to solve most real-world applications we have in mind. We then proceed to discuss how on top of an optimal dispersion, we can implement high diversity. A logical next step is to obtain optimal maximum diversity as well, which is however only limited to rather small data sets, as we show. Due to its bicriterion nature, the BILS by @brusco2019 is naturally suited to maintain optimum dispersion while optimizing diversity. We discuss several adaptations of the BILS, which are compared in a simulation study. 

# Discussion

\newpage

# References

\begingroup
<div id="refs" custom-style="Bibliography"></div>
\endgroup

