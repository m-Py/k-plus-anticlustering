---
title             : "k-plus Anticlustering: An Improved k-means Criterion for Maximizing Between-Group Similarity"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

abstract: |

    Anticlustering refers to a collection of methods that partition data sets into disjoint groups while aiming for high between-group similarity and high within-group heterogeneity. Anticlustering is usually approached by maximzing instead of minimizing a clustering objective such as the k-means criterion. Introducing k-plus, this paper presents an extension of k-means to maximize between-group similarity. While k-means anticlustering minimizes between-group-differences only with regard to the mean of attribute values, k-plus anticlustering can be used to minimize between-group-differences with regard to the variance, higher order moments, and covariance structure. To minimize differences in variance, each variable is duplicated by computing the squared deviation between the original values and the variable's mean. Because the variance is defined as the expected value of the squared deviations from the mean, appending these duplicated variables to the data set---and then applying standard k-means anticlustering---suffices to equalize the variance between groups. Following a similar logic, differences in skewness, kurtosis and covariances can be minimized. Thus, while k-plus is a separate criterion for anticlustering, it can be reduced to an augmentation of the input data while the original k-means objective remains unchanged. A computer simulation and practical examples on real data show that k-plus anticlustering achieves high similarity with regard to mean attribute values as well as the variance and higher order moments, usually with no noticable sacrifice to either criterion. The k-plus extension is therefore preferred over classical k-means anticlustering for optimizing between-group similarity. Example are given on how k-plus anticlustering can be applied using the open source R package `anticlust`, which is freely available via CRAN (https://cran.r-project.org/package=anticlust) and Github (https://github.com/m-Py/anticlust).
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, covariance, skewness, kurtosis, higher order moments"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : 
  papaja::apa6_pdf: 
    number_sections: true
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
library("dplyr")
library("ggplot2")
library("tidyr")
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library("prmisc") # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```

Anticlustering is a collection of methods to partition a set of elements into disjunct groups, with the goal of obtaining high between-group similarity or within-group heterogeneity [@brusco2019; @spath1986; @valev1998]. Oftentimes, both goals coincide: some anticlustering objectives imply that similarity between groups is maximal whenever similarity within groups is minimal [@brusco2019; @papenberg2020; @feo1990]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and heterogeneity between clusters [@rokach2005; @steinley2006]. Whereas a variety of different approaches exist for cluster analysis such as hierarchical methods, model-based clustering, or network approaches [see @brusco_emergent_2012, for an overview], anticlustering is usually formalized as a partitioning process that divides a set of elements into disjunct groups in such a way that an objective function---representing between-group similarity and/or within-group heterogeneity---is optimized [@papenberg2020; @spath1986; @brusco2019].

Anticlustering is useful in many tasks surrounding research work in psychology. For example, anticlustering can be used to split a test into parts of equal difficulty [@gierl2017], to assign students to work groups [@baker2002] or when assigning stimuli to different---but parallel---experimental conditions [@lahl2006]. For a more comprehensive overview of anticlustering applications in Psychology, see @brusco2019 and @papenberg2020. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, these problems can be formalized as mathematical optimization problems [e.g., @brusco2019; @spath1986; @baker2002; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods can be distinguished by the input data and the objective function that is used to optimize group between-group similarity / within-group heterogeneity. Some anticlustering objectives quantify group similarity on the basis of pairwise dissimilarity rating such as the Euclidean distance. The most prominent criterion on the basis of pairwise dissimilarity ratings is the *diversity*, which is the total sum of dissimilarities between any elements within the same group. Maximizing the diversity optimizes within-group heterogeneity, which simultaneously leads to similar groups---if all groups have the same size. Anticlustering objective functions usually represent the reversal of a clustering objective, only the direction of the optimization is changed.  Another important criterion based on pairwise dissimilarities is the *dispersion*, which is the minimum dissimilarity between any two elements within the same group [@fernandez2013; @brusco2019]. Maximizing the dispersion increases the within-group heterogeneity by ensuring that any two elements in the same group as dissimilar from each other as possible.

Oftentimes, researchers work with attribute values [or *features*; @dry2009] instead of dissimilarity ratings. This use case is for example common when selecting stimuli for an experiment on the basis of norming data [e.g., @kurdi2017introducing].  For word stimuli in psycholinguistic experiments, norming data may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attribute values are binary and may represent the presence or absence of a feature [@tversky1977]. In the context of anticlustering, one approach for handling attribute data is to compute an appropriate dissimilarity measure such as the pairwise Euclidean or squared Euclidean distance across the set of attributes [@brusco2019]. Subsequently, an anticlustering criterion such as the diversity can be optimized to partition the elements into groups. A different approach to anticlustering directly works with the attribute data by maximizing the k-means criterion. K-means is probably the best-known clustering method. Reversing the k-means objective function---using maximization instead of minimization---has been independently recognized as a useful anticlustering tool by Späth [-@spath1986] and Valev [-@valev1998; -@valev1983]. 

In k-means clustering, the sum of the squared Euclidean distances between elements and the center of the cluster they are assigned to is minimized [@brusco2006branch], usually using the k-means heuristic [@jain2010; @steinley2006]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. For the anticlustering application, the k-means criterion is maximized instead, thereby achieving similarity between groups. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. Hence, groups are similar with regard to the mean of the distribution of the numeric attributes wheras other parameters---such as the variance---are ignored. When requiring that groups are overall similar to each other, this focus may be misguided as similar means can be obtained even when the underlying distributions are different [e.g., @anscombe1973]. @papenberg2020 showed that maximizing the diversity (using the Euclidean distance) is better suited to minimize difference with regard to both the mean and the variance of the data; even an entirely random split usually resulted in more similar variances because k-means solely focuses on the features' means.

To optimize overall group similarity---and hence to overcome the limitations of k-means anticlustering---this paper presents k-plus anticlustering. The k-plus criterion is an extension of the k-means objective that includes one or several terms to quantify similarity with regard to other important characteristics of a data set rather than only the mean. Hence, the k-plus criterion is not a single objective function for anticlustering, but instead a familiy of objectives that extend the standard k-means objective to minimize differences between groups with regard to the variance, covariances, and higher order moments such as skewness and kurtosis. Which of these objectives is pursued depends on a researchers' needs. In the most basic---and arguably most important---case, k-plus anticlustering can be used to not just minimize differences with regard to the mean of variables but also their variance. Measures of location and spread are arguably most important to researchers when they "eyeball" their data and they are routinely reported as characterizations of a data set. Hence, by default, the k-plus objective is refered to when considering the mean and the variance of variables in the anticlustering process. 

While less routinely reported, skewness are kurtosis are also important descriptive characterizations of a data set [@decarlo1997; @westfall2014]. To optimize overall similarity between groups, it may thus be desirable to also minimize differences with regard to these distribution moments. In principle, any other higher order moments can also be included as part of the k-plus objective. Futhermore, k-plus anticlustering can also be used to obtain similar covariance structures among groups. This approach may be of interest in cross validation applications where prediction accuracy depends on the correlations among features and criteria [@zeng2000; @papenberg2020]. Interestingly, in each case, the k-plus criterion can be reduced to an augmentation of the original data matrix, leaving the original k-means objective unchanged. 

After mathematically introducing the k-plus objective, a simulation study is reported showing that k-plus anticlustering is well suited to create groups that have minimal differences according to multiple distribution characteristics. Fulfilling multiple criteria at the same time becomes more feasible as the size of the data set increases. In two pratical examples I illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust`, an extension to the popular statistical programming language R [@papenberg2020; @R-base; @R-anticlust].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means clustering (and anticlustering) is used to partition two-way, two-mode data into groups. That is, $N$ elements each having $P$ attributes are assigned to $K$ groups $(C_1., \ldots, C_K)$, $C_k$ being the set of $n_k$ objects in group $k$. The partitioning is subject to the requirement that each element is assigned to exactly one group, formalized by the following two constraints [@papenberg2020]:

$$
\bigcup\limits_{k = 1}^{K} C_j = X
$$

$$
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k
$$

In anticlustering applications, we impose an additional constraint on the group sizes $n_k$, where the most common restriction requires that all groups have equal size:

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares ($\mathit{SSE}$), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging the $k$th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note there is a direct connection between the $\mathit{SSE}$ and the location of the cluster centroids [@spath1986]. If the $\mathit{SSE}$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $\mathit{SSE}$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values across clusters.

## Quantifying differences in variance

In the following, I present a variation of the $\mathit{SSE}$ that quantifies how the variance of numeric data differs between groups. To motivate the new criterion, we first consider a set of one-dimensional observations $(x_1, \ldots, x_N)$. The extension to the general case of $P$ attributes will be straight forward.

The sample variance of a set of observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
y_i = (x_i - \overline{\mathbf{x}})^2
$$

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average, k-means anticlustering can be employed to equalize the variance across groups. That is, we can formulate an adaption of the $\mathit{SSE}$ to quantify the degree to which the variance differs between groups. First, however, we generalize the idea to the case of $P$ attributes. For each attribute, we have to compute the squared deviation between all values and the mean of the attribute. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$, where

$$
y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2.
$$

To reflect how strongly the variances of all attributes vary across the $K$ clusters, we now define the criterion $\mathit{SSE_{Var}}$:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Analogously to the standard $\mathit{SSE}$, the centroid values $\overline{y}_j ^{(k)}$ are given as

$$
\overline{y}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} y_{ij}.
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points.

## A bicriterion model: K-plus anticlustering {#kplus}

It is unlikely that an exclusive optimization of $\mathit{SSE_{Var}}$ (i.e., equalizing variances) is of interest without also considering the $\mathit{SSE}$ (i.e., equalizing means). A combination of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$, however, can be employed to simultaneously optimize both objectives, which is a reasonable idea when striving for overall between-group similarity. The most simple approach for such a bicriterion optimization is the weighted sum approach [@naidu2014; @marler2010]. That is, both criteria are computed independently from each other and then combined into a single criterion through a weighted sum:

$$
w_1 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + 
w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

The weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of equalizing means and variances, respectively. Some simplifications can be arranged when ignoring the weights $w_1$ and $w_2$, e.g., by setting them both to 1. When defining $z_i = (x_{i1}, \ldots, x_{iP}, y_{i1}, \ldots, x_{iP})$, i.e., the concatenation of the $x_{ij}$ and $y_{ij}$ values, the bicriterion $\mathit{SSE_{kplus}}$ can be computed as

$$
\mathit{SSE_{kplus}} =
  \sum\limits_{j=1}^{2 P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (z_{ij} - \overline{z}_j ^{(k)})^2
$$.

This formula again has the same form as $\mathit{SSE}$ (and $\mathit{SSE_{Var}}$), but is computed on different data. In this case, both the original data $x_{ij}$ as well as the squared distances $y_{ij}$ are used as attributes that enter the objective function. That is, computing the k-plus criterion can be done by first computing the values $y_{ij}$, appending these to the original data matrix, and then computing the $\mathit{SSE}$ on the augmented data matrix. Hence, k-plus anticlustering reduces to an augmentation of the data matrix, leaving the original k-means criterion unchanged. 

During the remainder of this paper, whenever the k-plus criterion is optimized, the original data matrix is simply extended and the original k-means criterion $SSE$ is optimized. For a multicriterion objective such as the k-plus criterion, this corresponds to a rather basic approach, and future research may investigate more sophisticated optimization schemes for k-plus anticlustering [e.g., see @brusco2012; @brusco2019]. In this paper, however, I am primarily concerned with investigating the properties of new objective itself rather than extending the state of the art on multicriterion optimization methods. 

Note that a problem may arise if the weights $w_1$ and $w_2$ are ignored when optimizing the k-plus objective, i.e., when just appending the squared-distance-features to the data set. Since the computation of the data matrix $\mathbf{Y}_{N \times P}$ involves squaring, the squared distances $y_{ij}$ have much larger values, on average, than the raw values $x_{ij}$. Therefore, the $\mathit{SSE_{Var}}$ criterion naturally receives greater weight when computing $\mathit{SSE_{kplus}}$ than the standard $\mathit{SSE}$ criterion. This imbalance is most likely unwarranted because optimizing similarity with regard to variances is not more important than optimizing similarity with regard to means. A solution is to standardize all values $x_{ij}$ and $y_{ij}$ to the same scale before computing $SSE_{kplus}$, e.g. via $z$-standardization.

## Skewness, kurtosis, and higher order moments

The outlined logic to equalize the variance, which is the second moment of a distribution, can be generalized to higher order moments in a straight forward manner. The *j*'th sample moment of a variable $x = (x_1, \ldots, x_N)$ can be computed as 

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^j.
$$

where $j = 2$ gives the variance, $j = 3$ the skewness, and $j = 4$ the kurtosis [@joanes1998]. Thus, by changing $j$---the power of the deviation between mean and data points---we can use the k-plus logic to equalize any desired sample moment between groups. For each moment $j$ and for each variable $x$, this is accomplished by adding a new variable $y^{(j)} = (y^{(j)}_1, \ldots, y^{(j)}_N)$ with $y^{(j)}_i = (x_i - \overline{\mathbf{x}})^j$ to the data set and subsequently applying standard k-means anticlustering. I expect this to be particularly interesting for the skewness and kurtosis when aiming to minimize differences with regard to distribution asymmetry (skewness) and the propensity to include outliers (kurtosis).

## Covariances

Because the covariance is also defined as an expected value, the k-plus criterion can be extended to reflect differences in covariances between data sets. The covariance between two variables $x$ and $y$ is defined as

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}}).
$$

Thus, to maximize between-groups similarity with regard to covariance structure, it is possible to add an additional variable for each pair of variables $x$ and $y$, computed as $(x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}})$. This requires computing $\binom{N}{2}$ new variables and thus turns out be be rather costly: The number of additional variables needed to minimize differences with regard to covariance structure grows quadratically with increasing number of variables; the number of additional variables needed to minimize differences with regard to distribution moments only grows linearly with increasing number of variables.

# Simulation study

To thorougly evaluate the k-plus approach to anticlustering, three specific k-plus criteria were implemented in a simulation study: (i) "standard" k-plus anticlustering, minimizing differences with regard to the mean and variance; (ii) k-plus *skew/kurtosis*, minimizing differences with regard to the mean, variance, skew and kurtosis; (iii) k-plus *correlation*, minimizing differences with regard to the mean, variance and covariance structure. 

The three k-plus methods were compared to (i) k-means anticlustering, minimizing differences with regard to group means; (ii) diversity anticlustering, maximizing the sum of Euclidean distances within groups; (iii) a simple random allocation of elements to groups. All anticlustering methods used a $z$-standardization of the data sets as a preprocessing step. In the case of k-plus anticlustering, the augmented input data---i.e., the original data as well as the features that were added to incorporate the additional optimization criteria---were subjected to a standardization, which is recommended.

The simulation was implemented using the statistical programming language R [Version 4.2; @R-base]. The R package `anticlust` [Version 0.6.1; @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R package `faux` [Version 1.1.0; @R-faux] was used to generate the multivariate normal data sets that were processed during the simulation. The R packages `dplyr` [Version 1.0.7; @R-dplyr], `ggplot2` [Version 3.3.5; @R-ggplot2], `papaja` [Version 0.1.0.9997; @R-papaja], `tidyr` [Version 1.1.3; @R-tidyr] and `DescTools` [Version 0.99.45; @R-desctools] were used for data analysis and result presentation. The simulation study is fully reproducible via code and data that is accessible from the accompanying OSF repository (**link TODO**).

## Optimization algorithm

The same local maximum search [Method LCW in @weitz1998] was applied to optimize each of the five competing anticlustering objectives. Building on an initial random assignment of elements to groups, the algorithm proceeds by swapping elements between groups in such a way that each swap improves the objective criterion by the largest possible margin. That is, for each element, all possible exchanges are simulated and the resulting objective values are stored. After each exchange has been tested, the one exchange is realized that improves the objective the most. No exchange is realized if no improvement is possible. The exchange procedure is repeated for each element. Once the last element has been processed, the procedure restarts at the beginning and once again iterates through all elements, repeating the exchange procedure. The procedure stops as soon as an iteration through the entire data set no longer yielded an improvement, i.e., as soon as a local maximum has been found. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. The simulation employed five repetitions of the local maximum search.

## Conditions

First, I generated 10,000 data sets following a normal distribution. The data sets were subsequently processed by the competing methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features varied between 2 and 5; (c) the standard deviation of all features was set to 1, 2 or 3 (the mean was always zero); (d) the correlation between all features was $r = 0$, $r = .1$, $r = .2$, $r = .3$, $r = .4$ or $r = .5$. All anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$ (i.e., each method was applied 30,000 times). Groups sizes were always equal. 

## Evaluation criteria

After the five anticlustering methods and the random allocation procedure had been applied to the 10,000 data sets for $K = 2$, $K = 3$, $K = 4$, I investigated how well the competing methods were able to create similar groups. To this end, I investigated the discrepancy in means, standard deviations, skewness, kurtosis, and correlation between groups---for each data set for each $K$. To quantify the discrepancy in group means, the following computation was used: for each of the (2, 3, 4 or 5) features, all (2, 3 or 4) group means were computed. For each feature, the difference between the minimum and maximum group mean was used as a measure of discrepancy; the global discrepancy in means ($\Delta M$) was then determined as the average discrepancy across features. The same procedure was applied to compute the discrepancy in standard deviations ($\Delta \mathit{SD}$), skewness ($\Delta \mathit{Skew}$) and kurtosis ($\Delta \mathit{Kurtosis}$). Quantifying discrepancy in correlations ($\Delta \mathit{Cor}$) followed the same rule, but discrepancies in correlations between groups had to be aggregated across pairs of features instead of single features.

## Results

```{r Table1}

# Load required packages
library(dplyr)
library(ggplot2)
library(tidyr)
source("./Simulation_Study/0-functions-generate-data.R")

## Analyze data for K = 2 and K = 3 and K = 4
simulation_results <- list()
for (K in 2:4) {
  filename <- paste0("./Simulation_Study/results-K", K, "-objectives-raw.csv")
  df <- read.csv(filename, sep = ";", stringsAsFactors = FALSE)
  df$K <- K
  simulation_results[[paste0("K-", K)]] <- df
}

df <- do.call(rbind, simulation_results)
rownames(df) <- NULL

# Make long format
ldf <- pivot_longer(
  df,
  cols = paste0(c("means", "sd", "skew", "kur", "cor"), "_obj"),
  names_to = "Objective",
  names_pattern = "(.*)_obj"
)

## Global results, aggregated across all simulation variables
tab <- ldf %>% 
  group_by(method, Objective) %>% 
  summarise(Mean = round(mean(value), 3)) %>% 
  pivot_wider(names_from = Objective, values_from = Mean) %>% 
  select(c(means, sd, skew, kur, cor))

colnames(tab) <- c("",
  "$\\Delta M$", 
  "$\\Delta \\mathit{SD}$",
  "$\\Delta \\mathit{Skew}$",
  "$\\Delta \\mathit{Kurtosis}$",
  "$\\Delta \\mathit{Cor}$"
)
  
apa_table(
  tab,
  caption = "Results of the simulation study",
  note = "The global results of the simulation study aggregated across all conditions. Cells contain information about the average between-group discrepancy with regard to means, standard deviations, skewness, kurtosis, and correlations. Lower values indicate less discrepancy, i.e., higher between-group similarity. Each cell value is the result of averaging across 30,000 data points (10,000 data sets $\\times$ $K = 2$, $K = 3$ or $K = 4$).",
  escape = FALSE,
  align = c("r")
)

```

```{r Figure1, fig.width = 8, fig.height = 10, fig.cap = "Simulation results by $N$."}

# Plot the results, by N
ldf %>% 
  group_by(method, Objective, N) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(method != "random") %>% 
  mutate(
    Objective = ordered(
      Objective, 
      levels = c("means", "sd", "skew", "kur", "cor"),
      labels = c("M", "SD", "Skew", "Kurtosis", "Correlation"))
  ) %>%
  ggplot(aes(x = N, y = Mean, colour = method)) + 
  geom_line(aes(linetype = method), size = .85) + 
  facet_grid(rows = vars(Objective), scales = "free") + 
  ylab("Mean discrepancy") +
  theme_bw(base_size = 16)

```

Table 1 displays the global simulation results aggregated across all conditions. Standard k-means performed best at minimizing discrepancy with regard to means, but disregards all other distribution characteristics. K-plus anticlustering also addresses $\Delta \mathit{M}$ quite well and at the same time tends to minimize discrepancy with regard to the standard deviations ($\Delta \mathit{SD}$). With regard to minimizing $\Delta \mathit{SD}$, all three k-plus approaches outperform all other methods. As Figure 1 shows, with increasing $N$, k-means and all k-plus methods converge on minimizing $\Delta \mathit{M}$. This is an important observation because it shows that including the k-plus term in the optimization process does not necessarily prevent minimizing discrepancy in means; instead, multiple objectives can be addressed at the same time.

While k-plus anticlustering is capable of addressing multiple objectives simultaneously, it is apparent that increasing the number of optimization criteria aggravates fulfilling each single criterion. K-plus skew/kurtosis and k-plus correlation are worse at minimizing discrepancy with regard to standard deviations than standard k-plus, which is slightly worse at minimizing $\Delta \mathit{M}$ than k-means anticlustering. Still, every k-plus method does what it is supposed to do: k-plus skew/kurtosis is best at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$, k-plus correlation is best at minimizing discrepancy with regard to $\Delta \mathit{Cor}$. At the same time, these two k-plus objectives maintain a comparably good level of addressing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$, outperforming diversity anticlustering. 

It is maybe surprising that k-plus skew/kurtosis does not seem to be much better than diversity anticlustering with regard to skew and---in particular---kurtosis, even though the method has been specifically tailored to these criteria. Figure 1 gives a more fine grained assessment of this observation by splitting the results by $N$: Surprisingly, for small $N$, k-plus skew/kurtosis is even worse than diversity anticlustering at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. However, with increasing $N$, it clearly outperforms diversity anticlustering. These results illustrate the cost of optimizing several criteria at the same time: for low $N$, k-plus seems skew/kurtosis to be preoccupied with fulfilling the objectives $\Delta \mathit{M}$ and $\Delta \mathit{SD}$. Increasing $N$ then facilitates to also address $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. 

Generally, it should be noted that diversity anticlustering is a good all-arounder method that tends to somewhat address all distribution characteristics. For the specific objectives that are addressed by the k-plus methods, however, diversity anticlustering is outperformed. As as side note, apart from k-plus correlation, diversity anticlustering is the only method that equalizes correlation structure among groups. As far as I know, this observation has not been made previously, even though the maximum diversity problem has been widely studied. 

# K-plus anticlustering using the R package anticlust 

This section demonstrates how researchers and other data analysists can easily employ k-plus anticlustering using the R package `anticlust` [@papenberg2020], using an interface function called `kplus_anticlustering()`, which is available from version 0.6.3 onward. The package can be installed in the R environment using the `install.packages()` command:

```R
install.packages("anticlust")
```

## Example Application I

In the first example, I use norming data for the OASIS image data set that is freely available online [@kurdi2017introducing]. Kurdi et al.  assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense collected an additional rating dimension by measuring how the same 900 images were rated with regard to their beauty. In my application, I use all three features; the corresponding data is available from Github.[^githubbeauty]

[^githubbeauty]: https://github.com/aenneb/OASIS-beauty

I used k-plus anticlustering to divide the data set of 900 images into 9 groups of 100 images each. The task was accomplished using the following R code:

```R
library(anticlust) # load the package anticlust

kplus_anticlustering(
  features,
  K = 9,
  variance = TRUE,
  skew = TRUE,
  kurtosis = FALSE,
  covariances = FALSE,
  moments = NULL,
  method = "local-maximum",
  standardize = TRUE
)
```

The code runs in about 10 seconds on a contemporary personal computer; the run time may vary depending on how many iterations are needed to find the local maximum. The variable `features` is a data table containing the image data: 900 rows representing images and 3 columns representing the features beauty, arousal, and valence. The accompanying OSF repository contains code to reproduce the example (**TODO**). 

The `kplus_anticlustering()` function has four boolean arguments to specify whether variance, skewness, kurtosis and covariances should be included as part of the k-plus criterion. Only the argument `variance` is set to `TRUE` by default---corresponding to the understanding that k-plus is an extension to k-means that at least equalizes variances in addition to means. The arguments corresponding to skewness, kurtosis and convariance have to be spefically "turned on", i.e., by setting them to `TRUE` in the function call of `kplus_anticlustering()`. If other higher order moments should be included as part of the optimization, the optional argument `moments` can be used to specify the desired moments as an integer vector. The argument `method = "local-maximum"` ensures that the k-plus criterion is optimized using the local maximum search method described in the previous section. The argument `K` describes the number of groups. By default, the function ensures that all features are standardized before the optimization starts; in particular, this option enforces that all k-plus criteria receive the same weight during the optimization process. This behaviour can be adjusted using the boolean argument `standardize`. The documentation of the `anticlust` package also provides detailed information on how to use the `kplus_anticlustering()` function.

Table 2 shows the results of the anticlustering application by listing the descriptive statistics (means, standard deviations and skewness) for each of the 9 groups and for each of the 3 features. Table 2 also includes results of k-means anticlustering and diversity anticlustering. It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations, and the skewness was also very evenly matched. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations and especially skewness. Diversity anticlustering was well-suited to match means, standard deviations and skew, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---diversity anticlustering does not directly maximize similarity with regard to these criteria, but instead maximizes within-group heterogeneity.

```{r}

example1_file <- "results_oasis_same_size.csv"
if (!file.exists(example1_file)) {

    oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
    )

    features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

    K <- 9

    # k-plus variance / skew
    groups_kplus <- kplus_anticlustering(
        features,
        K = K,
        variance = TRUE,
        skew = TRUE,
        method = "local-maximum",
        standardize = TRUE
    )

    # k-means
    groups_kmeans <- anticlustering(
        features,
        K = K,
        objective = "variance",
        method = "local-maximum",
        standardize = TRUE
    )

    # diversity
    groups_diversity <- anticlustering(
        features,
        K = K,
        objective = "diversity",
        method = "local-maximum",
        standardize = TRUE
    )


    # Functions to compute means / SD / skew per group 
    descriptives_by_group <- function(features, anticlusters, FUN, name) {
        df <- data.frame(
            t(data.frame(lapply(by(features, anticlusters, FUN), c)))
        )
        df$group <- 1:nrow(df)
        df$Descriptive = name
        df
    }


    all_descriptives_by_group <- function(features, anticlusters) {
    means <- descriptives_by_group(features, anticlusters, colMeans, "mean")
    sds <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, sd),
        "sds"
    )
    skew <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, DescTools::Skew),
        "skew"
    )
    rbind(means, sds, skew)
    }


    kplus <- all_descriptives_by_group(features, groups_kplus)
    kplus <- data.frame(kplus, Objective = "kplus")

    kmeans <- all_descriptives_by_group(features, groups_kmeans)
    kmeans <- data.frame(kmeans, Objective = "kmeans")
    diversity <- all_descriptives_by_group(features, groups_diversity)
    diversity <- data.frame(diversity, Objective = "diversity")

    all_objs <- rbind(kplus, kmeans, diversity) %>% 
    pivot_wider(names_from = Descriptive, values_from = c(beauty_mean, Valence_mean, Arousal_mean))
    
    write.table(
        all_objs, example1_file, row.names = FALSE, sep = ";"
    )

    
} else {
  all_objs <- read.csv(example1_file, sep = ";")
}

table_all_objs <- subset(all_objs, select = -Objective)
table_all_objs <- as.matrix(table_all_objs)
colnames(table_all_objs) <- c("Group", rep(c("$M$", "$\\mathit{SD}$", "Skew"), 3))
table_all_objs <- apply(table_all_objs, 2, force_or_cut)
#mode(table_all_objs) <- "character"

apa_table(
  table_all_objs,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The k-plus criterion that was optimized included a term to minimize differences with regard to means, variances, and skewness.",
  escape = FALSE,
  align = c("c", rep("r", 9)),
  col_spanners = list(Beauty = c(2, 4), Valence = c(5, 7), Arousal = c(8, 10)), 
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Diversity anticlustering}" = 19:27)
)
```

Table 3 illustrates another advantage of k-plus anticlustering over diversity anticlustering: When creating groups of unequal size, diversity anticlustering tends to increase the spread of the data in the largest group in comparison to the other groups. This is due the diversity criterion being a measure of within-group heterogeneity; maximizing within-group heterogeneity only incidentally maximizes between-group similarity. However, because the diversity criterion is a sum rather than an average, the correspondence between within-group heterogeneity and between-group similarity no longer holds unequal-sized groups. K-plus anticlustering strives for between-group similarity regardless of group size.

Unequal group sizes can be requested in anticlust by passing the different group sizes to the argument `K`. The following code was used to equalize means and variances between four groups using the k-plus approach:

```R
kplus_anticlustering(
  features,
  K = c(100, 100, 100, 600),
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    if (criterion == "kplus") {
        oasis[[paste(criterion, "group", sep = "_")]] <-  kplus_anticlustering(
            features,
            K = c(100, 100, 100, 600),
            method = "local-maximum"
        )
    } else {
        oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
        features,
        standardize = TRUE,
        K = K,
        objective = criterion,
        method = "local-maximum"
        )
    }
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Diversity anticlustering}" = 9:12)
)
```

This function call relied on the default k-plus settings, ensuring that variances are equalized -- but no higher order moments or covariances.

## Example Application II: Small data set ($N = 96$)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore, a second application illustrates that k-plus anticlustering yields satisfactory results in practice even when group sizes are smaller. I make use of a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Dr. Schaper [@schaper2019metacognitive; @schaper2019metamory; also see @papenberg2020 for a description of the norming data]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

Table 4 illustrates the results that are obtained when using standard k-plus anticlustering to divide the stimulus set into 6 groups of size `r nrow(schaper2019) / 6` each. Even though the means and standard deviations are no longer perfectly matched between groups, arguably they are still similar enough for any practical purpose---especially when considering inevitable measurement error in norming studies.

```{r, echo = FALSE}
features <- schaper2019[, 3:6]
anticlusters <- anticlustering(
  features,
  K = 6,
  objective = "kplus",
  method = "local-maximum",
  repetitions = 5,
  standardize = TRUE
)
```

```{r}

tab <- mean_sd_tab(features, anticlusters)
tab <- cbind(1:6, 96 / 6, tab)
colnames(tab) <- c("Group", "N", "Typicality", "Atypicality", "Syllables", "Frequency")

apa_table(
  tab,
  caption = "Descriptive statistics by group for the Schaper et al. (2019a; 2019b) data set, after applying default k-plus anticlustering.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r")
)

```

# Discussion

In this paper, I presented the k-plus criterion for anticlustering. K-plus anticlustering focuses on dividing a data set into subgroups in such a way that these groups are similar to each other. The k-plus criterion extends the classical k-means criterion, which only represents how similar groups are with regard to their means. The k-plus criterion however may be used to also equalize the variance between groups, and any higher order moments such as skewness and kurtosis, as even covariances. Interestingly, k-plus basically does not change the original k-means criterion because it can be reduced to an augmentation of the input data. 

A simulation study and examples on real norming data showed that the k-plus criterion is well suited to maximize between-group similarity with regard to multiple criteria. In particular, this work indicates that k-plus anticlustering should be a replacement of the classical k-means approach when striving for between-group similarity. K-means anticlustering disregards any distribution characteristics other than the mean; k-plus anticlustering ensures between-group similarity with regard to multiple criteria while still taking into account differences in means. For large data sets, there is virtually no reason to use k-means anticlustering because the practical differences between k-means and k-plus regarding similarity in means vanish completely. A reasonable exception is given when the input data consists of binary variables because in this case the mean and the standard deviation are directly related. 

The results showed that fulfilling several criteria at the same time may be difficult to achieve for small data sets. Still, users can try out different objectives and see which specification fulfils their needs. A reasonable approach it to start greedy, i.e., requesting multiple distribution moments to be equalized, and only if the results are subpar, go for fewer criteria. Unlike in statistical analysis, there is no problem with "just trying out" different approaches when it comes to anticlustering.

K-plus anticlustering was also shown to outperform diversity anticlustering with regard to the specific objectives that are included in the optimization process; diversity is still a reasonable all-arounder that strives for overall betwee-group similarity. Which method should be employed depends on a user's preferences. In doubt, several anticlustering methods can be applied sequentially and the results can be compared directly. 

## Limitations and outlook

This paper focused on the presentation of new objective function but the did not properly consider the multicriterion nature of the k-plus objective. Instead, a simple (unweighted) sum approach was applied to optimize the k-plus objective, independent of the number of criteria that were incorporated. Other approaches, such as direct algorithms, may yield improved results and better possibilities to select partitionings according to differently weighted criteria. @brusco2019 showed how direct algorithms can be used to optimize multiple critera in anticlustering applications. Their study focused on an approach to find the pareto set of solutions that simultaneously optimized diversity and dispersion, thereby maximizing within-group heterogeneity with regard to both criteria.[^inanticlust]

[^inanticlust]: The `anticlust` package now also includes an implementation of the bicriterion anticlustering algorithm by @brusco2019.

Focusing on the objetive function only and not on specialized algorithmic optimization approaches followed two reasonings. First, it was of interest to investigate the value of the k-plus objective function per se, independently from the algorithm that is used to optimize it. Hence, to obtain a fair assessment, it was important in the simulation study that each anticlustering objective is optimized using the same procedure, instead of using specialized multicriterion optimization schemes. It was of particular interest to compare the k-plus objective with the popular diversity objective. @papenberg2020 compared k-means anticlustering and diversity anticlustering[^clusterediting] in a simulation study and concluded that the diversity criterion should usually be preferred because k-means does not equalize the spread of the distribution between groups, whereas the diversity maintains an appropriate balance between location and spread. Using the k-plus extension, however, the k-means criterion regains a lot of attractivity: when optimizing for similarity in means and variance, k-plus anticlustering clearly outperformed the diversity criterion in the simulation study. Second, the applications show that the simple sum approach using a general local maximum search yields very satisfying results in practice. It seems that the space of solutions in anticlustering settings includes partitionings that fulfil multiple objectives and these partitionings can be found using a local maximum search.

[^clusterediting]: In their study, they used the term anticluster editing to refer to the maximization of the diversity criterion. The diversity criterion is the objective function used in the clustering method cluster editing, where it has to be minimized [@zahn1964; @shamir2004vf].

## Closing thoughts on the nature of anticlustering

- new criterion: optimizes between-group similarity, no correspondence to within-group heterogeneity [see @brusco2019 for a exceptional method maximizing within-group heterogeneity]

- Similarly: the reversed objective no longer has a useful clustering interpretation: Maximizing differences in variances does not lead to homogeneous and/or well separated clusters

## Conclusion

This paper introduced k-plus anticlustering for maximizing between-group similarity, improving the traditional k-means approach. K-plus anticlustering can be adopted whenever a data set has to be partitioned into equivalent parts. The method is accessible and easily usable via the R package `anticlust`.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
