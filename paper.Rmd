---
title             : "A bicriterion extension of k-means anticlustering to optimize between-group similarity"
shorttitle        : "K-means anticlustering extended"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf.  


abstract: |
  Anticlustering is an automated method to divide a data set into groups
  in such a way that high between-group similarity and high within-group 
  heterogeneity is obtained. Anticlustering is accomplished by maximizing 
  instead of minimizing a clustering objective. 
  
bibliography      : ["lit.bib"]
  
keywords          : "K-Means, Anticlustering, Variance"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE) 
```

Anticlustering is a method to partition a set of elements into different 
groups with the focus of obtaining high between-group similarity and 
within-group heterogeneity. Anticlustering reverses the logic of its 
better known twin, cluster analysis that seeks homogeneity within 
clusters and heterogeneity between clusters [@rokach2005; 
@steinley2006]. The method of anticlustering is useful in many 
tasks surrounding the work of researchers in psychology, for example, 
when splitting a test into parts of equal difficulty [@gierl2017], when 
assigning students to work groups [@baker2002], or when assigning 
stimuli to experimental conditions [@lahl2006]. Solving such problems 
"by hand" is a tedious and time-consuming task and the quality of 
manually established groups is usually improvable. Fortunately, these 
problems can be formalized mathematically via several anticlustering 
objective functions [e.g., @brusco2019; @spath1986; @baker2002; 
@fernandez2013], and accessible open source software solutions exist 
[@papenberg2020].

Several anticlustering objectives are available to assess between-group 
similarity or within-group heterogeneity. In many cases, both objectives 
are directly related, that is, maximizing heterogeneity within groups 
simultaneously maximizes between-group similarity. Several 
anticlustering objectives are based pairwise dissimilarity ratings; the 
most prominent criterion of this kind is the *diversity*, which is the 
total sum of dissimilarities between any elements within the same group. 
Note that maximizing the diversity is equivalent to minimizing 
the sum of dissimilarities between elements in different clusters, 
directly illustrating the correspondence between within-group 
heterogeneity and between-group similarity. Another important criterion 
based on pairwise dissimilarities is the dispersion, which is the 
minimum dissimilarity between any two items in the same group 
[@fernandez2013; @brusco2019]. Maximizing the dispersion increases the 
within-group heterogeneity by ensuring that any two elements as 
dissimilar as possible from each other. Note that maximizing the 
dispersion no longer simultaneously maximize between-group similarity. 
In this case, these two potential aims of anticlustering diverge.

Oftentimes, the available data does not represent pairwise similarity, 
but rather numeric, categorical or binary attributes [@dry2009]. That 
is, the data is a two-way two-mode [@arabie1992] table where rows 
represent the entities of study, and columns represent variables 
describing the elements. For example, numeric ratings may describe the 
attributes of word stimuli for psycholinguistic experiments. Such data 
may include Norms for imagery, concreteness and orthographic variables 
[@friendly1982]. In other cases, attributes are binary and may represent 
the presence or absence of a feature [@tversky1977]. Such data can be 
converted to pairwise dissimilarities for example by computing an 
appropriate dissimilarity measure, such as the pairwise Euclidean or 
squared Euclidean distance across a set of numeric attributes 
[@brusco2019]. Then, anticlustering criteria based on pairwise 
dissimilarity can be applied to partition the elements. 

A different approach to anticlustering directly works with feature based 
data and does not require a transformation into pairwise 
dissimilarities. Instead, the sum of the squared Euclidean distances 
between elements and the cluster they are assigned to is minimized 
[brusco2006branch], most prominently using the k-means heuristic 
[@jain2010; @steinley2006]. K-means clustering maximizes between-cluster 
separation and within-cluster homogeneity simultaneously 
[@aloise2009np]. Späth [-@spath1986] and Valev [-@valev1998; 
-@valev1983] independently recognized that by optimizing the reversed 
k-means objective that is, maximizing the sum of distances between 
elements and cluster centroids -- a data set is partitioned into similar 
groups, thereby reversing the logic of classical clustering. They 
independently from each other coined the term anticlustering for this 
purpose. K-means anticlustering maximizes the sum of squared Euclidean 
distances from each element to the center of the cluster to which it 
belongs. As @spath1986 noted, k-means anticlustering thereby directly 
minimizes differences with regard to the means of the numeric variables 
between groups. Hence, groups only become similar with regard to the 
mean of the distribution of the numeric attributes, which may perform 
poorly as a measure of overall similarity; the spread of the data is not 
targeted and may differ between sets. @papenberg2020 showed that 
maximizing the diversity (based on the pairwise Euclidean distances) is 
better suited to minimize difference with regard to both the mean and 
the variance of the data; it was shown that even an entirely random 
split usually resulted in more similar standard deviations as k-means 
solely focuses on the mean. When requiring that groups are overall 
similar to each other, this focus may be detrimental as similar means 
can be obtained even when the underlying distributions are very 
different [@anscombe1973]. 

In this paper, I present a simple extension of the k-means objective 
function to maximize simililarity with regard to means and variances at 
the same time. The aim is to establish groups that are overall similar 
to each other. It is shown that this is achieved by formulating a 
bicriterion optimization objective for k-means anticlustering. 
Interestingly, under specific---and important---circumstances, this 
reformulation reduces to an augmentation of the original data matrix 
while the optimization objective remains unchanged. This insight has 
important consequences as it allows to adapt standard optimization 
schemes for single criterion anticlustering. I show that the extended 
k-means criterion outperforms other anticlustering criteria when the 
goal is the minimize difference with regard to the mean and variance of 
the different groups. In most cases, both objectives can be optimized
satisfactorily at the same time without any loss on either; when the 
data set is large enough (usually, when $N > XX$), the bicriterion 
optimization leads to > 99.99% objective performance as compared to the 
single optimization of either k-means or k-variance.

## A Motivating Example

```{r}

set.seed(4318) # 4318

N <- 14
K <- 2
partitions <- generate_partitions(N, K)
features <- round(rnorm(N, 15, 3), 1)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
diff_mean <- function(clusters, features) {
  abs(diff(tapply(features, clusters, mean)))
}

diff_vars <- function(clusters, features) {
  abs(diff(tapply(features, clusters, var)))
}

all_objectives_mean <- sapply(
  partitions,
  FUN = diff_mean,
  features = features
)

all_objectives_var <- sapply(
  partitions,
  FUN = diff_vars,
  features = features
)

df <- data.frame(
  diff_mean = all_objectives_mean,
  diff_var  = all_objectives_var
)

plot(df, pch = 4, col = "darkgrey", las = 1)
# Illustrate best partition wrt simililarity in means
points(
  all_objectives_mean[which.min(all_objectives_mean)],
  all_objectives_var[which.min(all_objectives_mean)],
  cex = 2, col = "#DF536B", pch = 16
)

df$sum <- rowSums(df)

points(
  all_objectives_mean[which.min(df$sum)],
  all_objectives_var[which.min(df$sum)],
  cex = 2, col = "#2297E6", pch = 16
)

points(
  all_objectives_mean[which.min(df$diff_var)],
  all_objectives_var[which.min(df$diff_var)],
  cex = 2, col = "#61D04F", pch = 16
)

```

An motivating example illustrates that aiming to optimize similarity of 
means of standard deviations in a data set is not only desirable, but 
also expected to be feasible. To this end, I created 14 data points from 
a normal distribution ($M = 15$, $SD = 3$, see Table 1). The data points 
in Figure 1 illustrate all `r length(partitions)`  ways to partition a 
data set of $N = 14$ into two parts. Red point: maximum similarity in 
means; green point = maximum similarity in variance. Blue point: Almost 
no difference to red point wrt similarity in means, but variance is much 
more similar. This blue partition is certainly preferable to the red one 
when maximal overall similarity is required; the red one just optimizes 
similarity in means (which is what maximizing the k-means objective 
does). 


```{r datatables}

best_partition_mean <- partitions[[which.min(df$diff_mean)]]
best_partition_var  <- partitions[[which.min(df$diff_var)]]
best_partition_combined <- partitions[[which.min(df$sum)]]

data <- data.frame(
  x = features,
  best_mean = best_partition_mean,
  best_var = best_partition_var,
  best_combined = best_partition_combined
  
)

descriptives <- cbind(
  mean_sd_tab(as.matrix(features), best_partition_mean),
  mean_sd_tab(as.matrix(features), best_partition_var),
  mean_sd_tab(as.matrix(features), best_partition_combined)
)

descriptives <- paste("M =", gsub("[(]", "(SD = ", descriptives))
dim(descriptives) <- c(2, 3)
colnames(descriptives) <- colnames(data[,-1])
apa_table(data, caption = "Illustrative data set")
apa_table(descriptives)

```


Unfortunately, improving algorithmic procedures will not help to find 
better partitions -- the objective function has to change. I will show 
an objective function that captures both similarity in means and 
variance.

## K-Means objective

In this treatment of k-means, I adopt the notation Steinley (2006) 
provided in his synthesis of the k-means literature. In clustering---or 
anticlustering---, an item pool $X$ = $\{x_1, \ldots, x_N\}$ has to be 
partitioned into $K$ groups $\{C_1, \ldots, C_K\}$. We assume that each 
element $x_i$ ($i = 1, \ldots, N$), is a vector of length $P$, and each 
entry describes one of its numeric measurements. Generally, $X$ 
can is interpreted as an $N \times M$ matrix where each row represents 
an data point $x_i$ (e.g., representing a stimulus or a person) and each 
column is a numeric attribute (e.g., the number of syllables in a word, 
or the attractiveness rating of a face). 

K-Means anticlustering aims to maximize the error sum of squares (SSE), 
the within-group variance, which is the sum of the squared Euclidean 
distances between each data point and its cluster centroid. The cluster 
centroid is given by averaging the values on each variable over the 
elements within that cluster. The centroid of the *j*th variable in the 
*k*th cluster, consisting of $n_k$ elements, is given as: 

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits {i \in C_k} x_{ij}
$$

The k-means objective function, the error sum of squares, is then given 
as:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

In anticlustering applications, we usually assume that all cluster are 
of equal size. Hence 

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

## K-variances

**This is old, adjust to Steinley (2006):**

The squared distance between an element $x = (x_1, \ldots, x_M)$ and the 
average of all elements $\overline{\mathrm{\boldsymbol{x}}}$ is:

$$
\vert \vert x - \overline{\mathrm{\boldsymbol{x}}} \vert \vert^2 = 
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2
$$

Now define new data points representing the squared distances between 
data points and the grand mean:

$$ 
y_i = (x_i - \mathrm{\boldsymbol{x}})^2, \forall i \in N
$$

where $\mathrm{\boldsymbol{x}}$ is a vector of length $M$ containing 
the average for each feature.

Now:

$$
\sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
  \sum\limits_{m = 1}^{M}(y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2 =
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
    (y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2
$$

If we define $x_{m + 1} = y_1$, $x_{m + 2} = y_2$, ..., $x_{2M} = y_M$.
this is the same as:

$$
\sum\limits_{m = 1}^{2M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 
$$

Thus, we redefine the data points and include an additional $M$ features 
for each element. Then, we maximize $\mathit{Var}_{\mathrm{within}}$, 
which corresponds to a tradeoff between minimizing differences in mean 
features between clusters, and the average squared distance to the grand 
mean, i.e., the within-cluster variance. That is: We try to maximize the 
within-cluster variance, but to a certain degree we also strive to 
obtain similar within-cluster variance in each cluster.

Important to note: We solve the same optimization problem, we just 
change the input a bit (we double the number of "columns"). 

**Example table!!**

## Bicriterion optimization model

- formalize the weighted-sum objective and use the weighted-sum method.
works well on this specific problem [although in general there are 
caveats to the weighted-sum method, see @brusco2019]. is fast, good 
results. in fact, the objectives seem to work well together (optimizing 
k-means extended often yields more similar variances than just 
k-variances!!!)

- In this paper: Formalization of the objective function, recognition
that this formulation does not change the basic structure of the k-means
objective, because it reduces to augmentation of the data input

- not interested in the Pareto efficient set! Because: Overall 
similarity should be maximized, which requires that both criteria are 
satisfied to a strong degree (not interested in just maximizing 
similarity wrt variance; this may be interesting in other applications). 
Instead: A tradeoff is possible, and in many cases, simultaneous 
optimization will meet both criteria almost as well as optimizing just
either criterion. To justify this approach: show the pareto front, which 
is not a diagonal / circle [see @brusco2012] but a RIGHT TRIANGLE! (this 
is probably because this is *anti*clustering!)

- Evaluation will consist of a comparison of different objectives based 
on the same (simple) exchange algorithm

- Future research: Application of more sophisticated bicriterion 
optimization algorithms are interesting

- How to select weights? (Oftentimes: for equal weights, the results 
are good)

- Standardization of the extended data matrix to ensure that the two
criterion functions do no differ markedly in scale [@brusco2019]

- Using variable weighting (see Steinley, 2006, p. 13), we can 
adjust the relative importance of minimizing differences in means
and variances. Standard in `anticlust`: Equal weight, this is 
achieved by first creating the new variables, representing 
the squared differences from the mean, and then standardizing
the entire feature matrix. If $N$ is large enough (relative to $K$),
there is usually no need for weighting.

# Evaluation 

- Simulation 1: Show that k-means extended obtains 99.999% of the 
k-means and k-variance objective, on average. 
  + New objective seems to work best when N is high relative to K; optimizing 
two objectives at the same time is hard when N is low. Easier when only 
optimizing one objective (i.e., the mean, as done by normal k-means 
anticlustering). What I can say: When N / K > X, the objective is 
usually 99.99% of both objectives.
  + Two examples: $K = 2$, $N = 20$ vs. $N = 100$, using classical
k-means: only mean; using k-variance: only variance; combination:
both, but works better for larger $N$

- Simulation 2: show that k-variance anticlustering outperforms 
other objectives with regard to similarity in means / variances 
(in particular: anticluster editing)

- Often, maximizing k-means-variance yields more similar variances 
than k-variance alone. Why is that? I guess: structure in the solution 
space, local maxima are real. When also considering means, you more 
often escape bad local maxima? The finding is: k-means-extended better 
minimizes the maximum difference between the variance across groups 
than k-variance (for K > 2), which in the eye of the researcher, may 
be a more intuitive criterion.

# Examples

- use some examples (e.g., OASIS data set, 900 items allocated to 10 
sets, same mean and variance across the 10 sets)

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
