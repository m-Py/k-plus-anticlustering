---
title             : "k-plus Anticlustering: An Improved k-means Criterion for Maximizing Between-Group Similarity"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich-Heine-Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

abstract: |

    Anticlustering refers to the process of partitioning elements into disjoint groups with the goal of obtaining high between-group similarity and high within-group heterogeneity. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---and is usually approached by maximizing instead of minimizing a clustering objective function. Introducing k-plus, this paper presents an extension of the classical k-means objective to maximize between-group similarity in anticlustering applications. The k-plus criterion represents between-group similarity as discrepancy in distribution moments (means, variance, skewness, and higher order moments), whereas the k-means criterion only reflects the degree to which groups differ in their means. While constituting a new criterion for anticlustering, it is surprisingly shown that k-plus anticlustering can be implemented by optimizing the original k-means criterion after the input data has been augmented with additional variables. A computer simulation and practical examples show that k-plus anticlustering achieves high between-group similarity with regard to multiple objectives. In particular, optimizing between-group similarity with regard to variances usually does not compromise similarity with regard to means; the k-plus extension is therefore generally preferred over classical k-means anticlustering. Examples are given on how k-plus anticlustering can be applied to real norming data using the open source R package `anticlust`, which is freely available via CRAN (https://cran.r-project.org/package=anticlust).
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, skewness, kurtosis, stimulus selection"
wordcount         : "4992"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

output            : 
  papaja::apa6_pdf
appendix: "appendix.Rmd"

header-includes:
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{amsmath}
  - \floatstyle{plaintop}
  - \restylefloat{figure}
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - |
    \makeatletter
    \renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
      \hskip -\arraycolsep
      \let\@ifnextchar\new@ifnextchar
      \array{#1}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
---

```{r setup, include = FALSE}
library(papaja) # use the development version: `remotes::install_github("crsh/papaja@devel")`; currently 0.1.1.9001
library(anticlust)
library(knitr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(DescTools)
# for formatting numbers in Rmarkdown, use my package prmisc (https://github.com/m-Py/prmisc)
library(prmisc) # remotes::install_github("m-Py/prmisc") 
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```

Anticlustering refers to the process of partitioning elements into disjoint groups with the goal of obtaining high between-group similarity and high within-group heterogeneity [@brusco2019; @spath1986; @valev1998; @valev1983]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and separation between clusters [@rokach2005]. Anticlustering has many applications in research psychology [@steinley2006; @brusco2019; @papenberg2020]. Examples include splitting tests into parts of equal difficulty [@gierl2017], assigning students to work groups [@baker2002], and assigning stimuli to different, but parallel experimental conditions [@lahl2006]. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, anticlustering problems can be formalized as mathematical optimization problems [e.g., @baker2002; @brusco2019; @spath1986; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods are characterized by (a) an objective function that quantifies between-group similarity and/or within-group heterogeneity, and (b) an algorithm that determines how elements are assigned to groups to maximize the objective function. Several anticlustering objective functions use pairwise dissimilarity ratings such as the Euclidean distance as input. The most prominent criterion of this kind is the *diversity*, which is the total sum of pairwise dissimilarities between elements within the same group [@brusco2019; @gallego2013]. By considering pairwise dissimilarities between elements in the same group, the diversity criterion reflects the total degree of within-group heterogeneity. High within-group diversity simultaneously ensures high between-group similarity.[^equalsizedgroups] Another important anticlustering criterion based on the information in a dissimilarity matrix is the *dispersion*, which is the minimum dissimilarity between any two elements within the same group [@fernandez2013]. Maximizing the dispersion increases within-group heterogeneity by ensuring that any two elements in the same group are as dissimilar from each other as possible. @brusco2019 convincingly argued that anticlustering applications striving for within-group heterogeneity should incorporate both dispersion and diversity, and they presented a bicriterion algorithm to approximate the Pareto efficient set of solutions according to both criteria. 

[^equalsizedgroups]: As will be shown later, the diversity objective only strictly equalizes within-group heterogeneity and between-group similarity when groups are equal-sized.

Oftentimes, partitioning applications in psychology are focused on between-group similarity rather than within-group heterogeneity, even though both goals oftentimes coincide. High between-group similarity is for example desirable when designing experimental conditions using different stimulus sets that should be as similar as possible with regard to response-relevant attributes [@lahl2006]. Such stimulus selection tasks are usually conducted on the basis of attribute data and not on pairwise (dis)similarity ratings. That is, individual stimuli are numerically coded on relevant dimensions [@dry2009]. For stimuli in psycholinguistic experiments, attributes may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attributes are binary and may represent the presence or absence of a feature [@tversky1977]. When attribute values are available, anticlustering approaches that do not require a dissimilarity matrix[^dissimilarityfromfeatures] can be used to partition the data. K-means is probably the best-known clustering objective that can directly be computed on attribute values. In k-means clustering, the sum of the squared Euclidean distances between data points and their cluster centers is minimized, usually using the k-means heuristic [@jain2010; @steinley2006; @brusco2006branch]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. Reversing the k-means objective function---using maximization instead of minimization---has been recognized as a useful approach when aiming for high between-group similarity [@spath1986; @valev1998; @valev1983]. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. However, other distribution characteristics---such as the variance---are not targeted. @papenberg2020 showed that k-means anticlustering tends to over-optimize between-group similarity with regard to the mean at the cost of similarity in variance. However, when aiming for overall between-group similarity, neglecting all distribution characteristics other than the mean is misguided; similar means can be obtained even when the underlying distributions are clearly different [e.g., @anscombe1973]. 

[^dissimilarityfromfeatures]: It is still possible to convert the attribute data into a dissimilarity matrix, e.g. by computing all pairwise Euclidean distances, and then optimize a distance-based anticlustering criterion to obtain a partitioning. For likert scale data or binary attributes, the Manhattan distance may be preferred over the Euclidean distance; for mixed-type attributes, it is even possible to employ a combined distance metric [@rokach2005].

To optimize overall between-group similarity---and hence to overcome the limitations of k-means anticlustering---this paper introduces k-plus anticlustering. K-plus is an extension of the k-means objective that quantifies between-group similarity as discrepancy with regard to several distribution characteristics instead of only the mean. Specifically, k-plus offers the possibility to minimize differences with regard to the variance and higher order moments such as skewness and kurtosis. After formally introducing the k-plus objective, two simulation studies show that k-plus anticlustering is well-suited to create groups having minimal differences according to multiple distribution characteristics, significantly outperforming traditional criteria such as the diversity and the k-means objective. Practical examples illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust` [@papenberg2020; @R-anticlust], an extension to the popular statistical programming language R [@R-base].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means anticlustering is used to partition $N$ elements each having $P$ attributes into $K$ groups $(C_1, \ldots, C_K)$. The partitioning is subject to the requirement that each element is assigned to exactly one group. In anticlustering applications, an additional constraint is usually imposed on the group sizes $n_k$ ($k = 1, \ldots K$), with the most common restriction being that all groups have equal size, i.e. $n_k = \frac{N}{K}$ [@papenberg2020].

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares ($\mathit{SSE}$), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging to the $k$'th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note that there is a direct connection between the $\mathit{SSE}$ and the location of the cluster centroids [@spath1986]. If the $\mathit{SSE}$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_j$ ($j = 1, \ldots, P$) is the overall mean value on the $j$'th attribute:

$$
\overline{\mathbf{x}}_j = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ij}
$$

If the $\mathit{SSE}$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values between clusters.

## Minimizing discrepancy in variance between groups

In the following, I present a variation of the $\mathit{SSE}$ that can be used to minimize differences in variances between groups. To motivate the new criterion, we first consider a set of one-dimensional observations. The sample variance of a set of $N$ observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, i.e. $y_i = (x_i - \overline{\mathbf{x}})^2$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average---and k-means anticlustering minimizes differences with regard to averages---k-means anticlustering can be employed to equalize the variance across groups. This constitutes the central insight motivating the new k-plus criterion. 

Thus, to obtain an adaptation of the $\mathit{SSE}$ whose maximization can result in minimal differences in variances between groups, a new variable is computed for each original variable: The new variable represents the squared distance of each of the original values to the overall mean of the variable. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$ with $y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2$. On the basis of $\mathbf{Y}_{N \times P}$, we define the criterion $\mathit{SSE_{Var}}$:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points. Maximizing $\mathit{SSE_{Var}}$ can be used to obtain groups that have similar variances across the $P$ attributes. As a caveat, however, it is crucial to note that maximizing $\mathit{SSE_{Var}}$ only reliably yields similar group variances if the group centroids $\overline{\mathbf{x}}^{(k)}$ ($k = 1, \dots, K$) are close to the overall data centroid $\overline{\mathbf{x}}$. This is because the within-group variance is calculated on the basis of the group centroid $\overline{\mathbf{x}}^{(k)}$, but the new squared distance variables $y_{ij}$ are calculated using the overall centroid $\overline{\mathbf{x}}$ as reference point.  To ensure that $\mathit{SSE_{Var}}$ is a valid criterion for between-group similarity in variance, we must therefore simultaneously ensure that group centroids of the original variables are close to the overall data centroid (and therefore to each other). Ensuring that the group means are close to the overall centroid is accomplished when maximizing the classical $\mathit{SSE}$, i.e., via standard k-means anticlustering [@spath1986]. Combining the maximization of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$ at the same time leads to the k-plus approach for anticlustering, which is detailed in the next section. 

## A bicriterion model: K-plus anticlustering {#kplus}

An exclusive maximization of $\mathit{SSE_{Var}}$ (i.e., equalizing variances) is not feasible---and most likely not interesting on its own---without also considering the standard $\mathit{SSE}$ (i.e., equalizing means). The most simple approach for an optimization that considers multiple criteria at the same time is the weighted sum approach [@naidu2014; @marler2010]. That is, all criteria are computed independently from each other and are then combined into a single criterion through a weighted sum. By combining $\mathit{SSE_{Var}}$ and $\mathit{SSE}$ in a single criterion, we define the objective function for k-plus anticlustering:

$$
\mathit{SSE_{kplus}} = 
w_1 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + 
w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Maximizing $\mathit{SSE_{kplus}}$ will lead to similar means and variances between $K$ groups, while the weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of equalizing means and variances, respectively. 

```{r motivatingExample}

# some functions for motivating example

# Variance objective function with changed order of the arguments 
# (can be called by sapply() etc.)
var_objective <- function(clusters, features) {
  anticlust::variance_objective(features, clusters)
}

# Compute features for k-plus criterion
squared_from_mean <- function(data) {
  apply(data, 2, function(x) (x - mean(x))^2)
}

# First rendering is slow because the data needs to be generated
path <- "partition_objectives.csv"

if (!file.exists(path)) {
  set.seed(2207) # 2207
  # Generate data and all partitions
  N <- 14
  K <- 2
  partitions <- generate_partitions(N, K)
  features <- rnorm(N, 15, 3)

  # Compute objectives for each partition
  # For each partition: Compute SSE objective, and SSE(VAR)
  # 1. SSE
  all_objectives_kMeans <- sapply(
    partitions,
    FUN = var_objective,
    features = features
  )
  
  squared_distances_from_mean <- squared_from_mean(as.matrix(features))
  
  # 2. SSE(VAR)
  all_objectives_kVar <- sapply(
    partitions,
    FUN = var_objective,
    features = squared_distances_from_mean
  )
  
  # 3. SSE(KPLUS)
  all_objectives_kPlus <- sapply(
    partitions,
    FUN = var_objective,
    features = cbind(features, squared_distances_from_mean)
  )
  
  df <- data.frame(
    kmeans = all_objectives_kMeans,
    kvar = all_objectives_kVar,
    kplus = all_objectives_kPlus
  )

  write.table(
    df, path, row.names = FALSE, sep = ","
  )
  
  # investigate the best 0.5% of k-means anticlustering partitions, for discussion
  # best_0.1percent_anticlusterings <- df$kmeans /max(df$kmeans) > .999
  # best_partitions <- partitions[best_0.1percent_anticlusterings]
  # 
  # neighbor_matrix <- anticlust:::selection_matrix_from_clusters
  # neighbourhoods <- lapply(best_partitions, neighbor_matrix)
  # neighbourhoods <- lapply(neighbourhoods, function(x) as.integer(x[lower.tri(x)]))
  # pairs <- Reduce("+", neighbourhoods)
  # length(pairs)
  # length(neighbourhoods[[1]])
  # choose(14, 2)
  # min(pairs) # 40
  # max(pairs) # 95
  # length(best_partitions) # 146
  # -> all items are grouped with one another, and no items are always grouped with one another

} else {
  df <- read.csv(path)
}

```

Research on multiobjective optimization indicates that solutions oftentimes fail to satisfactorily address several criteria if the underlying objectives are in conflict (Ferligoj & Batagelj, 1992). As $\mathit{SSE_{kplus}}$ is a novel criterion, it is so far unknown whether maximizing it can yield a partitioning where both constituting criteria are similar between groups, i.e., whether these objectives are aligned or in conflict. A motivating example illustrates that aiming to simultaneously optimize similarity of means and variances may be feasible. I created 14 data points following a univariate normal distribution ($M = 15$, $SD = 3$) and then generated all `r n_partitions(14, 2)` possible ways to partition the 14 data points into $K = 2$ equal-sized groups. Figure 1 shows the scatterplot of the criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ across partitions. The partition that is represented by the squared symbol has the overall best highest $\mathit{SSE}$, but is outperformed by many other partitions with regard to $\mathit{SSE_{Var}}$. Similarly, the partition that maximizes $\mathit{SSE_{Var}}$ (represented by the triangular symbol) is not well-suited to optimize $\mathit{SSE}$ at the same time. Thus, when only considering one of the objectives, the mathematically optimal partitioning fails to establish groups that are similar both with regard to their means and variances. The diamond symbol represents the partition that maximizes the unweighted sum of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$ (i.e., $w_1$ = $w_2$ = 1 in the computation of $\mathit{SSE_{kplus}}$), which yields `r df$kmeans[which.max(df$kplus)] / max(df$kmeans) * 100`% of the global maximum $\mathit{SSE}$, and `r df$kvar[which.max(df$kplus)] / max(df$kvar) * 100`% of the global maximum value of $\mathit{SSE_{Var}}$. Thus, even using a very simple unweighted sum approach, the two constituting criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ can be fulfilled to an astonishing degree.

```{r, fig.cap = "The relationship between the objectives $\\mathit{SSE}$ and $\\mathit{SSE_{Var}}$ across all ways to partition $N = 14$ normally distributed random values into $K = 2$ groups. The partition represented by the square maximizes $\\mathit{SSE}$ and thus minimizes the difference in means between groups; the partition represented by the triangular maximizes $\\mathit{SSE_{Var}}$. The partition represented by the diamond maximizes $\\mathit{SSE_{kplus}}$ for $w_1 = w_2 = 1$, and satisfies both constituting criteria to an almost optimal degree.", fig.height = 5, fig.width = 5}

# Plot SSE vs SSE(VAR)
plot(
  df[, c("kmeans", "kvar")], 
  pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "",
  ylab = ""
)
mtext(side = 2, text = expression(italic(SSE[Var])), line = 3)
mtext(side = 1, text = expression(italic(SSE)), line = 3)

# Illustrate best partition wrt difference in means (i.e., SSE)
points(
  df$kmeans[which.max(df$kmeans)],
  df$kvar[which.max(df$kmeans)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt difference in variance (i.e., SSE(VAR))
points(
  df$kmeans[which.max(df$kvar)],
  df$kvar[which.max(df$kvar)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt k-plus criterion
points(
  df$kmeans[which.max(df$kplus)],
  df$kvar[which.max(df$kplus)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

## Generalizing k-plus anticlustering to higher order moments

The k-plus logic to equalizing group variances as well as group means can be seamlessly generalized to higher order moments. The *t*'th sample moment of a variable $x = (x_1, \ldots, x_N)$ can be computed as 

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^t.
$$

where $t = 2$ gives the variance, $t = 3$ the skewness, and $t = 4$ the kurtosis [@joanes1998]. Thus, by changing $t$---the power of the deviation between mean and data points---we can use the k-plus logic to equalize any desired sample moment between groups. That is, we compute new variables for each original variable and for each distribution moment that should be considered in the anticlustering process. Analogously to the computation of $\mathit{SSE_{Var}}$, we could then define an $\mathit{SSE_{Skew}}$ using $t = 3$ or an $\mathit{SSE_{Kurtosis}}$ using $t = 4$. However, to formalize the computation of the k-plus criterion for an arbitrary number of distribution moments, some new notation has to be provided.

We define the $j$'th column ($j = 1, \ldots, P$) of the original data matrix $\mathbf{X}_{N \times P}$ as $x_{\cdot j} =  (x_{1j}, \ldots, x_{Nj})$ where the column mean---the overall mean of the $j$'th attribute---is $\overline{\mathbf{x}}_j$. Let $T$ be the number of distribution moments that should be similar across $K$ groups. For each moment $t$ ($t = 1, \ldots, T$) and for each variable $x_{\cdot j}$ ($j = 1, \ldots, P$), a new *k-plus variable* $\hat{x}_{t \cdot j} = (\hat{x}_{t1j}, \ldots, \hat{x}_{tNj})$ is defined where

$$
\hat{x}_{tij} = 
\begin{cases}
    x_{ij} & \text{if } t = 1\\
    (x_{ij} - \overline{\mathbf{x}}_j)^t, & \text{if } t > 1.
\end{cases}
$$

This computation results in a three dimensional tensor $\mathbf{\hat{X}}_{T \times N \times P}$ = $\{\hat{x}_{tij}\}_{T \times N \times P}$ where---in comparison to the original data matrix $\mathbf{X}_{N \times P}$---the additional dimension is given by the $T$ distribution moments that are considered in the anticlustering process. When defining $\hat{x}_{tij}$, the condition $t = 1$ is separately included to keep the unchanged original data as first entry in the "moment dimension" in $\mathbf{\hat{X}}_{T \times N \times P}$. It is used to equalize the mean---the first moment---between groups. 

Using $\mathbf{\hat{X}}_{T \times N \times P}$ and a vector of weights $\mathbf{w} = (w_1, \dots, w_T$), the generalized multicriterion for k-plus anticlustering $\mathit{SSE_{kplusGeneralized}}$ can be formalized:

$$
\mathit{SSE_{kplusGeneralized}} =
  \sum\limits_{t=1}^{T} 
  w_t \left(
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (\hat{x}_{tij} - \overline{x}_{tj} ^{(k)})^2
  \right)
$$

Here, $w_t$ is the weight associated with the $t$'th moment and $\overline{\mathbf{X}}^{(k)}_{T \times P} = \{\overline{x}_{tj}^{(k)}\}_{T \times P}$ is a matrix containing the mean values for all k-plus variables in the $k$'th group, i.e.

$$
\overline{x}_{tj}^{(k)} = \frac{1}{n_k} \sum_{i \in C_k} \hat{x}_{tij}
$$

Maximizing $\mathit{SSE_{kplusGeneralized}}$ can be used to minimize differences with regard to the first $T$ distribution moments between $K$ groups.

## The unweighted sum approach for k-plus anticlustering

Some simplifications in the computation of $\mathit{SSE_{kplusGeneralized}}$ can be arranged when ignoring the moment weights $\mathbf{w}$, e.g. by setting $w_t = 1$ ($t = 1, \dots, T$), leading to the *unweighted sum approach* for k-plus anticlustering. In this case, the additional dimension in $\mathbf{\hat{X}}_{T \times N \times P}$, representing distribution moments can be dropped; instead, the tensor is flattened into a two-dimensional matrix, where the variables representing the additional distribution moments are appended to the original data matrix $\mathbf{X}_{N \times P}$, resulting in the augmented matrix $\mathbf{X'}_{N \times \mathit{TP}}$:

$$
  \mathbf{X'}_{N \times \mathit{TP}} =
  \left[ {\begin{array}{ccc|ccc|c|ccc}
    x_{11} & \cdots & x_{1P} & 
        (x_{11} - \overline{\mathbf{x}}_1)^2 & \cdots & (x_{1P} - \overline{\mathbf{x}}_P)^2 & 
        \cdots & 
        (x_{11} - \overline{\mathbf{x}}_1)^T & \cdots & (x_{1P} - \overline{\mathbf{x}}_P)^T \\
    x_{21} & \cdots & x_{2P} & 
        (x_{21} - \overline{\mathbf{x}}_1)^2 & \cdots & (x_{2P} - \overline{\mathbf{x}}_P)^2 & 
        \cdots & 
        (x_{21} - \overline{\mathbf{x}}_1)^T & \cdots & (x_{2P} - \overline{\mathbf{x}}_P)^T \\
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \cdots & \vdots & \ddots & \vdots\\
    x_{N1} & \cdots & x_{NP} & 
        (x_{N1} - \overline{\mathbf{x}}_1)^2 & \cdots & (x_{NP} - \overline{\mathbf{x}}_P)^2 & 
        \cdots & 
        (x_{N1} - \overline{\mathbf{x}}_1)^T & \cdots & (x_{NP} - \overline{\mathbf{x}}_P)^T \\
  \end{array} } \right]
$$

When defining the rows of $\mathbf{X'}_{N \times \mathit{TP}}$ with the notation $x'_{i \cdot} = (x'_{i1}, \ldots, x'_{i(TP)})$ ($i = 1, \ldots, N$), the criterion $\mathit{SSE_{kplusGeneralized}}$ can be computed as

$$
\mathit{SSE_{kplusGeneralized}} =
  \sum\limits_{j=1}^{\mathit{TP}}
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x'_{ij} - \overline{x'}_j ^{(k)})^2.
$$

Thus, when setting the weights for all moments to 1, the k-plus criterion $\mathit{SSE_{kplusGeneralized}}$ has the same form as the standard k-means criterion $\mathit{SSE}$, but is based on different data. Here, computing the k-plus criterion is done by first creating the additional k-plus variables, appending them as new columns to the original data matrix, and then computing the $\mathit{SSE}$ on the augmented data. Using the unweighted sum approach, k-plus anticlustering therefore leaves the original k-means criterion unchanged.

Note that a problem may arise if the weights $\mathbf{w}$ are ignored when computing $\mathit{SSE_{kplusGeneralized}}$. Because the computation of the new k-plus variables involves exponentiation, their range of values is expected to be very different from the original variables. This imbalance can strongly and unpredictably influence the relative weight of the different moments when computing $\mathit{SSE_{kplusGeneralized}}$ (e.g., see Figure 1). To ensure a comparable weight of all criteria, all attributes---i.e., the original data as well as the new k-plus variables---can be standardized before computing $\mathit{SSE_{kplusGeneralized}}$, e.g. via $z$-standardization. This approach will henceforth be referred to as the unweighted standardization approach for k-plus anticlustering.

# Evaluation

Two simulation studies were conducted to evaluate the new k-plus objective for anticlustering. A first small scale simulation compared different approaches for weight selection for the k-plus criterion $\mathit{SSE_{kplusGeneralized}}$. In a second large scale simulation study, the k-plus objective was compared to other popular anticlustering objectives, specifically, the diversity and the k-means objectives. 

The simulation studies were implemented using the statistical programming language R [Version 4.2, @R-base]. The R package `anticlust` [Version 0.6.1, @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R package `faux` [Version 1.1.0, @R-faux] was used to generate the multivariate normal data sets that were processed during Simulation 2. The R packages `dplyr` [Version 1.0.7, @R-dplyr], `ggplot2` [Version 3.3.5, @R-ggplot2], `papaja` [Version 0.1.1.9001, @R-papaja], `tidyr` [Version 1.1.3, @R-tidyr], `fossil` [Version 0.4.0, @R-fossil], and `DescTools` [Version 0.99.45, @R-desctools] were used for data analysis and result presentation. The simulation study is fully reproducible via code and data that has been made accessible in an accompanying OSF repository [@osf2023].

## Optimization algorithm

In both simulation studies, the same local maximum search was applied to optimize the competing anticlustering objectives [Method LCW, @weitz1998]. Building on an initial random allocation, the algorithm proceeds by swapping elements between groups in such a way that each swap improves the objective criterion by the largest possible margin. That is, it starts with the first element and simulates all exchanges with elements that are currently assigned to a different group. After each exchange has been evaluated, the one exchange is realized that improves the objective function the most. No exchange is realized if no improvement is possible. The exchange process is repeated for each element. After that, the procedure restarts at the beginning and is repeated until an iteration through the entire data set no longer yields an improvement. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. In the simulation studies, five repetitions were used. Note that the local maximum search can be performed on the basis of any arbitrary initial grouping. In particular, the initial grouping directly determines the resulting group sizes, because the following exchange procedure never changes the sizes of the groups. Hence, while the data sets in the simulation studies were always partitioned into equal-sized groups, it is possible to specify different sized groups through the initial assignment, or if the data set cannot evenly be split, it is possible to ensure that the group sizes differ by 1 at most.

## Simulation 1: Selecting weights for k-plus anticlustering

```{r getresultsSim1}
dir <- "./Simulation_1/"
files <- list.files(dir, pattern = ".Rdata")
results <- list()
for (file in files) {
  results <- c(results, get(load(paste0(dir, file))))
}

```

In general, the weighting scheme for k-plus anticlustering can become highly complex as it might encompass an arbitrary number of distribution moments, and hence, an arbitrary number of weights. While an exhaustive evaluation of the influence of weights is beyond the scope of the current paper, a small scale simulation study was conducted to investigate the influence of weights for a selected number of moments. The results of this initial test determined the benchmark implementation of k-plus anticlustering in the follow-up large scale comparison in Simulation 2.

In Simulation 1, the target was the optimization of $\mathit{SSE_{kplusGeneralized}}$ with $T = 4$, i.e. minimizing differences with regard to group means, variances, skewness and kurtosis. Six weighting schemes were compared. Two unweighted approaches were implemented by defining $\mathbf{w} = (1, 1, 1, 1)$. The unweighted standardization approach implemented a $z$-standardization of all columns---the original as well as the augmented k-plus variables---of the input data before the criterion $\mathit{SSE_{kplusGeneralized}}$ was optimized. In the second case, the unstandardised data matrix was used. Four weighted approaches were implemented by giving a tenfold increased weight to one of the four moments, respectively. That is, to compute $\mathit{SSE_{kplusGeneralized}}$ favouring similarity in means during the optimization process, I set $\mathbf{w} = (10, 1, 1, 1)$; the weights favouring similarity in variances, skewness and kurtosis, were defined as $\mathbf{w} = (1, 10, 1, 1)$, $\mathbf{w} = (1, 1, 10, 1)$, and $\mathbf{w} = (1, 1, 1, 10)$, respectively. The weighted approaches did not use a standardization of the input data.

To compare the different weighting schemes, `r length(results)` data sets from were generated from a normal distribution ($M = 0$, $\mathit{SD} = 1$). The number of variables $P$ was determined randomly for each data set and varied between 1 and 5; the number of anticlusters $K$ was also determined randomly for each data set and varied between 2 and 4; the sample size $N$ randomly varied between 20 and 100 with the restriction that $N$ had to be divisible by $K$. For each data set, $\mathit{SSE_{kplusGeneralized}}$ was optimized using the local maximum algorithm, once for each of the six competing weighting schemes. 


```{r}
final_tab_sim1 <- round(Reduce("+", results) / length(results) * 100, 2)
rownames(final_tab_sim1) <- c(
  "$\\mathbf{w} = (1, 1, 1, 1)$ / unstandardized",
  "$\\mathbf{w} = (1, 1, 1, 1)$ / standardized",
  "$\\mathbf{w} = (10, 1, 1, 1)$",
  "$\\mathbf{w} = (1, 10, 1, 1)$",
  "$\\mathbf{w} = (1, 1, 10, 1)$",
  "$\\mathbf{w} = (1, 1, 1, 10)$"
)
colnames(final_tab_sim1) <- c(
  "$\\mathit{SSE}$",
  "$\\mathit{SSE_{Var}}$", 
  "$\\mathit{SSE_{Skew}}$",
  "$\\mathit{SSE_{Kurtosis}}$",
  "Average"
)
note <- paste0("Cells indicate how well the maximization of $\\mathit{SSE_{kplusGeneralized}}$ fulfilled each of the four constituting criteria $\\mathit{SSE}$, $\\mathit{SSE_{Var}}$, $\\mathit{SSE_{Skew}}$, and $\\mathit{SSE_{Kurtosis}}$, depending on the weighting scheme. For each simulation run, the relative performance was computed as the $\\mathit{SSE}$ that was obtained by the given weighting scheme, divided by the best $\\mathit{SSE}$ that was obtained by any weighting scheme in this simulation run. Cell values represent the average relative performance across all ", length(results), " simulation runs.")

apa_table(
  final_tab_sim1, 
  escape = FALSE, 
  caption = "Average relative performance of the competing weighting schemes in Simulation 1.",
  note = note,
  stub_indents = list("Unweighted approaches" = 1:2,
                      "Weighted approaches" = 3:6)
)

unstd_best <- sapply(results, function(x) names(which.max(x[, 5]))) == "unweighted_std"

# significance test
sig_test <- prop.test(x = sum(unstd_best), n = length(unstd_best), p = 1/6)

```

Table 1 illustrates how well each of the six weighting schemes fulfilled each of the four constituting criteria, averaged across the `r length(results)` simulation runs. For the weighted approaches, the criterion that had received the highest weight during the optimization was fulfilled to the highest degree, as should be expected. Interestingly, the unweighted standardization approach had the best average performance across the four criteria. While the degree of advantage by itself does not seem to be remarkable, this pattern was observed in an astonishing `r mean(unstd_best) * 100`% of all simulation runs, while the level of chance would have been `r 1/6 * 100`% (`r format_p(sig_test$p.value)`). Among the competing weighting schemes, the unweighted standardization approach therefore yielded the most promising results. It also yielded satisfying results with regard to maximizing the standard $\mathit{SSE}$, even slightly outperforming the weighting scheme that explicitly favoured $\mathit{SSE}$. Hence, the unweighted standardization approach implicitly gave high priority to minimizing differences in means. This is a pleasant result because, as discussed earlier, for the higher order moment $\mathit{SSE}$s to be valid, the group means have to be close to the overall mean, which is best accomplished if the standard $\mathit{SSE}$ has high priority. 

## Simulation 2: Comparing k-plus to other anticlustering objectives

In Simulation 2, the performance of two k-plus criteria was compared to the performance of two traditional anticlustering criteria. "Standard" k-plus anticlustering was implemented by maximizing $\mathit{SSE_{kplus}}$, thus minimizing differences with regard to group means and variances. "Generalized" k-plus anticlustering was implemented by maximizing $\mathit{SSE_{kplusGeneralized}}$ using $T = 4$, thus minimizing differences with regard to group means, variances, skewness and kurtosis. The unweighted standardization approach that performed best in Simulation 1 was employed. Thus, to optimize a k-plus criterion, the original data set was augmented to incorporate the additional criteria and all variables---the original and the augmented variables---were $z$-standardized. Subsequently, the standard k-means criterion $\mathit{SSE}$ was maximized on the basis of the augmented data set. The k-plus methods were compared to (i) k-means anticlustering, minimizing differences with regard to group means, (ii) diversity anticlustering, maximizing the sum of pairwise Euclidean distances within groups, and also (iii) a random allocation of elements to groups.

### Conditions

For Simulation 2, I generated 10,000 data sets following a normal distribution. The data sets were subsequently processed by the competing methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features varied between 2 and 5; (c) the standard deviation of all features was set to 1, 2 or 3 (the mean was always zero); (d) the correlation between all features was $r = 0$, $r = .1$, $r = .2$, $r = .3$, $r = .4$ or $r = .5$. All anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$ (i.e., each method was applied 30,000 times). Groups sizes were always equal. 

### Evaluation criteria

After the four anticlustering methods and the random allocation procedure had been applied to the 10,000 data sets for $K = 2$, $K = 3$, $K = 4$, I investigated how well the competing methods were able to create similar groups. To this end, I analyzed the discrepancy in means, standard deviations, skewness and kurtosis between groups---for each data set for each $K$. To quantify the discrepancy in group means, the following computation was used: for each of the (2, 3, 4 or 5) features, all (2, 3 or 4) group means were computed. For each feature, the difference between the minimum and maximum group mean was used as a measure of discrepancy; the global discrepancy in means ($\Delta M$) was then determined as the average discrepancy across features. The same procedure was applied to compute the discrepancy in standard deviations ($\Delta \mathit{SD}$), skewness ($\Delta \mathit{Skew}$) and kurtosis ($\Delta \mathit{Kurtosis}$).

As a secondary purpose of Simulation 2, I investigated the similarity of partitions obtained when optimizing the different anticlustering objectives. In cluster analysis, partitions identified through different approaches tend to overlap considerably, but it is an open question whether this result also holds for different anticlustering partitions. To tackle this issue, I computed the adjusted Rand index (ARI) for each pair of partitions that were returned by the competing methods [@rand1971; @hubert1985]. The ARI is an index of the similarity between two partitions, where an index of 0 indicates no similarity and a value of 1 indicates the highest possible similarity (i.e., when two partitions are the same). The pairwise ARIs were computed for each simulation run, i.e., for data set and for each $K$. 

### Results

Table 2 displays the global simulation results aggregated across all conditions. Standard k-means performed best at minimizing discrepancy with regard to means ($\Delta \mathit{M}$), but did not outperform a random assignment with regard to all other distribution characteristics. Replicating @papenberg2020, k-means' performance with regard to similarity in standard deviations was even below that of a random assignment. K-plus anticlustering addressed $\Delta \mathit{M}$ almost as well as k-means, and at the same time minimized discrepancy with regard to the standard deviations ($\Delta \mathit{SD}$); both k-plus approaches strongly outperformed all other methods with regard to minimizing $\Delta \mathit{SD}$. 

\begin{singlespace}

```{r Table1}

# Load required packages
library(dplyr)
library(ggplot2)
library(tidyr)

## Analyze data for K = 2 and K = 3 and K = 4
simulation_results <- list()
for (K in 2:4) {
  filename <- paste0("./Simulation_2/results-K", K, "-objectives-raw.csv")
  df <- read.csv(filename, sep = ";", stringsAsFactors = FALSE)
  df$K <- K
  simulation_results[[paste0("K-", K)]] <- df
}

df <- do.call(rbind, simulation_results)
rownames(df) <- NULL

# Make long format
ldf <- pivot_longer(
  df,
  cols = paste0(c("means", "sd", "skew", "kur"), "_obj"),
  names_to = "Objective",
  names_pattern = "(.*)_obj"
)

ldf <- ldf[ldf$method != "k-plus-correlation", ] # removed because this deserves closer attention than what was given in the paper; check out the first version of the preprint for the previous results

ldf$method[ldf$method == "k-plus-skew-kurtosis"] <- "k-plus generalized"

## Global results, aggregated across all simulation variables
tab <- ldf %>% 
  group_by(method, Objective) %>% 
  summarise(Mean = round(mean(value), 3)) %>% 
  pivot_wider(names_from = Objective, values_from = Mean) %>% 
  select(c(means, sd, skew, kur))

colnames(tab) <- c("",
  "$\\Delta M$", 
  "$\\Delta \\mathit{SD}$",
  "$\\Delta \\mathit{Skew}$",
  "$\\Delta \\mathit{Kurtosis}$"
)

apa_table(
  tab,
  caption = "Results of Simulation 2.",
  note = "The global results of the simulation study aggregated across all conditions. Cells contain information about the average global between-group discrepancy with regard to means, standard deviations, skewness and kurtosis. Lower values indicate less discrepancy, i.e., higher between-group similarity. Each cell value is the result of averaging across 30,000 data points (10,000 data sets $\\times$ $K = 2$, $K = 3$ or $K = 4$).",
  escape = FALSE,
  align = c("r")
)

```

\end{singlespace}

```{r Figure1, fig.width = 8, fig.height = 10, fig.cap = "Results of the simulation study: Depicts the performance of the anticlustering methods with regard to minimizing discrepancy in means, variances, skewness and kurtosis (split by $N$, averaged across the remaining variables that varied in the simulation)."}

# Plot the results, by N
axis_ticks <- seq(24, 300, by = 12)
axis_labels <- axis_ticks
axis_labels[c(FALSE, TRUE)] <- ""

ldf %>% 
  group_by(method, Objective, N) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(method != "random") %>% 
  mutate(
    Objective = ordered(
      Objective, 
      levels = c("means", "sd", "skew", "kur"),
      labels = paste0(
        "Delta[italic(",
        c("M", "SD", "Skew", "Kurtosis"),
        ")]"
      )
    ) 
  ) %>%
  ggplot(aes(x = N, y = Mean, colour = method)) + 
  geom_line(aes(linetype = method), size = .85) + 
  facet_grid(rows = vars(Objective), scales = "free", labeller = label_parsed) + 
  ylab("Mean discrepancy")+
  xlab(expression(italic(N))) +
  theme_bw(base_size = 16) + 
  scale_x_continuous(
    breaks = axis_ticks, 
    labels = axis_labels
  )

```


Figure 2 shows that with increasing $N$, k-means and the k-plus methods converged on the same level with regard to minimizing $\Delta \mathit{M}$. This is an important observation because it shows that k-plus anticlustering is capable of addressing multiple objectives at the same time---especially when $N$ increases. However, while k-plus anticlustering was capable of addressing multiple objectives simultaneously, increasing the number of optimization criteria made it more difficult to fulfill each single criterion: generalized k-plus performed worse at minimizing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$ than standard k-plus that only considered the mean and variances. However, as intended, generalized k-plus was best at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$ and at the same time, this k-plus objective maintained a comparably good level of addressing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$, outperforming diversity anticlustering. 

It is maybe surprising that generalized k-plus does not seem to have performed much better than diversity anticlustering with regard to skewness and kurtosis, even though the objective has been specifically tailored to these criteria. Figure 2 gives a more fine grained assessment of this observation: Surprisingly, for small $N$, generalized k-plus was even worse than diversity anticlustering at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. However, with increasing $N$, it clearly outperformed diversity anticlustering. These results illustrate the cost of optimizing several criteria at the same time: For low $N$, generalized k-plus was apparently preoccupied with fulfilling the objectives $\Delta \mathit{M}$ and $\Delta \mathit{SD}$. Increasing $N$ then facilitated to also address $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. Generally, it should be noted that diversity anticlustering is a good all-rounder method that tends to address all distribution characteristics. For the specific objectives that are addressed by the k-plus methods, however, diversity anticlustering was outperformed. The Appendix includes additional figures splitting the simulation results according to the other variables that varied between simulation runs (number of features; correlation between features; standard deviation of features; number of groups).

```{r}

ARIs <- as.matrix(read.table("Simulation_2/global_results_ari.csv", sep = ","))
colnames(ARIs) <- c(
  "diversity", "k-means", "k-plus", "k-plus generalized", "random"
)
rownames(ARIs) <- colnames(ARIs)
ARIs <- round(ARIs, 3)
ARIs[upper.tri(ARIs)] <- ""

# removing leading 0
ARIs <- gsub("0.", ".", ARIs, fixed = TRUE)

apa_table(
  ARIs,
  caption = "Average pairwise ARI returned by the competing methods in Simulation 2.",
  note = "Each cell value is the result of averaging across 30,000 data points (10,000 data sets $\\times$ $K = 2$, $K = 3$ or $K = 4$).",
  escape = FALSE,
  align = c("l")
)

```

Table 3 illustrates the degree of overlap between anticlustering partitions that were identified in Simulation 2. Overall, it is shown that there was hardly any similarity between anticlustering partitions that were obtained via optimization of the different criteria. The average ARI was almost 0 in all cases---indicating no similarity of partitions whatsoever. Moreover, the ARI between two anticlustering partitions hardly differed from the ARI between an anticlustering partition and a random partition. This constitutes an interesting result, illustrating that high between-group similarity can be achieved via very different partitions.

# Applying k-plus anticlustering using the R package anticlust 

This section demonstrates how readers can easily employ k-plus anticlustering using the R package `anticlust` [@papenberg2020]. The package can be installed in the R environment using the `install.packages()` command.

```R
install.packages("anticlust")
```

The function `kplus_anticlustering()` available in `anticlust` implements the unweighted sum approach for k-plus anticlustering.

## Example Application I

In the first example, I used the OASIS norming data that is freely available online [@kurdi2017introducing]. Kurdi et al. assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense measured how the same 900 images were rated with regard to beauty. I used the function `kplus_anticlustering()` to divide all images into 9 groups of 100 images each. The first argument of the function call is a variable named `features`, which contains the image data as a table: 900 rows representing images and 3 columns representing the features beauty, arousal, and valence. An OSF repository accompanying this manuscript contains the full code to reproduce the example [@osf2023].

```R
library(anticlust) # load the package anticlust

kplus_anticlustering(
  features,
  K = 9,
  variance = TRUE,
  skew = TRUE,
  kurtosis = FALSE,
  moments = NULL,
  method = "local-maximum",
  standardize = TRUE
)
```

The function call takes about 10 seconds on a contemporary personal computer. The function `kplus_anticlustering()` has three boolean arguments to specify whether variance, skewness and kurtosis should be included as part of the k-plus criterion. Only the argument `variance` is set to `TRUE` by default---corresponding to the understanding that k-plus is an extension to k-means that at least equalizes variances in addition to means. The arguments corresponding to skewness and kurtosis have to be specifically "turned on", i.e., by setting them to `TRUE` in the function call of `kplus_anticlustering()`. If other higher order moments should be included as part of the optimization, the optional argument `moments` can be used to specify the desired moments as an integer vector. The argument `method = "local-maximum"` ensures that the k-plus criterion is optimized using the local maximum search method described in the previous section. The argument `K` describes the number of groups. By default, the function ensures that all features---the original and the augmented variables---are standardized before the optimization starts. Thus, by default, the unweighted standardization approach is used. Whether standardization is used can be adjusted using the boolean argument `standardize`.

Table 4 shows the results of the anticlustering application by listing the descriptive statistics (means, standard deviations and skewness) for each of the 9 groups and for each of the 3 features. Table 4 also contains the results after applying k-means anticlustering and diversity anticlustering. It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations, and the skewness was also very evenly matched. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations and skewness. Diversity anticlustering was well-suited to match means, standard deviations and skew, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---diversity anticlustering does not directly maximize similarity with regard to these criteria, but instead maximizes within-group heterogeneity.

```{r}

example1_file <- "results_oasis_same_size.csv"
if (!file.exists(example1_file)) {

    oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
    )

    features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

    K <- 9

    # k-plus variance / skew
    groups_kplus <- kplus_anticlustering(
        features,
        K = K,
        variance = TRUE,
        skew = TRUE,
        method = "local-maximum",
        standardize = TRUE
    )

    # k-means
    groups_kmeans <- anticlustering(
        features,
        K = K,
        objective = "variance",
        method = "local-maximum",
        standardize = TRUE
    )

    # diversity
    groups_diversity <- anticlustering(
        features,
        K = K,
        objective = "diversity",
        method = "local-maximum",
        standardize = TRUE
    )


    # Functions to compute means / SD / skew per group 
    descriptives_by_group <- function(features, anticlusters, FUN, name) {
        df <- data.frame(
            t(data.frame(lapply(by(features, anticlusters, FUN), c)))
        )
        df$group <- 1:nrow(df)
        df$Descriptive = name
        df
    }


    all_descriptives_by_group <- function(features, anticlusters) {
    means <- descriptives_by_group(features, anticlusters, colMeans, "mean")
    sds <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, sd),
        "sds"
    )
    skew <- descriptives_by_group(
        features, 
        anticlusters, 
        function(x) sapply(x, DescTools::Skew),
        "skew"
    )
    rbind(means, sds, skew)
    }


    kplus <- all_descriptives_by_group(features, groups_kplus)
    kplus <- data.frame(kplus, Objective = "kplus")

    kmeans <- all_descriptives_by_group(features, groups_kmeans)
    kmeans <- data.frame(kmeans, Objective = "kmeans")
    diversity <- all_descriptives_by_group(features, groups_diversity)
    diversity <- data.frame(diversity, Objective = "diversity")

    all_objs <- rbind(kplus, kmeans, diversity) %>% 
    pivot_wider(names_from = Descriptive, values_from = c(beauty_mean, Valence_mean, Arousal_mean))
    
    write.table(
        all_objs, example1_file, row.names = FALSE, sep = ";"
    )

    
} else {
  all_objs <- read.csv(example1_file, sep = ";")
}

table_all_objs <- subset(all_objs, select = -Objective)
table_all_objs <- as.matrix(table_all_objs)
colnames(table_all_objs) <- c("Group", rep(c("$M$", "$\\mathit{SD}$", "Skew"), 3))
table_all_objs <- apply(table_all_objs, 2, force_or_cut)
#mode(table_all_objs) <- "character"

apa_table(
  table_all_objs,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The k-plus criterion that was optimized included a term to minimize differences with regard to means, variances, and skewness.",
  escape = FALSE,
  align = c("c", rep("r", 9)),
  col_spanners = list(Beauty = c(2, 4), Valence = c(5, 7), Arousal = c(8, 10)), 
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Diversity anticlustering}" = 19:27)
)
```

Table 5 illustrates another advantage of k-plus anticlustering over diversity anticlustering: When creating groups of unequal size, diversity anticlustering tends to increase the spread of the data in the largest group in comparison to the other groups. K-plus anticlustering strives for between-group similarity regardless of group size. Unequal group sizes can be requested by passing the different group sizes to the argument `K`. The following code was used to equalize means and variances between four groups using the k-plus approach:

```R
kplus_anticlustering(
  features,
  K = c(100, 100, 100, 600),
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    if (criterion == "kplus") {
        oasis[[paste(criterion, "group", sep = "_")]] <-  kplus_anticlustering(
            features,
            K = c(100, 100, 100, 600),
            method = "local-maximum"
        )
    } else {
        oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
        features,
        standardize = TRUE,
        K = K,
        objective = criterion,
        method = "local-maximum"
        )
    }
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Diversity anticlustering}" = 9:12)
)
```

## Example Application II: Small data set ($N = 96$)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore, a second application illustrates that k-plus anticlustering yields satisfactory results in practice even when group sizes are smaller. I make use of a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Marie L. Schaper [@schaper2019metacognitive; @schaper2019metamory]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

Table 6 illustrates the results that are obtained when using standard[^standardkplus] k-plus anticlustering to divide the stimulus set into 6 groups of size `r nrow(schaper2019) / 6` each. Even though the means and standard deviations are no longer perfectly matched between groups, arguably they are similar enough for any practical purposes.

[^standardkplus]: I refer to *standard* k-plus anticlustering as the minimization of differences in means and variances, but not in higher order moments. 

```{r, echo = FALSE}
features <- schaper2019[, 3:6]
anticlusters <- anticlustering(
  features,
  K = 6,
  objective = "kplus",
  method = "local-maximum",
  repetitions = 5,
  standardize = TRUE
)
```

```{r}

tab <- mean_sd_tab(features, anticlusters)
tab <- cbind(1:6, 96 / 6, tab)
colnames(tab) <- c("Group", "N", "Typicality", "Atypicality", "Syllables", "Frequency")

apa_table(
  tab,
  caption = "Descriptive statistics by group for the Schaper et al. (2019a; 2019b) data set, after applying k-plus anticlustering.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r")
)

```

# Discussion

Anticlustering is a partitioning method that aims for high between-group similarity and high within-group heterogeneity. In this paper, I presented the k-plus criterion for anticlustering, which focuses on maximizing between-group similarity. K-plus represents between-group similarity as discrepancies in distribution moments between groups: means, variances, skewness, kurtosis, and higher order moments. Depending on the application, a different subset of these objectives can be included in the anticlustering process. The k-plus criterion is an extension of the classical k-means criterion, which only reflects how similar groups are with regard to their means. Interestingly, when using the unweighted sum approach presented here, the k-plus extension does not change the original k-means criterion, because additional criteria can be incorporated by augmenting the input data.

Two simulation studies and examples on real norming data showed that the k-plus criterion is well-suited to maximize between-group similarity with regard to multiple criteria. Given that similarity with regard to variances was achieved with almost no sacrifice to similarity in means, k-plus anticlustering should be considered as the default replacement of classical k-means when striving for between-group similarity.[^binarydata] The results did however show that fulfilling multiple criteria at the same time can be challenging in small data sets. In practice, users can try out different objectives and see which specification fulfills their needs. A reasonable approach is to start "greedy", e.g. by specifying that multiple distribution moments should equalized. If the results of such a greedy setup are not satisfying, users can reduce their requirements by opting for fewer criteria (e.g., only the mean and variance). Using the open source R package `anticlust`, the (repeated) application of k-plus anticlustering is easily accessible and free. 

[^binarydata]: A reasonable exception is given when the input data consists of binary variables because for binary data, the mean and the variance are directly dependent. 

## Limitations and outlook

The present paper focused on the new k-plus criterion for anticlustering, but only briefly considered the multicriterion nature of this criterion. A simple unweighted sum approach was used to optimize the k-plus objective, while more sophisticated algorithmic procedures exist to tackle multicriterion optimization problems [e.g., @brusco2009cross]. The sole focus on the objective function followed two reasonings. First, the applications and the simulations showed that the unweighted standardization approach yielded satisfying results that probably suffice in most applications. Moreover, it is possible that k-plus anticlustering has a fortunate solution space where many suitable partitions exist that fulfill multiple criteria to a satisfactory degree (as suggested by Figure 1). Still, more sophisticated approaches may yield theoretically improved results, as well as the opportunity to deliberately select among partitionings that may be preferred according to different k-plus objectives. @brusco2019 gave an example of how direct algorithms can be employed to optimize multiple criteria in anticlustering applications aiming for high within-group heterogeneity. Future research should investigate if k-plus anticlustering may profit from similar algorithmic treatment. Specifically, it should be investigated if the bicriterion approach by Brusco et al. can be transformed into a multicriterion algorithm for the k-plus objective. A second reason for focusing on the k-plus objective per se---independently from the algorithm that is used to optimize it---was to obtain a fair comparison to existing anticlustering objectives. In the simulation study, differences in performance could not have been interpreted if the objective function as well as the algorithm used to optimize it had differed between anticlustering methods. It was of particular interest to compare the k-plus objective with the popular diversity objective. @papenberg2020 compared k-means anticlustering and diversity anticlustering[^clusterediting] in a simulation study and concluded that the diversity criterion should be preferred because k-means does not equalize the spread of the distribution between groups. In contrast, maximizing the diversity leads to an appropriate balance between location and spread. Using the k-plus extension, however, the k-means criterion has regained attractivity: When opting for similarity in means and variances---and for increasing $N$, skewness and kurtosis as well---, k-plus anticlustering clearly outperformed diversity anticlustering. Future research should investigate whether this advantage also holds for other data distributions than normal. For example, it may be hypothesized that a generalized k-plus criterion is even more useful when applied to data that is not symmetric or contains outliers, because it is able to specifically incorporate the skewness and kurtosis as criteria in the anticlustering process. 

As an additional outlook, it should be noted that because the covariance is defined as an expected value, the k-plus logic can be extended to minimize differences with regard to covariances, a possibility that was not investigated in the current paper. In this case, we have to compute an additional variable for each of pair of variables whose covariance should be similar between groups. In future research, the unweighted standardization approach presented here may serve as a starting point for investigating covariance similarity in anticlustering applications. 

[^clusterediting]: In their study, they used the term anticluster editing to refer to the maximization of the diversity criterion. This is because the diversity criterion is minimized as part of the cluster editing method [@shamir2004vf].

## Closing remarks on the nature of anticlustering

```{r}
df <- read.csv("partition_objectives.csv")
```

Anticlustering is usually implemented by reversing an objective function used in classical cluster analysis. Whereas cluster analysis seeks homogeneity within groups and separation between groups, the reversal results in heterogeneity within groups and similarity between groups. While tightly linked in a mathematical sense, an important semantic difference is that cluster analysis is concerned with finding "true" structure in the data, e.g., by defining a taxonomy on naturally occurring groups [@everitt1979]. Anticlustering instead imposes structure on the data, while the anticlusters themselves do not represent any underlying "true" pattern. Correspondingly, different---but similarly adequate---anticlustering partitions tend to look very different. In contrast, different clustering solutions tend to look similar because good clusterings are characterized by assigning similar elements into the same cluster and dissimilar elements into different clusters. A good anticlustering solution however does not depend on assigning individual items either to the same or a different group, independent of their pairwise dissimilarity.[^dispersion] This result was observed in Simulation 2 and can also be clarified using Figure 1. Here, we observe a high concentration of suitable anticlustering partitions but only few suitable clustering solutions. An astonishing `r mean(df$kmeans / max(df$kmeans) >.999) * 100`% of all partitions are within 0.1% of the global maximum in $\mathit{SSE}$, i.e., the best k-means anticlustering solution. In contrast, there are only `r sum(min(df$kmeans)/ df$kmeans > .90)` partitions (`r mean(min(df$kmeans)/ df$kmeans > .90)*100`%) that approximate the global minimum in $\mathit{SSE}$ (i.e., the best k-means clustering solution) by even 10%. Thus, even though the problems are based on the same objective function, cluster analysis and anticlustering exhibit a stark discrepancy with regard to the solution space of partitions. Interestingly, the k-plus objective introduces an additional distinction between cluster analysis and anticlustering. Reversing the k-plus anticlustering objective no longer yields a useful clustering interpretation, because identifying groups that are different with regard to higher order moments does not constitute a clustering. Thus, even though k-plus extends the classical k-means clustering criterion, it is a pure anticlustering objective.

[^dispersion]: An exception is given when using the dispersion objective, which ensures that the most similar elements are assigned to different clusters. However, anticlustering objectives striving for overall between-group similarity are mostly unaffected by the grouping of individual item pairs.

An interesting mixture of cluster analysis and anticlustering is fair clustering [e.g. @chhabra2021; @delBarrio2022]. Fair clustering performs a cluster analysis under the restriction that the distribution of a set of "protected" attributes---such as race, sex or age---should be similar between clusters. Thus, as compared to pure cluster analysis, fair clustering biases the identification of groups with regard to the protected attributes, striving for algorithmic fairness. A relevant difference to anticlustering is that fair clustering is still concerned with identifying "true" structure in the data, while anticlustering divides data sets into artificial subgroups in such a way that between-group similarity / within-group heterogeneity is maximized. However, these methods are logically similar because anticlustering corresponds to fair clustering when only considering the protected attributes. Hence, a fruitful outlook for research might be to investigate if approaches stemming from the literature on fair clustering can be adopted for anticlustering, and vice versa.

## Conclusion

This paper introduced k-plus anticlustering for maximizing between-group similarity, improving the classical k-means approach. K-plus anticlustering can be adopted whenever a group of elements has to be partitioned into equivalent parts. The method is accessible and easily usable via the R package `anticlust`.

\newpage

# References

\begingroup
<div id="refs" custom-style="Bibliography"></div>
\endgroup


\newpage

# (APPENDIX) Appendix {-}

```{r child = "appendix.Rmd"}
```

