---
title             : "k-plus Anticlustering: An Improved k-means Criterion for Maximizing Between-Group Similarity"
shorttitle        : "k-plus anticlustering"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich Heine University Düsseldorf.

abstract: |

    Anticlustering refers to a collection of methods that partition data sets into disjoint groups while aiming for high between-group similarity and high within-group heterogeneity. Anticlustering is usually approached by maximzing instead of minimizing a clustering objective such as the k-means criterion. Introducing k-plus, this paper presents an extension of k-means to maximize between-group similarity. While k-means anticlustering minimizes between-group-differences only with regard to the mean of attribute values, k-plus anticlustering can be used to minimize between-group-differences with regard to the variance, higher order moments, and covariance structure. To minimize differences in variance, each variable is duplicated by computing the squared deviation between the original values and the variable's mean. Because the variance is defined as the expected value of the squared deviations from the mean, appending these duplicated variables to the data set---and then applying standard k-means anticlustering---suffices to equalize the variance between groups. Following a similar logic, differences in skewness, kurtosis and covariances can be minimized. Thus, while k-plus is a separate criterion for anticlustering, it can be reduced to an augmentation of the input data while the original k-means objective remains unchanged. A computer simulation and practical examples on real data show that k-plus anticlustering achieves high similarity with regard to mean attribute values as well as the variance and higher order moments, usually with no noticable sacrifice to either criterion. The k-plus extension is therefore preferred over classical k-means anticlustering for optimizing between-group similarity. Example are given on how k-plus anticlustering can be applied using the open source R package `anticlust`, which is freely available via CRAN (https://cran.r-project.org/package=anticlust) and Github (https://github.com/m-Py/anticlust).
  
bibliography      : ["lit.bib"]
  
keywords          : "Anticlustering, k-means, k-plus, variance, covariance, skewness, kurtosis, higher order moments"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "doc"
output            : 
  papaja::apa6_pdf: 
    number_sections: true
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
library("dplyr")
library("ggplot2")
library("tidyr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE, dev = "cairo_pdf") 

```

Anticlustering is a collection of methods to partition a set of elements into disjunct groups, with the goal of obtaining high between-group similarity or within-group heterogeneity [@brusco2019; @spath1986; @valev1998]. Oftentimes, both goals coincide: some anticlustering objectives imply that similarity between groups is maximal whenever similarity within groups is minimal [@brusco2019; @papenberg2020; @feo1990]. Anticlustering thereby reverses the logic of its better known twin---cluster analysis---which seeks homogeneity within clusters and heterogeneity between clusters [@rokach2005; @steinley2006]. Whereas a variety of different approaches exist for cluster analysis such as hierarchical methods, model-based clustering, or network approaches [see @brusco_emergent_2012, for an overview], anticlustering is usually formalized as a partitioning process that divides a set of elements into disjunct groups in such a way that an objective function---representing between-group similarity and/or within-group heterogeneity---is optimized [@papenberg2020; @spath1986; @brusco2019].

Anticlustering is useful in many tasks surrounding research work in psychology. For example, anticlustering can be used to split a test into parts of equal difficulty [@gierl2017], to assign students to work groups [@baker2002] or when assigning stimuli to different---but parallel---experimental conditions [@lahl2006]. For a more comprehensive overview of anticlustering applications in Psychology, see @brusco2019 and @papenberg2020. Solving anticlustering problems "by hand" is a tedious and time-consuming task, and the quality of manual partitioning is usually subpar. Fortunately, these problems can be formalized as mathematical optimization problems [e.g., @brusco2019; @spath1986; @baker2002; @fernandez2013] and accessible open source software solutions to tackling these problems exist [@papenberg2020]. 

Anticlustering methods can be distinguished by the input data and the objective function that is used to optimize group between-group similarity / within-group heterogeneity. Some anticlustering objectives quantify group similarity on the basis of pairwise dissimilarity rating such as the Euclidean distance. The most prominent criterion on the basis of pairwise dissimilarity ratings is the *diversity*, which is the total sum of dissimilarities between any elements within the same group. Maximizing the diversity optimizes within-group heterogeneity, which simultaneously leads to similar groups---if all groups have the same size. Anticlustering objective functions usually represent the reversal of a clustering objective, only the direction of the optimization is changed.  Another important criterion based on pairwise dissimilarities is the *dispersion*, which is the minimum dissimilarity between any two elements within the same group [@fernandez2013; @brusco2019]. Maximizing the dispersion increases the within-group heterogeneity by ensuring that any two elements in the same group as dissimilar from each other as possible.

Oftentimes, researchers work with attribute values [or *features*; @dry2009] instead of dissimilarity ratings. This use case is for example common when selecting stimuli for an experiment on the basis of norming data [e.g., @kurdi2017introducing].  For word stimuli in psycholinguistic experiments, norming data may consist of ratings for imagery and concreteness, as well as orthographic variables [@friendly1982]. In other cases, attribute values are binary and may represent the presence or absence of a feature [@tversky1977]. In the context of anticlustering, one approach for handling attribute data is to compute an appropriate dissimilarity measure such as the pairwise Euclidean or squared Euclidean distance across the set of attributes [@brusco2019]. Subsequently, an anticlustering criterion such as the diversity can be optimized to partition the elements into groups. A different approach to anticlustering directly works with the attribute data by maximizing the k-means criterion. K-means is probably the best-known clustering method. Reversing the k-means objective function---using maximization instead of minimization---has been independently recognized as a useful anticlustering tool by Späth [-@spath1986] and Valev [-@valev1998; -@valev1983]. 

In k-means clustering, the sum of the squared Euclidean distances between elements and the center of the cluster they are assigned to is minimized [@brusco2006branch], usually using the k-means heuristic [@jain2010; @steinley2006]. Minimizing the k-means criterion simultaneously maximizes between-cluster separation and within-cluster homogeneity [@aloise2009np]. For the anticlustering application, the k-means criterion is maximized instead, thereby achieving similarity between groups. Specifically, as @spath1986 noted, k-means anticlustering minimizes differences with regard to the means of the numeric attributes across clusters. Hence, groups are similar with regard to the mean of the distribution of the numeric attributes wheras other parameters---such as the variance---are ignored. When requiring that groups are overall similar to each other, this focus may be misguided as similar means can be obtained even when the underlying distributions are different [e.g., @anscombe1973]. @papenberg2020 showed that maximizing the diversity (using the Euclidean distance) is better suited to minimize difference with regard to both the mean and the variance of the data; even an entirely random split usually resulted in more similar variances because k-means solely focuses on the features' means.

To optimize overall group similarity---and hence to overcome the limitations of k-means anticlustering---this paper presents k-plus anticlustering. The k-plus criterion is an extension of the k-means objective that includes one or several terms to quantify similarity with regard to other important characteristics of a data set rather than only the mean. Hence, the k-plus criterion is not a single objective function for anticlustering, but instead a familiy of objectives that extend the standard k-means objective to minimize differences between groups with regard to the variance, covariances, and higher order moments such as skewness and kurtosis. Which of these objectives is pursued depends on a researchers' needs. In the most basic---and arguably most important---case, k-plus anticlustering can be used to not just minimize differences with regard to the mean of variables but also their variance. Measures of location and spread are arguably most important to researchers when they "eyeball" their data and they are routinely reported as characterizations of a data set. Hence, by default, the k-plus objective is refered to when considering the mean and the variance of variables in the anticlustering process. 

While less routinely reported, skewness are kurtosis are also important descriptive characterizations of a data set [@decarlo1997; @westfall2014]. To optimize overall similarity between groups, it may thus be desirable to also minimize differences with regard to these distribution moments. In principle, any other higher order moments can also be included as part of the k-plus objective. Futhermore, k-plus anticlustering can also be used to obtain similar covariance structures among groups. This approach may be of interest in cross validation applications where prediction accuracy depends on the correlations among features and criteria [@zeng2000; @papenberg2020]. Interestingly, in each case, the k-plus criterion can be reduced to an augmentation of the original data matrix, leaving the original k-means objective unchanged. 

After mathematically introducing the k-plus objective, a simulation study is reported showing that k-plus anticlustering is well suited to create groups that have minimal differences according to multiple distribution characteristics. Fulfilling multiple criteria at the same time becomes more feasible as the size of the data set increases. In two pratical examples I illustrate how readers can easily apply k-plus anticlustering using the free and open source software package `anticlust`, an extension to the popular statistical programming language R [@papenberg2020; @R-base; @R-anticlust].

# Problem formalization

For the purpose of problem formalization, I adopt the notation @steinley2006 provided in his comprehensive synthesis of half a century of research on k-means. K-means clustering (and anticlustering) is used to partition two-way, two-mode data into groups. That is, $N$ elements each having $P$ attributes are assigned to $K$ groups $(C_1., \ldots, C_K)$, $C_k$ being the set of $n_k$ objects in group $k$. The partitioning is subject to the requirement that each element is assigned to exactly one group, formalized by the following two constraints [@papenberg2020]:

$$
\bigcup\limits_{k = 1}^{K} C_j = X
$$

$$
C_j \cap C_k = \emptyset, \; \forall j, k \in \{1, \ldots, K\}, \; j \ne k
$$

In anticlustering applications, we impose an additional constraint on the group sizes $n_k$, where the most common restriction requires that all groups have equal size:

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

For a data matrix $\mathbf{X}_{N \times P} = \{x_{ij}\}_{N \times P}$, k-means anticlustering aims to maximize the error sum of squares ($\mathit{SSE}$), which is the sum of the squared Euclidean distances between each data point and its cluster centroid:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

The *k*'th centroid $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_P^{(k)})$ is a vector of length $P$, where each entry is the mean value of one of the $P$ attributes, computed across all observations belonging the $k$th cluster $C_k$ ($k = 1, \ldots, K$):

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}
$$

Note there is a direct connection between the $\mathit{SSE}$ and the location of the cluster centroids [@spath1986]. If the $\mathit{SSE}$ is minimal---which is the goal of k-means *clustering*---, the centroid values $\overline{\mathbf{x}}^{(k)} (k = 1, \ldots, K)$ are as far away as possible from the overall data centroid $\overline{\mathbf{x}} = (\overline{\mathbf{x}}_1, \overline{\mathbf{x}}_2, \ldots, \overline{\mathbf{x}}_P$), where $\overline{\mathbf{x}}_p$ is the overall mean value on the $p$th attribute:

$$
\overline{\mathbf{x}}_p = \frac{1}{N} \sum\limits_{i=1}^{N} x_{ip}
$$

If the $\mathit{SSE}$ is maximal---which is the goal of k-means *anticlustering*---, the cluster centroids are as close as possible to the overall centroid $\overline{\mathbf{x}}$ and therefore to each other [@spath1986]. Thus, k-means anticlustering directly optimizes the similarity of the mean attribute values across clusters.

## Quantifying differences in variance

In the following, I present a variation of the $\mathit{SSE}$ that quantifies how the variance of numeric data differs between groups. To motivate the new criterion, we first consider a set of one-dimensional observations $(x_1, \ldots, x_N)$. The extension to the general case of $P$ attributes will be straight forward.

The sample variance of a set of observations $x = (x_1, \ldots, x_N)$ is computed as the mean of the squared distances between observations and their mean $\overline{\mathbf{x}}$:

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^2
$$

When defining a new variable $y = (y_1, \ldots, y_N)$ as the squared deviation of each observation from the mean $\overline{\mathbf{x}}$, the sample variance of $x$ can be reformulated as the mean of $y$:

$$
y_i = (x_i - \overline{\mathbf{x}})^2
$$

$$
\mathit{Var(x)} = \frac{1}{N} \sum\limits_{i=1}^{N} y_i
$$

Because the sample variance is basically defined as an average, k-means anticlustering can be employed to equalize the variance across groups. That is, we can formulate an adaption of the $\mathit{SSE}$ to quantify the degree to which the variance differs between groups. First, however, we generalize the idea to the case of $P$ attributes. For each attribute, we have to compute the squared deviation between all values and the mean of the attribute. We obtain a new data matrix $\mathbf{Y}_{N \times P} = \{y_{ij}\}_{N \times P}$, where

$$
y_{ij} = (x_{ij} - \overline{\mathbf{x}}_j)^2.
$$

To reflect how strongly the variances of all attributes vary across the $K$ clusters, we now define the criterion $\mathit{SSE_{Var}}$:

$$
\mathit{SSE_{Var}} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

Analogously to the standard $\mathit{SSE}$, the centroid values $\overline{y}_j ^{(k)}$ are given as

$$
\overline{y}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} y_{ij}.
$$

Note that $\mathit{SSE_{Var}}$ has the same form as the standard $\mathit{SSE}$. However, the input data reflects the squared difference between each data point and the attribute mean instead of the raw data points.

## A bicriterion model: K-plus anticlustering {#kplus}

It is unlikely that an exclusive optimization of $\mathit{SSE_{Var}}$ (i.e., equalizing variances) is of interest without also considering the $\mathit{SSE}$ (i.e., equalizing means). A combination of $\mathit{SSE_{Var}}$ and $\mathit{SSE}$, however, can be employed to simultaneously optimize both objectives, which is a reasonable idea when striving for overall between-group similarity. The most simple approach for such a bicriterion optimization is the weighted sum approach [@naidu2014; @marler2010]. That is, both criteria are computed independently from each other and then combined into a single criterion through a weighted sum:

$$
w_1 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2 + 
w_2 \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (y_{ij} - \overline{y}_j ^{(k)})^2
$$

The weights $w_1$ and $w_2$ can be chosen to adjust the relative importance of equalizing means and variances, respectively. Some simplifications can be arranged when ignoring the weights $w_1$ and $w_2$, e.g., by setting them both to 1. When defining $z_i = (x_{i1}, \ldots, x_{iP}, y_{i1}, \ldots, x_{iP})$, i.e., the concatenation of the $x_{ij}$ and $y_{ij}$ values, the bicriterion $\mathit{SSE_{kplus}}$ can be computed as

$$
\mathit{SSE_{kplus}} =
  \sum\limits_{j=1}^{2 P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (z_{ij} - \overline{z}_j ^{(k)})^2
$$.

This formula again has the same form as $\mathit{SSE}$ (and $\mathit{SSE_{Var}}$), but is computed on different data. In this case, both the original data $x_{ij}$ as well as the squared distances $y_{ij}$ are used as attributes that enter the objective function. That is, computing the k-plus criterion can be done by first computing the values $y_{ij}$, appending these to the original data matrix, and then computing the $\mathit{SSE}$ on the augmented data matrix. Hence, k-plus anticlustering reduces to an augmentation of the data matrix, leaving the original k-means criterion unchanged. 

During the remainder of this paper, whenever the k-plus criterion is optimized, the original data matrix is simply extended and the original k-means criterion $SSE$ is optimized. For a multicriterion objective such as the k-plus criterion, this corresponds to a rather basic approach, and future research may investigate more sophisticated optimization schemes for k-plus anticlustering [e.g., see @brusco2012; @brusco2019]. In this paper, however, I am primarily concerned with investigating the properties of new objective itself rather than extending the state of the art on multicriterion optimization methods. 

Note that a problem may arise if the weights $w_1$ and $w_2$ are ignored when optimizing the k-plus objective, i.e., when just appending the squared-distance-features to the data set. Since the computation of the data matrix $\mathbf{Y}_{N \times P}$ involves squaring, the squared distances $y_{ij}$ have much larger values, on average, than the raw values $x_{ij}$. Therefore, the $\mathit{SSE_{Var}}$ criterion naturally receives greater weight when computing $\mathit{SSE_{kplus}}$ than the standard $\mathit{SSE}$ criterion. This imbalance is most likely unwarranted because optimizing similarity with regard to variances is not more important than optimizing similarity with regard to means. A solution is to standardize all values $x_{ij}$ and $y_{ij}$ to the same scale before computing $SSE_{kplus}$, e.g. via $z$-standardization.

## Skewness, kurtosis, and higher order moments

The outlined logic to equalize the variance, which is the second moment of a distribution, can be generalized to higher order moments in a straight forward manner. The *j*'th sample moment of a variable $x = (x_1, \ldots, x_N)$ can be computed as 

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}})^j.
$$

where $j = 2$ gives the variance, $j = 3$ the skewness, and $j = 4$ the kurtosis [@joanes1998]. Thus, by changing $j$---the power of the deviation between mean and data points---we can use the k-plus logic to equalize any desired sample moment between groups. For each moment $j$ and for each variable $x$, this is accomplished by adding a new variable $y^{(j)} = (y^{(j)}_1, \ldots, y^{(j)}_N)$ with $y^{(j)}_i = (x_i - \overline{\mathbf{x}})^j$ to the data set and subsequently applying standard k-means anticlustering. I expect this to be particularly interesting for the skewness and kurtosis when aiming to minimize differences with regard to distribution asymmetry (skewness) and the propensity to include outliers (kurtosis).

## Covariances

Because the covariance is also defined as an expected value, the k-plus criterion can be extended to reflect differences in covariances between data sets. The covariance between two variables $x$ and $y$ is defined as

$$
\frac{1}{N} \sum\limits_{i=1}^{N} (x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}}).
$$

Thus, to maximize between-groups similarity with regard to covariance structure, it is possible to add an additional variable for each pair of variables $x$ and $y$, computed as $(x_i - \overline{\mathbf{x}}) (y_i -\overline{\mathbf{y}})$. This requires computing $\binom{N}{2}$ new variables and thus turns out be be rather costly: The number of additional variables needed to minimize differences with regard to covariance structure grows quadratically with increasing number of variables; the number of additional variables needed to minimize differences with regard to distribution moments only grows linearly with increasing number of variables.

# Simulation study

To thorougly evaluate k-plus approach to anticlustering, three specific k-plus criteria were implemented in a simulation study: (i) "standard" k-plus anticlustering, minimizing differences with regard to the mean and variance; (ii) k-plus *skew/kurtosis*, minimizing differences with regard to the mean, variance, skew and kurtosis; (iii) k-plus *correlation*, minimizing differences with regard to the mean, variance and covariance structure. 

The three k-plus methods were compared to (i) k-means anticlustering, minimizing differences with regard to group means; (ii) diversity anticlustering, maximizing the sum of Euclidean distances within groups; (c) a simple random allocation of elements to groups. All anticlustering methods used a $z$-standardization of the data sets as a preprocessing step. In the case of k-plus anticlustering, the augmented input data---i.e., the original data as well as the features that were added to incorporate the additional optimization criteria---were subjected to a standardization, which is recommended.

The simulation was implemented using the statistical programming language R [Version 4.2; @R-base]. The R package `anticlust` [Version 0.6.1; @R-anticlust; @papenberg2020] was used to optimize the anticlustering objectives. The R package `faux` [Version 1.1.0; @R-faux] was used to generate the multivariate normal data sets that were processed during the simulation. The R packages `dplyr` [Version 1.0.7; @R-dplyr], `ggplot2` [Version 3.3.5; @R-ggplot2], `papaja` [Version 0.1.0.9997; @R-papaja], `tidyr` [Version 1.1.3; @R-tidyr] and `DescTools` [Version 0.99.45; @R-desctools] were used for data analysis and result presentation. The simulation study is fully reproducible via code and data that is accessible from the accompanying OSF repository (**link TODO**).

## Optimization algorithm

The same local maximum search was applied to optimize each of the five competing anticlustering objectives [Method LCW in @weitz1998]. Building on an initial random assignment of elements to groups, the algorithm proceeds by swapping elements between groups in such a way that each swap improves the objective criterion by the largest possible margin. That is, for each element, all possible exchanges are simulated and the resulting objective values are stored. After each exchange has been tested, the one exchange is realized that improves the objective the most. No exchange is realized if no improvement is possible. The exchange procedure is repeated for each element. Once the last element has been processed, the procedure restarts at the beginning and once again iterates through all elements, repeating the exchange procedure. The procedure stops as soon as an iteration through the entire data set no longer yielded an improvement, i.e., as soon as a local maximum has been found. To obtain better results, this local maximum search may be initialized multiple times [@spath1986]. The simulation employed five repetitions of the local maximum search.

## Conditions

First, I generated 10,000 data sets following a normal distribution. The data sets were subsequently processed by the competing methods. The following properties were determined randomly for each data set: (a) the sample size $N$ varied between 24 and 300 with intermediate steps of 12 so that each data set could be split evenly into $K = 2$, $3$, and $4$ groups; (b) the number of features varied between 2 and 5; (c) the standard deviation of all features was set to 1, 2 or 3 (the mean was always zero); (d) the correlation between all features was $r = 0$, $r = .1$, $r = .2$, $r = .3$, $r = .4$ or $r = .5$. All anticlustering methods were applied to each of the 10,000 data sets using $K = 2$, $K = 3$ and $K = 4$ (i.e., each method was applied 30,000 times). Groups sizes were always equal. 

## Evaluation criteria

After the five anticlustering methods and the random allocation procedure had been applied to the 10,000 data sets for $K = 2$, $K = 3$, $K = 4$, I investigated how well the competing methods were able to create similar groups. To this end, I investigated the discrepancy in means, standard deviations, skewness, kurtosis, and correlation between groups---for each data set for each $K$. To quantify the discrepancy in group means, the following computation was used: for each of the (2, 3, 4 or 5) features, all (2, 3 or 4) group means were computed. For each feature, the difference between the minimum and maximum group mean was used as a measure of discrepancy; the global discrepancy in means ($\Delta M$) was then determined as the average discrepancy across features. The same procedure was applied to compute the discrepancy in standard deviations ($\Delta \mathit{SD}$), skewness ($\Delta \mathit{Skew}$) and kurtosis ($\Delta \mathit{Kurtosis}$). Quantifying discrepancy in correlations ($\Delta \mathit{Cor}$) followed the same rule, but discrepancies in correlations between groups had to be aggregated across pairs of features instead of single features.

## Results

```{r Table1}

# Load required packages
library(dplyr)
library(ggplot2)
library(tidyr)
source("./Simulation_Study/0-functions-generate-data.R")

## Analyze data for K = 2 and K = 3 and K = 4
simulation_results <- list()
for (K in 2:4) {
  filename <- paste0("./Simulation_Study/results-K", K, "-objectives-raw.csv")
  df <- read.csv(filename, sep = ";", stringsAsFactors = FALSE)
  df$K <- K
  simulation_results[[paste0("K-", K)]] <- df
}

df <- do.call(rbind, simulation_results)
rownames(df) <- NULL

# Make long format
ldf <- pivot_longer(
  df,
  cols = paste0(c("means", "sd", "skew", "kur", "cor"), "_obj"),
  names_to = "Objective",
  names_pattern = "(.*)_obj"
)

## Global results, aggregated across all simulation variables
tab <- ldf %>% 
  group_by(method, Objective) %>% 
  summarise(Mean = round(mean(value), 3)) %>% 
  pivot_wider(names_from = Objective, values_from = Mean) %>% 
  select(c(means, sd, skew, kur, cor))

colnames(tab) <- c("",
  "$\\Delta M$", 
  "$\\Delta \\mathit{SD}$",
  "$\\Delta \\mathit{Skew}$",
  "$\\Delta \\mathit{Kurtosis}$",
  "$\\Delta \\mathit{Cor}$"
)
  
apa_table(
  tab,
  caption = "Results of the simulation study",
  note = "The global results of the simulation study aggregated across all conditions. Cells contain information about the average between-group discrepancy with regard to means, standard deviations, skewness, kurtosis, and correlations. Lower values indicate less discrepancy, i.e., higher between-group similarity. Each cell value is the result of averaging across 30,000 data points (10,000 data sets $\\times$ $K = 2$, $K = 3$ or $K = 4$).",
  escape = FALSE,
  align = c("r")
)

```

```{r Figure1, fig.width = 8, fig.height = 10, fig.cap = "Simulation results by $N$."}

# Plot the results, by N
ldf %>% 
  group_by(method, Objective, N) %>% 
  summarise(Mean = mean(value)) %>% 
  filter(method != "random") %>% 
  mutate(
    Objective = ordered(
      Objective, 
      levels = c("means", "sd", "skew", "kur", "cor"),
      labels = c("M", "SD", "Skew", "Kurtosis", "Correlation"))
  ) %>%
  ggplot(aes(x = N, y = Mean, colour = method)) + 
  geom_line(aes(linetype = method), size = .85) + 
  facet_grid(rows = vars(Objective), scales = "free") + 
  ylab("Mean discrepancy") +
  theme_bw(base_size = 16)

```

Table 1 displays the global simulation results aggregated across all conditions. Standard k-means performed best at minimizing discrepancy with regard to means, but disregards all other distribution characteristics. K-plus anticlustering also addresses $\Delta \mathit{M}$ quite well and at the same time tends to minimize discrepancy with regard to the standard deviations ($\Delta \mathit{SD}$). With regard to minimizing $\Delta \mathit{SD}$, all three k-plus approaches outperform all other methods. As Figure 1 shows, with increasing $N$, k-means and all k-plus methods converge on minimizing $\Delta \mathit{M}$. This is an important observation because it shows that including the k-plus term in the optimization process does not necessarily prevent minimizing discrepancy in means; instead, multiple objectives can be addressed at the same time.

While k-plus anticlustering is capable of addressing multiple objectives simultaneously, it is apparent that increasing the number of optimization criteria aggravates fulfilling each single criterion. K-plus skew/kurtosis and k-plus correlation are worse at minimizing discrepancy with regard to standard deviations than standard k-plus, which is slightly worse at minimizing $\Delta \mathit{M}$ than k-means anticlustering. Still, every k-plus method does what it is supposed to do: k-plus skew/kurtosis is best at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$, k-plus correlation is best at minimizing discrepancy with regard to $\Delta \mathit{Cor}$. At the same time, these two k-plus objectives maintain a comparably good level of addressing $\Delta \mathit{M}$ and $\Delta \mathit{SD}$, outperforming diversity anticlustering. 

It is maybe surprising that k-plus skew/kurtosis does not seem to be much better than diversity anticlustering with regard to skew and---in particular---kurtosis, even though the method has been specifically tailored to these criteria. Figure 1 gives a more fine grained assessment of this observation by splitting the results by $N$: Surprisingly, for small $N$, k-plus skew/kurtosis is even worse than diversity anticlustering at minimizing $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. However, with increasing $N$, it clearly outperforms diversity anticlustering. These results illustrate the cost of optimizing several criteria at the same time: for low $N$, k-plus seems skew/kurtosis to be preoccupied with fulfilling the objectives $\Delta \mathit{M}$ and $\Delta \mathit{SD}$. Increasing $N$ then facilitates to also address $\Delta \mathit{Skew}$ and $\Delta \mathit{Kurtosis}$. 

Generally, it should be noted that diversity anticlustering is a good all-arounder method that tends to somewhat address all distribution characteristics. For the specific objectives that are addressed by the k-plus methods, however, diversity anticlustering is outperformed. As as side note, apart from k-plus correlation, diversity anticlustering is the only method that equalizes correlation structure among groups. As far as I know, this observation has not been made previously, even though the maximum diversity problem has been widely studied. 

# K-plus anticlustering using the R package anticlust 

This section demonstrates how researchers and other data analysists can easily employ k-plus anticlustering using the R package `anticlust` [@papenberg2020], using an interface function called `kplus_anticlustering()`, which is available from version 0.6.3 onward. The package can be installed in the R environment using the `install.packages()` command:

```R
install.packages("anticlust")
```

## Example Application I

In the first example, I use norming data for the OASIS image data set that is freely available online [@kurdi2017introducing]. @kurdi2017introducing assembled 900 open-access color images and collected ratings on two affective dimensions: arousal and valence. @brielmann2019intense collected an additional rating dimension by measuring how the same 900 images were rated with regard to their beauty. In my application, I use all three features, currently available from Github.[^githubbeauty]

[^githubbeauty]: https://github.com/aenneb/OASIS-beauty

I used k-plus anticlustering to divide the data set of 900 images into 9 groups of 100 images each; this process takes about 3-5 seconds on a contemporary personal computer, depending on how many iterations are needed to find the local maximum. The task was accomplished using the following R code:

```R
library(anticlust) # load the package anticlust

kplus_anticlustering(
  features,
  K = 9,
  variance = TRUE,
  skew = TRUE,
  kurtosis = FALSE,
  covariances = FALSE,
  moments = NULL,
  method = "local-maximum",
  standardize = TRUE
)
```

Here, the variable `features` has to be a data table containing the image data: 900 rows representing images; 3 columns representing the features beauty, arousal, and valence. The accompanying OSF repository contains the fully reproducible code that includes downloading and reading the OASIS data, selecting the relevant columns, and finally storing them in the `features` variable (**TODO**). The `kplus_anticlustering()` function has four boolean arguments to specify whether variance, skewness, kurtosis and covariances should be included as part of the k-plus criterion. Only the argument `variance` is set to `TRUE` by default; the other arguments have to be "turned on". If other higher order moments should be included as part of the optimization, the optional argument `moments` can be used to specify the desired moments as an integer vector.

The argument `method = "local-maximum"` ensures that the k-plus criterion is optimized using the local maximum search method described in the previous section. The argument `K` describes the number of groups. By default, the function ensures that all features are standardized before the optimization starts; in particular, this option enforces that all k-plus criteria receive the same weight during the optimization process (this behaviour can be adjusted using the boolean argument `standardize`). 

Table 2 shows the results of the anticlustering application by listing the descriptive statistics (means and standard deviations) for each of the 9 groups and for each of the 3 features. Additionaly, Table 2 shows the descriptive statistics for an application of k-means anticlustering and anticluster editing. It is shown that---up to two decimals---k-plus anticlustering perfectly matched all groups with regard to the features' means and standard deviations. K-means anticlustering also perfectly matched the mean values, but showed decreased performance with regard to similarity in standard deviations. Anticluster editing was also well-suited to match both means and standard deviations, but was slightly outperformed by k-plus. The latter result is not surprising because---unlike k-plus anticlustering---anticluster editing does not directly maximize similarity with regard to the means and standard deviations, but instead maximizes within-group heterogeneity. 

```{r, eval = FALSE}

path <- "results_oasis_same_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))
  
  K <- 9
  for (criterion in c("kplus", "variance", "diversity")) {
    if (criterion == "kplus") {
        oasis[[paste(criterion, "group", sep = "_")]] <- kplus_anticlustering(
            features,
            K = K,
            variance = TRUE,
            skew = TRUE,
            kurtosis = FALSE,
            covariances = FALSE,
            moments = NULL,
            method = "local-maximum",
            standardize = TRUE
        )
    } else {
        oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
            features,
            standardize = TRUE,
            K = K,
            objective = criterion,
            method = "local-maximum"
        )
    }

  }
  
    descriptive_by_group <- function(features, anticlusters, FUN, name) {
    df <- data.frame(
      t(data.frame(lapply(by(features, anticlusters, FUN), c)))
    )
    df$group <- 1:nrow(df)
    df$Descriptive = name
    df
  }
  
  
  descriptive_by_group_all <- function(features, anticlusters) {
    means <- descriptive_by_group(features, anticlusters, colMeans, "mean")
    sds <- descriptive_by_group(
      features, 
      anticlusters, 
      function(x) sapply(x, sd),
      "sds"
    )
    skew <- descriptive_by_group(
      features, 
      anticlusters, 
      function(x) sapply(x, DescTools::Skew),
      "skew"
    )
    rbind(means, sds, skew)
  }


kplus <- descriptive_by_group_all(features, oasis$kplus_group)
kplus <- data.frame(kplus, Objective = "kplus")

kmeans <- descriptive_by_group_all(features, oasis$variance_group)
kmeans <- data.frame(kmeans, Objective = "kmeans")
diversity <- descriptive_by_group_all(features, oasis$variance_group)
diversity <- data.frame(diversity, Objective = "diversity")

all_objs <- rbind(kplus, kmeans, diversity) %>% 
  pivot_wider(names_from = Descriptive, values_from = c(beauty_mean, Valence_mean, Arousal_mean))
  
  
  # append all tables to the same table:
  rownames(all_objs) <- NULL
  colnames(all_objs) <- c("Group", "N", "Beauty", "Valence", "Arousal")


} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Descriptive statistics for OASIS features by group and anticlustering method.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:9,
                      "\\textbf{K-means anticlustering}" = 10:18,
                      "\\textbf{Anticluster editing}" = 19:27)
)
```

Table 3 illustrates another advantage of k-plus anticlustering over diversity anticlustering: When creating groups of unequal size, diversity anticlustering tends to increase the spread of the data in the largest group in comparison to the other groups. K-plus anticlustering strives for equal means and variances in all groups, regardless of group size. Unequal group sizes can be requested in anticlust by passing the different group sizes to the `K` argument:

```R
anticlustering(
  features,
  K = c(100, 100, 100, 600),
  standardize = TRUE,
  objective = "kplus",
  method = "local-maximum"
)
```

```{r}

path <- "results_oasis_unequal_size.csv"

if (!file.exists(path)) {
  oasis <- read.csv(
    "https://raw.githubusercontent.com/aenneb/OASIS-beauty/master/means_per_image.csv"
  )

  features <- subset(oasis, select = c(beauty_mean, Valence_mean, Arousal_mean))

  K <- c(100, 100, 100, 600)
  mean_sd_tabs <- list()
  for (criterion in c("kplus", "variance", "diversity")) {
    oasis[[paste(criterion, "group", sep = "_")]] <- anticlustering(
      features,
      standardize = TRUE,
      K = K,
      objective = criterion,
      method = "local-maximum"
    )
    mean_sd_tabs[[criterion]] <- mean_sd_tab(
      features,
      oasis[[paste(criterion, "group", sep = "_")]]
    )
    mean_sd_tabs[[criterion]] <- cbind(1:length(K), K, mean_sd_tabs[[criterion]])
  }

  # append all tables to the same table:
  oasis_tab <- do.call(rbind, mean_sd_tabs)
  rownames(oasis_tab) <- NULL
  colnames(oasis_tab) <- c("Group", "N", "Beauty", "Valence", "Arousal")

  write.table(
    oasis_tab, path, row.names = FALSE, sep = ";"
  )

} else {
  oasis_tab <- read.csv(path, sep = ";")
}

apa_table(
  oasis_tab,
  caption = "Performance of the diversity and k-plus objectives for unequal group sizes.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r"),
  stub_indents = list("\\textbf{K-plus anticlustering}" = 1:4,
                      "\\textbf{K-means anticlustering}" = 5:8,
                      "\\textbf{Anticluster editing}" = 9:12)
)
```

## Example Application II: Small data set ($N = 96$)

The simulation study indicated that k-plus may show decreased performance for smaller group sizes. Therefore, a second application illustrates that k-plus anticlustering yields satisfactory results in practice even when group sizes are smaller. I make use of a norming data set of 96 word stimuli that was contributed to the `anticlust` package by Dr. Schaper [@schaper2019metacognitive; @schaper2019metamory; also see @papenberg2020 for a description of the norming data]. After loading the `anticlust` package, the data set can be accessed as follows: 

```{r, echo = TRUE}
data(schaper2019)
```

Table 4 illustrates the results that are obtained when using k-plus anticlustering to divide the stimulus set into 6 groups of size `r nrow(schaper2019) / 6` each. Even though the means and standard deviations are no longer perfectly matched between groups, arguably they are still similar enough for any practical purpose---especially when considering inevitable measurement error in norming studies. The following code was used to achieve the results reported in Table 4:

```{r, echo = TRUE}
features <- schaper2019[, 3:6]
anticlusters <- anticlustering(
  features,
  K = 6,
  objective = "kplus",
  method = "local-maximum",
  repetitions = 5,
  standardize = TRUE
)
```

```{r}

tab <- mean_sd_tab(features, anticlusters)
tab <- cbind(1:6, 96 / 6, tab)
colnames(tab) <- c("Group", "N", "Typicality", "Atypicality", "Syllables", "Frequency")

apa_table(
  tab,
  caption = "Descriptive statistics by group for the Schaper et al. (2019a; 2019b) data set, after applying k-plus anticlustering.",
  note = "The cells contain mean values by cluster and the standard deviation (in parentheses).",
  escape = FALSE,
  align = c("r")
)

```

# Discussion

In this paper, I presented the k-plus criterion for anticlustering. K-plus anticlustering focuses on dividing a data set into subgroups in such a way that these groups are similar to each other. The k-plus criterion extends the classical k-means criterion, which only represents how similar groups are with regard to their means. The k-plus family may be used to equalize the variance as well, any higher order moments such as skewness and kurtosis, as well as covariances. Interestingly, k-plus basically does not change the original k-means criterion because it can be reduced to an augmentation of the input data. A simulation study and examples on real norming data indicated that the k-plus criterion is well suited to maximize between-group similarity with regard to multiple criteria. 

This work indicates that k-plus anticlustering should be the default method as a replacement of k-means anticlustering when it comes to creating similar groups, given that similarity in means and variances can usually be achieved succesfully at the same time. Depending on a user's needs, equalizing higher order moments can also be requested, but it should be noted that fulfilling several criteria at the same time may be difficult to achieve for small data sets. In any way, a user can just try out different objectives, i.e., start greedy by requesting that multiple distribution moments are equalized, and if the results for the means and standard deviations are subpar, try out fewer criteria. K-plus anticlustering was also shown to outperform diversity anticlustering with regard to the specific objectives that are included in the optimization process; diversity is still a reasonable all-arounder that strives for overall betwee-group similarity. Which method should be employed depends on a user's preferences. In doubt, several anticlustering methods can be applied sequentially and the results can be compared directly. Unlike in statistical methods, there is no problem with trying out different approaches when it comes to anticlustering.



## Limitations and outlook

- This paper focused on the presentation of new objective function, the bicriterion optimization process might be improved [e.g., by using a direct algorithm as @brusco2019 did]. From the algorithmic point of view, this paper did not strive to apply the latest cutting edge methodology. For most practical purposes, however, I assume that optimizing the k-plus criterion will yield very satisfactory results for researchers. I have already worked with researchers planning their experiments who are generally amazed by the groupd that are returned by k-plus anticlustering. 

Moreover, it was of interest to investigate the value of the k-plus objective function per se, independently from the algorithm that is used to optimize it. Hence, to obtain a fair assessment, it was important in the simulation study that each anticlustering objective is optimized using the same procedure, instead of using specialized multicriterion optimization schemes. It was of particular interest to compare the k-plus objective with the popular diversity objective. @papenberg2020 compared k-means anticlustering and diversity anticlustering[^clusterediting] in a simulation study and concluded that the diversity criterion should usually be preferred because k-means does not equalize the spread of the distribution between groups, whereas the diversity maintains an appropriate balance between location and spread. Using the k-plus extension, however, the k-means criterion regains a lot of attractivity: when optimizing for similarity in means and variance, k-plus anticlustering clearly outperformed the diversity criterion in the simulation study. It should also be noted that the k-plus criterion should almost always be preferred to standard k-means. This is because even for low $N$, k-plus equalizes means quite well, and for larger $N$ the practical differences between k-means and k-plus vanish completely. A reasonable exception is given when the data set consists of binary variables where mean and standard deviation are directly related. 

[^clusterediting]: In their study, they used the term anticluster editing to refer to the maximization of the diversity criterion. The diversity criterion is the objective function used in the clustering method cluster editing, where it has to be minimized [@zahn1964; @shamir2004vf].

## Future research

- Future research: Application of more sophisticated bicriterion optimization algorithms are interesting

- extending k-means anticlustering on maximizing similarity with regard to covariances? Mahalanobis distance?

# General thoughts on anticlustering

- new criterion: optimizes between-group similarity, no correspondence to within-group heterogeneity [see @brusco2019 for a exceptional method maximizing within-group heterogeneity]

- Similarly: the reversed objective no longer has a useful clustering interpretation: Maximizing differences in variances does not lead to homogeneous and/or well separated clusters


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
