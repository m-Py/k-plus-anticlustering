---
title             : "An extension of k-means anticlustering to optimize between-group similarity"
shorttitle        : "K-means anticlustering extended"

author:
  - name          : "Martin Papenberg"
    affiliation   : "1"
    corresponding : yes
    address       : "Heinrich Heine Universität Düsseldorf,  Institut für Experimentelle Psychologie, Universitätsstraße 1, 40225 Düsseldorf, Germany"
    email         : "martin.papenberg@hhu.de"

affiliation:
  - id            : "1"
    institution   : "Heinrich Heine University Düsseldorf"

authornote: |

  Martin Papenberg, Department of Experimental Psychology, Heinrich 
  Heine University Düsseldorf.  


abstract: |
  Anticlustering is an automated method to divide a data set into groups
  in such a way that high between-group similarity and high within-group 
  heterogeneity is obtained. Anticlustering is accomplished by maximizing 
  instead of minimizing a clustering objective. 
  
bibliography      : ["lit.bib"]
  
keywords          : "K-Means, Anticlustering, Variance"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("anticlust")
library("knitr")
options(digits = 2)
knitr::opts_chunk$set(message=FALSE, warning = FALSE, echo = FALSE) 
```

Anticlustering is a method to partition a set of elements into different 
groups with the focus of obtaining high between-group similarity and 
within-group heterogeneity. Anticlustering reverses the logic of its 
better known twin, cluster analysis that seeks homogeneity within 
clusters and heterogeneity between clusters [@rokach2005; 
@steinley2006]. The method of anticlustering is useful in many 
tasks surrounding the work of researchers in psychology, for example, 
when splitting a test into parts of equal difficulty [@gierl2017], when 
assigning students to work groups [@baker2002], or when assigning 
stimuli to experimental conditions [@lahl2006]. Solving such problems 
"by hand" is a tedious and time-consuming task and the quality of 
manually established groups is usually improvable. Fortunately, these 
problems can be formalized mathematically via several anticlustering 
objective functions [e.g., @brusco2019; @spath1986; @baker2002; 
@fernandez2013], and accessible open source software solutions exist 
[@papenberg2020].

Several anticlustering objectives are available to assess between-group 
similarity or within-group heterogeneity. In many cases, both objectives 
are directly related, that is, maximizing heterogeneity within groups 
simultaneously maximizes between-group similarity. Several 
anticlustering objectives are based pairwise dissimilarity ratings; the 
most prominent criterion of this kind is the *diversity*, which is the 
total sum of dissimilarities between any elements within the same group. 
Note that maximizing the diversity is equivalent to minimizing 
the sum of dissimilarities between elements in different clusters, 
directly illustrating the correspondence between within-group 
heterogeneity and between-group similarity. Another important criterion 
based on pairwise dissimilarities is the dispersion, which is the 
minimum dissimilarity between any two items in the same group 
[@fernandez2013; @brusco2019]. Maximizing the dispersion increases the 
within-group heterogeneity by ensuring that any two elements as 
dissimilar as possible from each other. Note that maximizing the 
dispersion no longer simultaneously maximize between-group similarity. 
In this case, these two potential aims of anticlustering diverge.

Oftentimes, the available data does not represent pairwise 
similarity, but rather numeric, categorical or binary attributes 
[@dry2009]. That is, the data is a two-way table where rows represent 
elements and columns represent variables describing the elements. For 
example, numeric ratings may describe the attributes of word stimuli for 
psycholinguistic experiments. Such data may include Norms for imagery, 
concreteness and orthographic variables [@friendly1982]. In other cases, 
attributes are binary and may represent the presence or absence of 
a feature [@tversky1977]. Attribute data can be converted to pairwise 
dissimilarities for example by computing an appropriate dissimilarity 
measure, such as the pairwise Euclidean or squared Euclidean distance 
across a set of numeric attributes [@brusco2019]. Then, anticlustering 
criteria based on pairwise dissimilarity can be applied to partition the 
elements. 

A different approach to anticlustering directly works with feature based 
data and does not require a transformation into pairwise 
dissimilarities: k-means anticlustering. Späth [-@spath1986] and Valev 
[-@valev1998; -@valev1983] independently recognized that by optimizing 
the reversed objective function of the popular k-means algorithm 
[@jain2010; @steinley2006], a data set is partitioned into similar 
groups. They originally---and independently from each other---coined the 
term anticlustering for this purpose. K-means anticlustering maximizes the 
sum of squared Euclidean distances from each element to the center of 
the cluster to which it belongs [cf. @aloise2009np]. As @spath1986 
noted, k-means anticlustering thereby directly minimizes differences 
with regard to the means of the numeric variables between groups. Hence, 
groups only become similar with regard to the mean of the distribution 
of the numeric attributes, which may perform poorly as a measure of 
overall similarity; the spread of the data is not targeted and may 
differ between sets. @papenberg2020 showed that maximizing the 
diversity (based on the pairwise Euclidean distances) is better suited 
to minimize difference with regard to both the mean and the variance of 
the data; it was shown that even an entirely random split usually 
resulted in more similar standard deviations as k-means solely focuses 
on the mean. When requiring that groups are overall similar to each 
other, this focus may be detrimental as similar means can be obtained 
even when the underlying distributions are very different 
[@anscombe1973]. 

In this paper, I present a simple extension of the k-means objective 
function that ensures when maximizing the criterion, not only the means
but also the variances become similar between groups. Hence, this paper 
is more concerned with the goal of forming similar groups, rather than 
maximizing within-group heterogeneity. 

## A Motivating Example

```{r}

set.seed(4318) # 4318

N <- 14
K <- 2
partitions <- generate_partitions(N, K)
features <- round(rnorm(N, 15, 3), 1)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
diff_mean <- function(clusters, features) {
  abs(diff(tapply(features, clusters, mean)))
}

diff_vars <- function(clusters, features) {
  abs(diff(tapply(features, clusters, var)))
}

all_objectives_mean <- sapply(
  partitions,
  FUN = diff_mean,
  features = features
)

all_objectives_var <- sapply(
  partitions,
  FUN = diff_vars,
  features = features
)

df <- data.frame(
  diff_mean = all_objectives_mean,
  diff_var  = all_objectives_var
)

plot(df, pch = 4, col = "darkgrey", las = 1)
# Illustrate best partition wrt simililarity in means
points(
  all_objectives_mean[which.min(all_objectives_mean)],
  all_objectives_var[which.min(all_objectives_mean)],
  cex = 2, col = "#DF536B", pch = 16
)

df$sum <- rowSums(df)

points(
  all_objectives_mean[which.min(df$sum)],
  all_objectives_var[which.min(df$sum)],
  cex = 2, col = "#2297E6", pch = 16
)

points(
  all_objectives_mean[which.min(df$diff_var)],
  all_objectives_var[which.min(df$diff_var)],
  cex = 2, col = "#61D04F", pch = 16
)

```

An example will show that aiming not only for similarity in attribute 
means is not only useful, but also possible. I created 14 data points 
from a normal distribution ($M = 15$, $SD = 3$, see Table 1). The data 
points in Figure 1 illustrate all `r length(partitions)`  ways to 
partition a data set of $N = 14$ into two parts (the Appendix contains 
an explanation how to compute the number of partitions and how to 
generate them). Red point: maximum similarity in means; green point 
= maximum similarity in variance. Blue point: Almost no difference to 
red point wrt similarity in means, but variance is much more similar. 
This blue partition is certainly preferable to the red one when maximal 
overall similarity is required; the red one just optimizes similarity in 
means (which is what maximizing the k-means objective does). 


```{r datatables}

best_partition_mean <- partitions[[which.min(df$diff_mean)]]
best_partition_var  <- partitions[[which.min(df$diff_var)]]
best_partition_combined <- partitions[[which.min(df$sum)]]

data <- data.frame(
  x = features,
  best_mean = best_partition_mean,
  best_var = best_partition_var,
  best_combined = best_partition_combined
  
)

descriptives <- cbind(
  mean_sd_tab(as.matrix(features), best_partition_mean),
  mean_sd_tab(as.matrix(features), best_partition_var),
  mean_sd_tab(as.matrix(features), best_partition_combined)
)

descriptives <- paste("M =", gsub("[(]", "(SD = ", descriptives))
dim(descriptives) <- c(2, 3)
colnames(descriptives) <- colnames(data[,-1])
apa_table(data, caption = "Illustrative data set")
apa_table(descriptives)

```


Unfortunately, improving algorithmic procedures will not help to find 
better partitions -- the objective function has to change. I will show 
an objective function that captures both similarity in means and 
variance.

## K-Means criterion

In this treatment of k-means, I adopt the notation Steinley (2006) 
provided in his synthesis of the k-means literature. In clustering---or 
anticlustering---, an item pool $X$ = $\{x_1, \ldots, x_N\}$ has to be 
partitioned into $K$ groups $\{C_1, \ldots, C_K\}$. We assume that each 
element $x_i$ ($i = 1, \ldots, N$), is a vector of length $P$, and each 
entry describes one of its numeric measurements. Generally, $X$ 
can is interpreted as an $N \times M$ matrix where each row represents 
an data point $x_i$ (e.g., representing a stimulus or a person) and each 
column is a numeric attribute (e.g., the number of syllables in a word, 
or the attractiveness rating of a face). 

K-Means anticlustering aims to maximize the error sum of squares (SSE), 
the within-group variance, which is the sum of the squared Euclidean 
distances between each data point and its cluster centroid. The cluster 
centroid is given by averaging the values on each variable over the 
elements within that cluster. The centroid of the *j*th variable in the 
*k*th cluster, consisting of $n_k$ elements, is given as: 

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits {i \in C_k} x_{ij}
$$

The k-means objective function, the error sum of squares, is then given 
as:

$$
\mathit{SSE} = 
  \sum\limits_{j=1}^{P} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{ij} - \overline{x}_j ^{(k)})^2
$$

In anticlustering applications, we usually assume that all cluster are 
of equal size. Hence 

$$
n_k = \frac{N}{K}, \forall k \in \{1, \ldots, K\}
$$

---
  
**This is old, adjust to Steinley (2006):**

The squared distance between an element $x = (x_1, \ldots, x_M)$ and the 
average of all elements $\overline{\mathrm{\boldsymbol{x}}}$ is:

$$
\vert \vert x - \overline{\mathrm{\boldsymbol{x}}} \vert \vert^2 = 
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2
$$

Now define new data points representing the squared distances between 
data points and the grand mean:

$$ 
y_i = (x_i - \mathrm{\boldsymbol{x}})^2, \forall i \in N
$$

where $\mathrm{\boldsymbol{x}}$ is a vector of length $M$ containing 
the average for each feature.

Now:

$$
\sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
  \sum\limits_{m = 1}^{M}(y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2 =
  \sum\limits_{m = 1}^{M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 + 
    (y_{m} - \overline{\mathrm{\boldsymbol{y_m}}})^2
$$

If we define $x_{m + 1} = y_1$, $x_{m + 2} = y_2$, ..., $x_{2M} = y_M$.
this is the same as:

$$
\sum\limits_{m = 1}^{2M}(x_{m} - \overline{\mathrm{\boldsymbol{x_m}}})^2 
$$

Thus, we redefine the data points and include an additional $M$ features 
for each element. Then, we maximize $\mathit{Var}_{\mathrm{within}}$, 
which corresponds to a tradeoff between minimizing differences in mean 
features between clusters, and the average squared distance to the grand 
mean, i.e., the within-cluster variance. That is: We try to maximize the 
within-cluster variance, but to a certain degree we also strive to 
obtain similar within-cluster variance in each cluster.

Important to note: We solve the same optimization problem, we just 
change the input a bit (we double the number of "columns"). 

**Example table!!**

# Stuff

New objective seems to work best when N is high relative to K; 
optimizing two objectives at the same time is hard when N is low. Easier 
when only optimizing one objective (i.e., the mean, as done by normal
k-means anticlustering).

- Two examples: $K = 2$, $N = 20$ vs. $N = 100$, using classical
k-means: only mean; using k-variance: only variance; combination:
both, but works better for larger $N$

# Weighted k-means

Using variable weighting (see Steinley, 2006, p. 13), we can 
adjust the relative importance of minimizing differences in means
and variances. Standard in `anticlust`: Equal weight, this is 
achieved by first creating the new variables, representing 
the squared differences from the mean, and then standardizing
the entire feature matrix. If $N$ is large enough (relative to $K$),
there is usually no need for weighting.

# Simulation 

- show that k-variance anticlustering outperforms other objectives

# Examples

- use some examples (e.g., OASIS data set)

# Combined approach better than k-variance alone

- Often, maximizing k-means-variance yields more similar variances 
than k-variance alone. Why is that? I guess: structure in the solution 
space, local maxima are real. When also considering means, you more 
often escape bad local maxima?

# Test equivalence of objectives

Check that I can actually just append the squared difference data as 
additional columns to the original data frame:

```{r}

squared_from_mean <- function(data) {
  apply(data, 2, function(x) (x - mean(x))^2)
}


N <- 500
M <- 20
data <- matrix(rnorm(N * M), ncol = M)
var_data <- squared_from_mean(data)

K <- 10
clusters <- sample(rep_len(1:K, N))

obj1 <- variance_objective(data, clusters) + variance_objective(var_data, clusters)
obj2 <- variance_objective(cbind(data, var_data), clusters)

tolerance <- 1e-10

obj1 - obj2 < tolerance

obj1
obj2

```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
