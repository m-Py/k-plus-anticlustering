## Criterion tradeoff

```{r}

# First rendering is slow because the data needs to be generated
path <- "partition_objectives.csv"

if (!file.exists(path)) {
  set.seed(2207) # 2207
  # Generate data and all partitions
  N <- 14
  K <- 2
  partitions <- generate_partitions(N, K)
  features <- rnorm(N, 15, 3)

  # Compute objectives for each partition
  # For each partition: Compute SSE objective, and SSE(VAR)
  # 1. SSE
  all_objectives_kMeans <- sapply(
    partitions,
    FUN = var_objective,
    features = features
  )
  
  squared_distances_from_mean <- squared_from_mean(as.matrix(features))
  
  # 2. SSE(VAR)
  all_objectives_kVar <- sapply(
    partitions,
    FUN = var_objective,
    features = squared_distances_from_mean
  )
  
  # 3. SSE(KPLUS)
  # Use standardization to compute k-plus criterion!
  all_objectives_kPlus <- sapply(
    partitions,
    FUN = var_objective,
    features = scale(cbind(features, squared_distances_from_mean))
  )
  
  df <- data.frame(
    kmeans = all_objectives_kMeans,
    kvar = all_objectives_kVar,
    kplus = all_objectives_kPlus
  )

  write.table(
    df, path, row.names = FALSE, sep = ","
  )
} else {
  df <- read.csv(path)
}

```

When optimizing a bicriterion objective such as $SSE_{kplus}$, the ideal outcome would be to find a partition where both the means and the variances are very similar between groups, i.e., a which partition where both the standard $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ are close to the global optimum value. Generally, research on multi-objective optimization has shown that cases are rare where a single solution satisfies all criteria to an optimal degree [@ferligoj1992direct]. Usually, a tradeoff is necessary. However, a motivating example illustrates that aiming to simultaneously optimize similarity of means and variances is not only desirable, but may actually be feasible.

```{r, fig.cap = "The relationship between the objectives $SSE$ and $SSE_{Var}$ across all ways to partition $N = 14$ normally distributed random values into $K = 2$ groups. The partition represented by the triangular minimizes the difference in variances between groups; the partition represented by the square minimizes the difference in means between groups; the partition represented by the diamond maximizes $SSE_{kplus}$ and represents a good tradeoff, satisfying both criteria to an almost optimal degree.", fig.height = 5, fig.width = 5}

# Plot SSE vs SSE(VAR)
plot(
  df[, c("kmeans", "kvar")], 
  pch = 4, col = "darkgrey", 
  las = 1, cex = .5,
  xlab = "SSE",
  ylab = "SSE_Var"
)

# Illustrate best partition wrt difference in means (i.e., SSE)
points(
  df$kmeans[which.max(df$kmeans)],
  df$kvar[which.max(df$kmeans)],
  cex = 1.2, pch = 22, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt difference in variance (i.e., SSE(VAR))
points(
  df$kmeans[which.max(df$kvar)],
  df$kvar[which.max(df$kvar)],
  cex = 1.2, pch = 24, bg = "white", lwd = 1.7
)

# Illustrate best partition wrt k-plus criterion
points(
  df$kmeans[which.max(df$kplus)],
  df$kvar[which.max(df$kplus)],
  cex = 1.2, pch = 23, bg = "white", lwd = 1.7
)
```

For this example I created 14 data points following a univariate normal distribution ($M = 15$, $SD = 3$) and then generated all `r n_partitions(14, 2)` possible ways to partition the 14 data points into $K = 2$ equal-sized groups. Figure 1 shows the scatterplot of the criteria $\mathit{SSE}$ and $\mathit{SSE_{Var}}$ across partitions. Note that for both criteria, a larger value is better, i.e., indicates higher similarity with regard to means and variances, respectively. Therefore, to obtain overall between-group similarity, a partition should be chosen that is depicted in the upper right corner of the scatterplot. However, optimal k-means anticlustering would create two groups that are not particularly similar with regard to their variance. The partition that is represented by the squared symbol has the overall best highest $\mathit{SSE}$, but is outperformed by many other partitions with regard to $\mathit{SSE_{Var}}$. Similarly, the partition that maximizes $\mathit{SSE_{Var}}$ (the triangular symbol in Figure 1) is not well-suited to optimize SSE at the same time. The diamond symbol represents the partition that maximizes (the unweighted) $SSE_{kplus}$, i.e., the sum of $\mathit{SSE}$ and $\mathit{SSE_{Var}}$. This partition is located in the right upper corner and yields `r df$kmeans[which.max(df$kplus)] / max(df$kmeans) * 100`% of the global maximum $\mathit{SSE}$, and `r df$kvar[which.max(df$kplus)] / max(df$kvar) * 100`% of the global maximum value of $\mathit{SSE_{Var}}$.

This initial example shows that maximizing the combined k-plus criterion may well be suited to optimize both similarity with regard to means as well as variances between groups. In the next section I present a simulation study that more systematically investigates the performance of the k-plus criterion.
